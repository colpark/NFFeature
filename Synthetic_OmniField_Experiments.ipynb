{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic OmniField Experiments\n",
    "\n",
    "Runnable proofs for the four properties in **SYNTHETIC_OMNIFIELD_EXPERIMENTS.md**:\n",
    "1. **Geometry in representation** — correspondence under known warp T\n",
    "2. **Multi-scale semantics** — probe at coarse vs. fine scale\n",
    "3. **Decoupling from discretization** — train at one resolution, eval at others + random coords\n",
    "4. **Continuous correspondence refinement** — gradient ascent on similarity in y\n",
    "\n",
    "This notebook implements **synthetic data** and **Experiment 3** first (easiest); stubs for 1, 2, 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange, repeat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nf_feature_models import (\n",
    "    CascadedPerceiverIO,\n",
    "    GaussianFourierFeatures,\n",
    "    create_coordinate_grid,\n",
    "    prepare_model_input,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_field_example(h, w, device):\n",
    "    \"\"\"Continuous scalar field f(x,y) on [-1,1]^2 for Exp 3. Returns (H,W) tensor.\"\"\"\n",
    "    y_coord = torch.linspace(-1, 1, h, device=device)\n",
    "    x_coord = torch.linspace(-1, 1, w, device=device)\n",
    "    yy, xx = torch.meshgrid(y_coord, x_coord, indexing='ij')\n",
    "    # Example: distance-like field + smooth bump\n",
    "    f = torch.exp(-(xx**2 + yy**2) / 0.5) + 0.5 * torch.sin(3 * xx) * torch.cos(2 * yy)\n",
    "    return f\n",
    "\n",
    "def field_to_rgb(f):\n",
    "    \"\"\"Map scalar field to 3-channel image for OmniField (context = RGB).\"\"\"\n",
    "    f = (f - f.min()) / (f.max() - f.min() + 1e-8)\n",
    "    return f.unsqueeze(0).expand(3, -1, -1)\n",
    "\n",
    "class ContinuousFieldDataset(Dataset):\n",
    "    \"\"\"Synthetic dataset for Exp 3: continuous field sampled at (train_res, train_res).\"\"\"\n",
    "    def __init__(self, num_samples, res=32, device='cpu'):\n",
    "        self.res = res\n",
    "        self.device = device\n",
    "        self.num_samples = num_samples\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, i):\n",
    "        torch.manual_seed(i)\n",
    "        f = continuous_field_example(self.res, self.res, self.device)\n",
    "        img = field_to_rgb(f).clamp(0, 1)\n",
    "        return img, f  # image (3,H,W), scalar field (H,W) for GT at this res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_affine_warp(device, scale_range=(0.85, 1.0), max_translate=0.1, max_angle_deg=10):\n",
    "    angle = (torch.rand(1, device=device).item() * 2 - 1) * (max_angle_deg * math.pi / 180)\n",
    "    scale = scale_range[0] + (scale_range[1] - scale_range[0]) * torch.rand(1, device=device).item()\n",
    "    tx = (torch.rand(1, device=device).item() * 2 - 1) * max_translate\n",
    "    ty = (torch.rand(1, device=device).item() * 2 - 1) * max_translate\n",
    "    c, s = math.cos(angle), math.sin(angle)\n",
    "    R = torch.tensor([[c*scale, -s*scale], [s*scale, c*scale]], device=device)\n",
    "    t = torch.tensor([tx, ty], device=device)\n",
    "    return R, t\n",
    "\n",
    "def apply_warp_to_coords(coords, R, t):\n",
    "    return (coords @ R.T) + t\n",
    "\n",
    "def apply_warp_to_image(images, R, t):\n",
    "    R_inv = torch.inverse(R.unsqueeze(0)).squeeze(0)\n",
    "    theta = torch.cat([R_inv, -(R_inv @ t.unsqueeze(1))], dim=1).unsqueeze(0)\n",
    "    grid = F.affine_grid(theta.expand(images.size(0), -1, -1), images.size(), align_corners=True)\n",
    "    return F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "\n",
    "class WarpedPairsDataset(Dataset):\n",
    "    \"\"\"For Exp 1 & 4: (image_A, image_B, R, t) with B = warp(A).\"\"\"\n",
    "    def __init__(self, num_samples, res=32, device='cpu'):\n",
    "        self.res = res\n",
    "        self.device = device\n",
    "        self.num_samples = num_samples\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, i):\n",
    "        torch.manual_seed(i)\n",
    "        f = continuous_field_example(self.res, self.res, self.device)\n",
    "        img_a = field_to_rgb(f).clamp(0, 1)\n",
    "        R, t = sample_affine_warp(self.device)\n",
    "        img_b = apply_warp_to_image(img_a.unsqueeze(0), R.unsqueeze(0), t.unsqueeze(0)).squeeze(0)\n",
    "        return img_a, img_b, R, t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Decoupling representation from discretization\n",
    "\n",
    "Train on 32×32; evaluate at 32×32, 64×64, and at **random sub-pixel coordinates**. OmniField should maintain low error at all; a discrete baseline would only be defined on the training grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "FOURIER_MAPPING_SIZE = 96\n",
    "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
    "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
    "QUERIES_DIM = POS_EMBED_DIM\n",
    "LOGITS_DIM = CHANNELS\n",
    "\n",
    "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
    "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
    "model = CascadedPerceiverIO(\n",
    "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
    "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "train_ds = ContinuousFieldDataset(500, res=IMAGE_SIZE, device=DEVICE)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "print('Train batches:', len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(fourier_encoder.parameters()), lr=1e-3)\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    fourier_encoder.train()\n",
    "    total = 0.0\n",
    "    for imgs, fields in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        B = imgs.size(0)\n",
    "        input_data, pixels, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
    "        queries = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
    "        recon = model(input_data, queries=queries)\n",
    "        target = rearrange(imgs, 'b c h w -> b (h w) c')\n",
    "        loss = F.mse_loss(recon, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    print(f'Epoch {epoch+1} recon loss: {total/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_at_resolution(model, fourier_encoder, coords_train, res_eval, train_ds, num_samples=20, device=DEVICE):\n",
    "    \"\"\"Evaluate at resolution res_eval; context is always at training grid (32x32). GT uses same seed as sample.\"\"\"\n",
    "    model.eval()\n",
    "    fourier_encoder.eval()\n",
    "    coords_eval = create_coordinate_grid(res_eval, res_eval, device)\n",
    "    mse_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img, _ = train_ds[i]\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            B = 1\n",
    "            input_data, _, _ = prepare_model_input(img, coords_train, fourier_encoder)\n",
    "            queries = fourier_encoder(coords_eval.unsqueeze(0).expand(B, -1, -1))\n",
    "            recon = model(input_data, queries=queries)\n",
    "            torch.manual_seed(i)\n",
    "            gt_at_eval = continuous_field_example(res_eval, res_eval, device)\n",
    "            gt_rgb = field_to_rgb(gt_at_eval).unsqueeze(0).to(device)\n",
    "            target = rearrange(gt_rgb, 'b c h w -> b (h w) c')\n",
    "            mse_sum += F.mse_loss(recon, target).item()\n",
    "            n += 1\n",
    "    return mse_sum / max(n, 1)\n",
    "\n",
    "def eval_at_random_coords(model, fourier_encoder, coords_train, train_ds, num_points=500, num_samples=20, device=DEVICE):\n",
    "    \"\"\"Query at random (sub-pixel) coordinates; GT from continuous field (same seed as sample).\"\"\"\n",
    "    model.eval()\n",
    "    fourier_encoder.eval()\n",
    "    mse_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img, _ = train_ds[i]\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            B = 1\n",
    "            input_data, _, _ = prepare_model_input(img, coords_train, fourier_encoder)\n",
    "            coords_rand = (torch.rand(num_points, 2, device=device) * 2 - 1)\n",
    "            queries = fourier_encoder(coords_rand.unsqueeze(0).expand(B, -1, -1))\n",
    "            recon = model(input_data, queries=queries)\n",
    "            torch.manual_seed(i)\n",
    "            f_hr = continuous_field_example(128, 128, device)\n",
    "            sample_grid = coords_rand.view(1, num_points, 1, 2)\n",
    "            f_at_rand = F.grid_sample(f_hr.unsqueeze(0).unsqueeze(0), sample_grid, mode='bilinear', align_corners=True).squeeze()\n",
    "            f_norm = (f_at_rand - f_at_rand.min()) / (f_at_rand.max() - f_at_rand.min() + 1e-8)\n",
    "            target = f_norm.unsqueeze(0).unsqueeze(-1).expand(1, num_points, 3)\n",
    "            mse_sum += F.mse_loss(recon, target).item()\n",
    "            n += 1\n",
    "    return mse_sum / max(n, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval at training res (32), double res (64), and random coords\n",
    "mse_32 = eval_at_resolution(model, fourier_encoder, coords_32, 32, train_ds)\n",
    "mse_64 = eval_at_resolution(model, fourier_encoder, coords_32, 64, train_ds)\n",
    "mse_rand = eval_at_random_coords(model, fourier_encoder, coords_32, train_ds)\n",
    "print('MSE @ 32x32:', mse_32)\n",
    "print('MSE @ 64x64:', mse_64)\n",
    "print('MSE @ random coords:', mse_rand)\n",
    "print('Decoupling: model was trained only at 32x32 but can query at 64x64 and arbitrary points.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 (stub): Geometry in representation\n",
    "\n",
    "Use `WarpedPairsDataset`; encode A and B; for anchors in A compute φ_A(x), in B compute φ_B(y) on a grid; rank y by similarity; measure Recall@k where true match is T(x). See SYNTHETIC_OMNIFIELD_EXPERIMENTS.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load warped pairs; get_residual + projection head for phi; compute rank/Recall@k\n",
    "warped_ds = WarpedPairsDataset(100, res=IMAGE_SIZE, device=DEVICE)\n",
    "img_a, img_b, R, t = warped_ds[0]\n",
    "print('Warped pair shapes:', img_a.shape, img_b.shape, 'R', R.shape, 't', t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4 (stub): Continuous correspondence refinement\n",
    "\n",
    "For anchor x in A, maximize S(y) = similarity(φ_A(x), φ_B(y)) over continuous y via gradient ascent. Requires y.requires_grad_(True) and backprop through decoder to y. See doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: differentiable y; gradient ascent; report error before/after refinement\n",
    "print('Stub: implement refinement loop with decoder(queries=GFF(y), context=residual_b), y.requires_grad_(True)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
