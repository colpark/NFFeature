{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft InfoNCE on OmniField (CIFAR-10)\n",
        "\n",
        "**Minimal change** on top of the original OmniField reconstruction: add a **geometry-aware contrastive objective (Soft InfoNCE)** on probe-derived tokens from the neural field.\n",
        "\n",
        "- Sample anchor coords in view A and candidate coords in view B (B = affine augmentation of A with **known T**).\n",
        "- Build L2-normalized feature tokens φ(x) from the field (decoder hidden state → projection head).\n",
        "- Soft positives: \\(w_{ij} \\propto \\exp(-\\|x_j^B - T(x_i^A)\\|^2 / (2\\sigma^2))\\) (Gaussian kernel).\n",
        "- Loss: \\(\\mathcal{L} = \\mathcal{L}_{recon} + \\lambda_{ctr} \\mathcal{L}_{softNCE}\\).\n",
        "\n",
        "Defaults: N_a=256, N_b=1024, d=128, τ=0.1, σ=0.08, λ_ctr=0.1 (ramp 0→0.1 over 500 steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT_DIR = 'checkpoints'\n",
        "CKPT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint_best.pt')\n",
        "if not os.path.isfile(CKPT_PATH):\n",
        "    CKPT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint_last.pt')\n",
        "assert os.path.isfile(CKPT_PATH), f'No checkpoint in {CHECKPOINT_DIR}. Train AblationCIFAR10 first.'\n",
        "print(f'Device: {DEVICE}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OmniField config (match checkpoint)\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(in_features=2, mapping_size=FOURIER_MAPPING_SIZE, scale=15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM,\n",
        "    queries_dim=QUERIES_DIM,\n",
        "    logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512),\n",
        "    num_latents=(256, 256, 256),\n",
        "    decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "\n",
        "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
        "fourier_encoder.load_state_dict(ckpt['fourier_encoder_state_dict'], strict=False)\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(f'Loaded {CKPT_PATH}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_residual(model, data):\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def get_rgb_and_phi_raw(model, queries, residual):\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    phi_raw = x\n",
        "    rgb = model.to_logits(x)\n",
        "    return rgb, phi_raw\n",
        "\n",
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    sampled = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
        "    return sampled.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def make_grid_2d(h, w, device):\n",
        "    y = torch.linspace(-1, 1, h, device=device)\n",
        "    x = torch.linspace(-1, 1, w, device=device)\n",
        "    g = torch.stack(torch.meshgrid(y, x, indexing='ij'), dim=-1)\n",
        "    return g.reshape(-1, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Affine augmentation with known T (A → B)\n",
        "\n",
        "We apply an affine transform to get view B and store the forward map T so that for any coord in A we have T(x) in B space. Coords are in [-1, 1]²."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_affine_params(batch_size, device, scale_range=(0.8, 1.0), max_translate=0.1, max_angle_deg=15):\n",
        "    '''Returns T (A→B): R (B,2,2), t (B,2). T(p) = p @ R.T + t (row vectors).'''\n",
        "    angle = (torch.rand(batch_size, device=device) * 2 - 1) * (max_angle_deg * math.pi / 180)\n",
        "    scale = scale_range[0] + torch.rand(batch_size, device=device) * (scale_range[1] - scale_range[0])\n",
        "    tx = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
        "    ty = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
        "    c, s = torch.cos(angle), torch.sin(angle)\n",
        "    R = torch.stack([c * scale, -s * scale, s * scale, c * scale], dim=-1).view(batch_size, 2, 2)\n",
        "    t = torch.stack([tx, ty], dim=1)\n",
        "    return R, t\n",
        "\n",
        "def apply_affine_to_coords(coords, R, t):\n",
        "    '''coords (B, N, 2), R (B, 2, 2), t (B, 2). T(p)=R@p+t. Returns (B, N, 2) in B space.'''\n",
        "    return torch.einsum('bed,bnd->bne', R, coords) + t.unsqueeze(1)\n",
        "\n",
        "def apply_affine_to_image(images, R, t, align_corners=True):\n",
        "    '''View B = T(A). We need grid such that B is sampled from A at T_inv(B_coords). R,t = T.'''\n",
        "    B, C, H, W = images.shape\n",
        "    R_inv = torch.inverse(R)\n",
        "    t_exp = t.unsqueeze(1)\n",
        "    theta = torch.cat([R_inv, -(R_inv @ t.unsqueeze(2))], dim=2)\n",
        "    grid = F.affine_grid(theta, images.size(), align_corners=align_corners)\n",
        "    out = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=align_corners)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim=128):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, out_dim)\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "    def forward(self, z):\n",
        "        return F.normalize(self.proj(z), dim=-1)\n",
        "\n",
        "PROJ_DIM = 128\n",
        "projection_head = ProjectionHead(QUERIES_DIM, PROJ_DIM).to(DEVICE)\n",
        "print(projection_head)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def soft_infonce_loss(phi_a, phi_b, coords_a, coords_b, R, t, tau=0.1, sigma=0.08):\n",
        "    '''\n",
        "    phi_a (B, N_a, d), phi_b (B, N_b, d) L2-normalized.\n",
        "    coords_a (B, N_a, 2), coords_b (B, N_b, 2). T: A->B given by R (B,2,2), t (B,2).\n",
        "    Soft weights w_ij = exp(-||x_j^B - T(x_i^A)||^2 / (2*sigma^2)), normalized over j.\n",
        "    '''\n",
        "    B, N_a, _ = phi_a.shape\n",
        "    N_b = phi_b.size(1)\n",
        "    logits = torch.bmm(phi_a, phi_b.transpose(1, 2)) / tau\n",
        "    xi_mapped = apply_affine_to_coords(coords_a, R, t)\n",
        "    sqd = ((coords_b.unsqueeze(1) - xi_mapped.unsqueeze(2)) ** 2).sum(-1)\n",
        "    w = torch.exp(-sqd / (2 * sigma ** 2))\n",
        "    w = w / (w.sum(dim=2, keepdim=True) + 1e-8)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    loss = -(w * log_probs).sum(-1).mean()\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = {\n",
        "    'subset_size': 10000,\n",
        "    'batch_size': 32,\n",
        "    'N_a': 256,\n",
        "    'N_b': 1024,\n",
        "    'tau': 0.1,\n",
        "    'sigma': 0.08,\n",
        "    'lambda_ctr': 0.1,\n",
        "    'ramp_steps': 500,\n",
        "    'epochs': 5,\n",
        "    'lr': 1e-3,\n",
        "    'freeze_backbone': False,\n",
        "}\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_sub = Subset(train_ds, list(range(min(cfg['subset_size'], len(train_ds)))))\n",
        "train_loader = DataLoader(train_sub, batch_size=cfg['batch_size'], shuffle=True, num_workers=0)\n",
        "val_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop: recon + Soft InfoNCE (λ ramp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = list(projection_head.parameters())\n",
        "if not cfg.get('freeze_backbone'):\n",
        "    params += list(model.parameters()) + list(fourier_encoder.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=cfg['lr'])\n",
        "global_step = [0]\n",
        "\n",
        "def get_lambda_ctr(step):\n",
        "    ramp = cfg['ramp_steps']\n",
        "    if step >= ramp:\n",
        "        return cfg['lambda_ctr']\n",
        "    return cfg['lambda_ctr'] * (step / ramp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_psnr(model, fourier_encoder, loader, device, grid_size=32):\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    grid = make_grid_2d(grid_size, grid_size, device)\n",
        "    mse_sum, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            input_data, _, _ = prepare_model_input(imgs, grid, fourier_encoder)\n",
        "            residual = get_residual(model, input_data)\n",
        "            coords_batch = grid.unsqueeze(0).expand(B, -1, -1)\n",
        "            queries = fourier_encoder(coords_batch)\n",
        "            rgb, _ = get_rgb_and_phi_raw(model, queries, residual)\n",
        "            gt = sample_gt_at_coords(imgs, coords_batch)\n",
        "            mse_sum += F.mse_loss(rgb, gt, reduction='sum').item()\n",
        "            n += B * grid.size(0)\n",
        "    mse = mse_sum / max(n, 1)\n",
        "    return 10 * math.log10(1.0 / (mse + 1e-10))\n",
        "\n",
        "print(f'Baseline val PSNR: {eval_psnr(model, fourier_encoder, val_loader, DEVICE):.2f} dB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "psnr_log = []\n",
        "for ep in range(cfg['epochs']):\n",
        "    model.train()\n",
        "    fourier_encoder.train()\n",
        "    projection_head.train()\n",
        "    total_recon = 0.0\n",
        "    total_ctr = 0.0\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B, C, H, W = imgs.shape\n",
        "        R, t = sample_affine_params(B, DEVICE)\n",
        "        imgs_b = apply_affine_to_image(imgs, R, t)\n",
        "        N_a, N_b = cfg['N_a'], cfg['N_b']\n",
        "        anchors_a = (torch.rand(B, N_a, 2, device=DEVICE) * 2 - 1)\n",
        "        candidates_b = (torch.rand(B, N_b, 2, device=DEVICE) * 2 - 1)\n",
        "        input_a, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "        input_b, _, _ = prepare_model_input(imgs_b, coords_32, fourier_encoder)\n",
        "        residual_a = get_residual(model, input_a)\n",
        "        residual_b = get_residual(model, input_b)\n",
        "        queries_a = fourier_encoder(anchors_a)\n",
        "        queries_b = fourier_encoder(candidates_b)\n",
        "        rgb_a, phi_raw_a = get_rgb_and_phi_raw(model, queries_a, residual_a)\n",
        "        _, phi_raw_b = get_rgb_and_phi_raw(model, queries_b, residual_b)\n",
        "        phi_a = projection_head(phi_raw_a)\n",
        "        phi_b = projection_head(phi_raw_b)\n",
        "        loss_ctr = soft_infonce_loss(phi_a, phi_b, anchors_a, candidates_b, R, t, tau=cfg['tau'], sigma=cfg['sigma'])\n",
        "        coords_full = coords_32.unsqueeze(0).expand(B, -1, -1)\n",
        "        queries_full = fourier_encoder(coords_full)\n",
        "        rgb_full, _ = get_rgb_and_phi_raw(model, queries_full, residual_a)\n",
        "        gt_full = sample_gt_at_coords(imgs, coords_full)\n",
        "        loss_recon = F.mse_loss(rgb_full, gt_full)\n",
        "        lam = get_lambda_ctr(global_step[0])\n",
        "        loss = loss_recon + lam * loss_ctr\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        global_step[0] += 1\n",
        "        total_recon += loss_recon.item()\n",
        "        total_ctr += loss_ctr.item()\n",
        "    psnr = eval_psnr(model, fourier_encoder, val_loader, DEVICE)\n",
        "    psnr_log.append(psnr)\n",
        "    print(f'Epoch {ep+1} recon: {total_recon/len(train_loader):.4f} ctr: {total_ctr/len(train_loader):.4f} val PSNR: {psnr:.2f} dB')\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.plot(psnr_log, 'o-')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Val PSNR (dB)')\n",
        "plt.title('Reconstruction PSNR (baseline vs +Soft InfoNCE)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_psnr.png', dpi=100)\n",
        "plt.show()\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "nce_ckpt_path = os.path.join(CHECKPOINT_DIR, 'checkpoint_nce_best.pt')\n",
        "torch.save({'model_state_dict': model.state_dict(), 'fourier_encoder_state_dict': fourier_encoder.state_dict(), 'projection_head_state_dict': projection_head.state_dict()}, nce_ckpt_path)\n",
        "print('Saved NCE-trained model to', nce_ckpt_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n",
        "\n",
        "1. **Attention heatmap (soft weights)** for a few anchors: w_j over B's grid.\n",
        "2. **Coordinate error**: for each anchor, argmax_j similarity → coord; distance to T(x_i^A).\n",
        "3. **2D embedding** (PCA/t-SNE) of tokens colored by class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "projection_head.eval()\n",
        "imgs, labels = next(iter(val_loader))\n",
        "imgs = imgs[:4].to(DEVICE)\n",
        "labels = labels[:4]\n",
        "B = 4\n",
        "R, t = sample_affine_params(B, DEVICE)\n",
        "imgs_b = apply_affine_to_image(imgs, R, t)\n",
        "grid_b = make_grid_2d(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "N_a_vis = 64\n",
        "anchors_a = (torch.rand(B, N_a_vis, 2, device=DEVICE) * 2 - 1)\n",
        "with torch.no_grad():\n",
        "    input_a, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "    input_b, _, _ = prepare_model_input(imgs_b, coords_32, fourier_encoder)\n",
        "    residual_a = get_residual(model, input_a)\n",
        "    residual_b = get_residual(model, input_b)\n",
        "    coords_b_batch = grid_b.unsqueeze(0).expand(B, -1, -1)\n",
        "    queries_a = fourier_encoder(anchors_a)\n",
        "    queries_b = fourier_encoder(coords_b_batch)\n",
        "    _, phi_raw_a = get_rgb_and_phi_raw(model, queries_a, residual_a)\n",
        "    _, phi_raw_b = get_rgb_and_phi_raw(model, queries_b, residual_b)\n",
        "    phi_a = projection_head(phi_raw_a)\n",
        "    phi_b = projection_head(phi_raw_b)\n",
        "    logits = torch.bmm(phi_a, phi_b.transpose(1, 2)) / cfg['tau']\n",
        "    xi_mapped = apply_affine_to_coords(anchors_a, R, t)\n",
        "    sqd = ((coords_b_batch.unsqueeze(1) - xi_mapped.unsqueeze(2)) ** 2).sum(-1)\n",
        "    w = torch.exp(-sqd / (2 * cfg['sigma'] ** 2))\n",
        "    w = w / (w.sum(dim=2, keepdim=True) + 1e-8)\n",
        "b_show = 0\n",
        "n_anchors_show = 4\n",
        "fig, axs = plt.subplots(2, n_anchors_show, figsize=(12, 5))\n",
        "for i in range(n_anchors_show):\n",
        "    heat = w[b_show, i].cpu().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
        "    axs[0, i].imshow(heat, cmap='hot')\n",
        "    axs[0, i].set_title(f'Anchor {i} soft weights')\n",
        "    axs[0, i].axis('off')\n",
        "    axs[1, i].imshow(imgs_b[b_show].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
        "    axs[1, i].axis('off')\n",
        "plt.suptitle('Soft positive weights w_j over view B (one image)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_heatmap.png', dpi=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    best_j = logits[b_show].argmax(dim=1)\n",
        "    pred_coords = coords_b_batch[b_show][best_j]\n",
        "    gt_coords = xi_mapped[b_show]\n",
        "    err = (pred_coords - gt_coords).norm(dim=-1).cpu().numpy()\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.violinplot([err], positions=[0], showmeans=True)\n",
        "plt.ylabel('Coord error (argmax candidate vs T(x_i^A))')\n",
        "plt.title('Coordinate error (normalized space)')\n",
        "plt.xticks([0], ['Soft InfoNCE'])\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_coord_error.png', dpi=100)\n",
        "plt.show()\n",
        "print(f'Mean coord error: {err.mean():.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs_e, labels_e = next(iter(val_loader))\n",
        "imgs_e = imgs_e.to(DEVICE)\n",
        "n_embed = imgs_e.size(0)\n",
        "labels_e = labels_e.cpu().numpy()\n",
        "grid_flat = make_grid_2d(8, 8, DEVICE)\n",
        "coords_embed = grid_flat.unsqueeze(0).expand(n_embed, -1, -1)\n",
        "with torch.no_grad():\n",
        "    input_embed, _, _ = prepare_model_input(imgs_e, coords_32, fourier_encoder)\n",
        "    residual_embed = get_residual(model, input_embed)\n",
        "    queries_embed = fourier_encoder(coords_embed)\n",
        "    _, phi_raw_embed = get_rgb_and_phi_raw(model, queries_embed, residual_embed)\n",
        "    phi_embed = projection_head(phi_raw_embed)\n",
        "feats = phi_embed.cpu().numpy().reshape(-1, PROJ_DIM)\n",
        "label_tile = np.repeat(labels_e[:, None], 64, axis=1).reshape(-1)\n",
        "try:\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    feats_2d = pca.fit_transform(feats)\n",
        "except ImportError:\n",
        "    feats_2d = np.random.randn(feats.shape[0], 2)\n",
        "    print('Install sklearn for PCA; using random 2D for demo.')\n",
        "plt.figure(figsize=(6, 5))\n",
        "sc = plt.scatter(feats_2d[:, 0], feats_2d[:, 1], c=label_tile, cmap='tab10', s=1, alpha=0.6)\n",
        "plt.colorbar(sc, label='Class')\n",
        "plt.title('Token embedding PCA (colored by CIFAR-10 class)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_embedding.png', dpi=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Single intervention: **Soft InfoNCE** on probe-derived tokens (decoder hidden → projection head).\n",
        "- Geometry-aware soft positives with known affine T; λ_ctr ramped to avoid destabilizing recon.\n",
        "- **Visualizations**: PSNR curve; attention heatmap; coord error violin; **View A/B** (anchors → GT vs pred); **Retrieval @ ε**; **weight concentration** histogram; **heatmap overlay** (GT + pred on w_j); 2D token embedding PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What NCE adds: correspondence quality and weight concentration\n",
        "\n",
        "The next cells show (1) **View A → View B**: anchors on A and their GT vs predicted match on B; (2) **Retrieval @ ε**: fraction of anchors matched within tolerance; (3) **Soft weight concentration**: how peaked the soft positives are; (4) **Heatmap overlay**: GT and predicted match on the weight heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View A vs View B: anchors on A, GT and predicted match on B (reuses logits, anchors_a, xi_mapped, pred_coords from above)\n",
        "def norm_to_pixel(coords_norm, h, w):\n",
        "    y, x = coords_norm[..., 0], coords_norm[..., 1]\n",
        "    row = (y + 1) / 2 * (h - 1)\n",
        "    col = (x + 1) / 2 * (w - 1)\n",
        "    return col.cpu().numpy(), row.cpu().numpy()\n",
        "\n",
        "n_show = min(8, anchors_a.size(1))\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].imshow(imgs[b_show].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
        "axs[0].set_title('View A (anchors)')\n",
        "axs[0].axis('off')\n",
        "cx_a, cy_a = norm_to_pixel(anchors_a[b_show, :n_show], IMAGE_SIZE, IMAGE_SIZE)\n",
        "axs[0].scatter(cx_a, cy_a, c='lime', s=40, marker='o', edgecolors='black', linewidths=0.5, label='anchors')\n",
        "\n",
        "axs[1].imshow(imgs_b[b_show].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
        "axs[1].set_title('View B (GT vs predicted match)')\n",
        "axs[1].axis('off')\n",
        "cx_gt, cy_gt = norm_to_pixel(xi_mapped[b_show, :n_show], IMAGE_SIZE, IMAGE_SIZE)\n",
        "cx_pr, cy_pr = norm_to_pixel(pred_coords[:n_show], IMAGE_SIZE, IMAGE_SIZE)\n",
        "axs[1].scatter(cx_gt, cy_gt, c='lime', s=60, marker='+', linewidths=2, label='GT T(x)')\n",
        "axs[1].scatter(cx_pr, cy_pr, c='cyan', s=40, marker='x', linewidths=1.5, label='pred (argmax)')\n",
        "axs[1].legend(loc='upper right', fontsize=8)\n",
        "plt.suptitle('Correspondence: green = anchor/GT, cyan = model prediction')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_viewA_viewB.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieval @ ε: fraction of anchors whose predicted match is within ε of GT (normalized coords)\n",
        "eps_values = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "err_all = (pred_coords - xi_mapped[b_show]).norm(dim=-1).cpu().numpy()\n",
        "acc_at_eps = [(err_all < eps).mean() * 100 for eps in eps_values]\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "ax.bar(range(len(eps_values)), acc_at_eps, color='steelblue', edgecolor='black')\n",
        "ax.set_xticks(range(len(eps_values)))\n",
        "ax.set_xticklabels([str(e) for e in eps_values])\n",
        "ax.set_xlabel('ε (normalized coord distance)')\n",
        "ax.set_ylabel('% anchors with error < ε')\n",
        "ax.set_title('Retrieval accuracy: how often does argmax fall near GT?')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_retrieval_at_eps.png', dpi=100)\n",
        "plt.show()\n",
        "print('Retrieval @ 0.1: {:.1f}%  @ 0.2: {:.1f}%'.format(acc_at_eps[1], acc_at_eps[3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Soft weight concentration: max_j w_ij per anchor (higher = model puts mass on few candidates)\n",
        "w_max = w.max(dim=2).values.flatten().cpu().numpy()\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(w_max, bins=30, color='steelblue', edgecolor='black', alpha=0.8)\n",
        "plt.xlabel('max_j w_ij (soft weight on best candidate)')\n",
        "plt.ylabel('Count (anchors)')\n",
        "plt.title('Concentration: peaked weights = confident correspondence')\n",
        "plt.axvline(w_max.mean(), color='red', linestyle='--', label=f'mean={w_max.mean():.3f}')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_weight_concentration.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap overlay: soft weights with GT (+) and predicted (x) match for each anchor\n",
        "fig, axs = plt.subplots(2, n_anchors_show, figsize=(12, 5))\n",
        "for i in range(n_anchors_show):\n",
        "    heat = w[b_show, i].cpu().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
        "    axs[0, i].imshow(heat, cmap='hot')\n",
        "    axs[0, i].set_title(f'Anchor {i}')\n",
        "    cx_gt, cy_gt = norm_to_pixel(xi_mapped[b_show, i:i+1], IMAGE_SIZE, IMAGE_SIZE)\n",
        "    cx_pr, cy_pr = norm_to_pixel(pred_coords[i:i+1], IMAGE_SIZE, IMAGE_SIZE)\n",
        "    axs[0, i].scatter(cx_gt, cy_gt, c='lime', s=80, marker='+', linewidths=2, label='GT')\n",
        "    axs[0, i].scatter(cx_pr, cy_pr, c='cyan', s=50, marker='x', linewidths=1.5, label='pred')\n",
        "    axs[0, i].axis('off')\n",
        "    axs[1, i].imshow(imgs_b[b_show].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
        "    axs[1, i].scatter(cx_gt, cy_gt, c='lime', s=80, marker='+', linewidths=2)\n",
        "    axs[1, i].scatter(cx_pr, cy_pr, c='cyan', s=50, marker='x', linewidths=1.5)\n",
        "    axs[1, i].axis('off')\n",
        "plt.suptitle('Soft weights: lime = GT T(x), cyan = argmax prediction')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_heatmap_overlay.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature-level difference: what NCE learned\n",
        "\n",
        "Below we compare **positive pairs** (anchor ↔ its true correspondence in B) vs **negative pairs** (anchor ↔ other candidates) in **cosine similarity**. NCE pulls positives up and pushes negatives down, so we expect a clear gap. We also compare to a **baseline** (same backbone, random projection head) to see the feature-level effect of contrastive training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cosine similarity matrix (phi are L2-normalized, so logits * tau = cos sim)\n",
        "S_nce = (logits[b_show] * cfg['tau']).detach().cpu().numpy()\n",
        "N_a, N_b = S_nce.shape\n",
        "# GT match index per anchor: candidate j closest to T(anchor_i)\n",
        "sqd_b = ((coords_b_batch[b_show].unsqueeze(0) - xi_mapped[b_show].unsqueeze(1)) ** 2).sum(-1).cpu().numpy()\n",
        "j_gt = np.argmin(sqd_b, axis=1)\n",
        "pos_sims = S_nce[np.arange(N_a), j_gt]\n",
        "neg_mask = np.ones((N_a, N_b), dtype=bool)\n",
        "neg_mask[np.arange(N_a), j_gt] = False\n",
        "neg_sims = S_nce[neg_mask]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 3.5))\n",
        "ax.hist(neg_sims, bins=40, alpha=0.6, color='coral', label='negative pairs', density=True)\n",
        "ax.hist(pos_sims, bins=30, alpha=0.6, color='green', label='positive pairs (GT corr.)', density=True)\n",
        "ax.axvline(pos_sims.mean(), color='green', linestyle='--', linewidth=1.5, label=f'pos mean={pos_sims.mean():.3f}')\n",
        "ax.axvline(neg_sims.mean(), color='coral', linestyle='--', linewidth=1.5, label=f'neg mean={neg_sims.mean():.3f}')\n",
        "ax.set_xlabel('Cosine similarity φ(anchor) · φ(candidate)')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Feature-level: NCE-trained model (pos vs neg)')\n",
        "ax.legend(loc='upper left', fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_feature_pos_neg_hist.png', dpi=100)\n",
        "plt.show()\n",
        "print(f'Positive mean: {pos_sims.mean():.4f}  Negative mean: {neg_sims.mean():.4f}  Gap: {pos_sims.mean()-neg_sims.mean():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-anchor margin: sim to GT match minus mean sim to all candidates (higher = more discriminative)\n",
        "margin_nce = pos_sims - S_nce.mean(axis=1)\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(margin_nce, bins=30, color='steelblue', edgecolor='black', alpha=0.8)\n",
        "plt.axvline(margin_nce.mean(), color='red', linestyle='--', label=f'mean margin={margin_nce.mean():.3f}')\n",
        "plt.xlabel('Margin (sim to GT − mean sim to candidates)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Feature margin: how much higher is sim to true correspondence?')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_feature_margin.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline: same backbone, RANDOM projection head (no contrastive training) → feature-level comparison\n",
        "proj_baseline = ProjectionHead(QUERIES_DIM, PROJ_DIM).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    phi_bl_a = proj_baseline(phi_raw_a)\n",
        "    phi_bl_b = proj_baseline(phi_raw_b)\n",
        "S_bl = (torch.bmm(phi_bl_a, phi_bl_b.transpose(1, 2))[b_show]).cpu().numpy()\n",
        "pos_sims_bl = S_bl[np.arange(N_a), j_gt]\n",
        "neg_sims_bl = S_bl[neg_mask]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "ax.hist(neg_sims, bins=40, alpha=0.4, color='coral', label='NCE neg', density=True)\n",
        "ax.hist(pos_sims, bins=30, alpha=0.5, color='green', label='NCE pos', density=True)\n",
        "ax.hist(neg_sims_bl, bins=40, alpha=0.4, color='gray', histtype='step', linewidth=2, label='baseline neg', density=True)\n",
        "ax.hist(pos_sims_bl, bins=30, alpha=0.5, color='blue', histtype='step', linewidth=2, label='baseline pos', density=True)\n",
        "ax.set_xlabel('Cosine similarity')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Feature-level: NCE (filled) vs baseline / random proj (outline)')\n",
        "ax.legend(loc='upper left', fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_feature_baseline_vs_nce.png', dpi=100)\n",
        "plt.show()\n",
        "print('NCE:    pos mean={:.4f}  neg mean={:.4f}  gap={:.4f}'.format(pos_sims.mean(), neg_sims.mean(), pos_sims.mean()-neg_sims.mean()))\n",
        "print('Baseline: pos mean={:.4f}  neg mean={:.4f}  gap={:.4f}'.format(pos_sims_bl.mean(), neg_sims_bl.mean(), pos_sims_bl.mean()-neg_sims_bl.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TDSM: token-decoded spatial map (baseline vs NCE)\n",
        "\n",
        "Same as **TDSM_Classification.ipynb**: each **latent token** is used as the only context → decode at full 32×32 grid → one \"component\" image per token (texture-like in baseline). We compare **baseline** (no NCE) vs **NCE-trained**: do per-token reconstructions look less texture / more structure with NCE?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline model (same checkpoint, never NCE-trained) for comparison\n",
        "baseline_fourier = GaussianFourierFeatures(in_features=2, mapping_size=FOURIER_MAPPING_SIZE, scale=15.0).to(DEVICE)\n",
        "baseline_model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "ckpt_baseline = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "baseline_model.load_state_dict(ckpt_baseline['model_state_dict'], strict=False)\n",
        "baseline_fourier.load_state_dict(ckpt_baseline['fourier_encoder_state_dict'], strict=False)\n",
        "baseline_model.eval()\n",
        "baseline_fourier.eval()\n",
        "for p in baseline_model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in baseline_fourier.parameters():\n",
        "    p.requires_grad = False\n",
        "print('Baseline model (no NCE) loaded from checkpoint.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TDSM: one latent token -> full 32x32 decoded image (same as TDSM_Classification.ipynb)\n",
        "def decoder_forward(model, queries, context):\n",
        "    '''queries (B,N,qd), context (B,1 or B,L,ld) -> (B,N,3).'''\n",
        "    x = model.decoder_cross_attn(queries, context=context)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return model.to_logits(x)\n",
        "\n",
        "def get_tdsm(model, fourier_encoder, data, coords_32, device, num_tokens=256, token_step=4):\n",
        "    '''One decoded 32x32x3 per latent token (context = that token only); return (B, n_tokens, 32, 32) mean over RGB.'''\n",
        "    with torch.no_grad():\n",
        "        residual = get_residual(model, data)\n",
        "        B = data.size(0)\n",
        "        queries_32 = fourier_encoder(repeat(coords_32, 'n d -> b n d', b=B)).to(device)\n",
        "        component_images = []\n",
        "        for k in range(0, num_tokens, token_step):\n",
        "            ctx_k = residual[:, k:k+1, :]\n",
        "            logits_k = decoder_forward(model, queries_32, ctx_k)\n",
        "            img_k = logits_k.reshape(B, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "            component_images.append(img_k)\n",
        "        component_images = torch.stack(component_images, dim=1)\n",
        "        tdsm = component_images.mean(dim=-1)\n",
        "    return tdsm\n",
        "\n",
        "TDSM_TOKEN_STEP = 4\n",
        "imgs_tdsm, _ = next(iter(val_loader))\n",
        "imgs_tdsm = imgs_tdsm[:4].to(DEVICE)\n",
        "input_tdsm, _, _ = prepare_model_input(imgs_tdsm, coords_32, fourier_encoder)\n",
        "with torch.no_grad():\n",
        "    tdsm_baseline = get_tdsm(baseline_model, baseline_fourier, input_tdsm, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "    tdsm_nce      = get_tdsm(model, fourier_encoder, input_tdsm, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "# (B, 64, 32, 32) each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TDSM maps: same token indices, baseline vs NCE (one sample). Baseline = texture-like; does NCE change it?\n",
        "sample_idx = 0\n",
        "token_indices = [0, 16, 32, 48]\n",
        "n_show = len(token_indices)\n",
        "# Map token_indices (0..255) to TDSM slice index (0..63 when token_step=4): k // TDSM_TOKEN_STEP\n",
        "tdsm_slice_idx = [k // TDSM_TOKEN_STEP for k in token_indices]\n",
        "fig, axs = plt.subplots(2, n_show, figsize=(12, 5))\n",
        "for i, (k, sk) in enumerate(zip(token_indices, tdsm_slice_idx)):\n",
        "    axs[0, i].imshow(tdsm_baseline[sample_idx, sk].cpu().numpy(), cmap='viridis')\n",
        "    axs[0, i].set_title(f'Token {k} (baseline)')\n",
        "    axs[0, i].axis('off')\n",
        "    axs[1, i].imshow(tdsm_nce[sample_idx, sk].cpu().numpy(), cmap='viridis')\n",
        "    axs[1, i].set_title(f'Token {k} (NCE)')\n",
        "    axs[1, i].axis('off')\n",
        "plt.suptitle('TDSM: per-token recon (baseline = texture-like; does NCE change structure?)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_tdsm_baseline_vs_nce.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RGB component images (full 32x32 per token) for same tokens: baseline vs NCE\n",
        "with torch.no_grad():\n",
        "    residual_bl = get_residual(baseline_model, input_tdsm)\n",
        "    residual_nce = get_residual(model, input_tdsm)\n",
        "    queries_32 = fourier_encoder(repeat(coords_32, 'n d -> b n d', b=imgs_tdsm.size(0))).to(DEVICE)\n",
        "    comps_bl, comps_nce = [], []\n",
        "    for k in token_indices:\n",
        "        ctx_bl = residual_bl[:, k:k+1, :]\n",
        "        ctx_nce = residual_nce[:, k:k+1, :]\n",
        "        comps_bl.append(decoder_forward(baseline_model, queries_32, ctx_bl).reshape(imgs_tdsm.size(0), IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "        comps_nce.append(decoder_forward(model, queries_32, ctx_nce).reshape(imgs_tdsm.size(0), IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "    comps_bl = torch.stack(comps_bl, dim=0)\n",
        "    comps_nce = torch.stack(comps_nce, dim=0)\n",
        "def to_display(t):\n",
        "    return (t.cpu() / 2 + 0.5).clamp(0, 1) if t.abs().max() > 1.5 else t.cpu().clamp(0, 1)\n",
        "fig, axs = plt.subplots(2, n_show + 1, figsize=(14, 5))\n",
        "axs[0, 0].imshow(to_display(imgs_tdsm[sample_idx]).permute(1, 2, 0).numpy())\n",
        "axs[0, 0].set_title('Input')\n",
        "axs[0, 0].axis('off')\n",
        "axs[1, 0].axis('off')\n",
        "for i in range(n_show):\n",
        "    axs[0, i+1].imshow(to_display(comps_bl[i, sample_idx]).numpy())\n",
        "    axs[0, i+1].set_title(f'Token {token_indices[i]} baseline')\n",
        "    axs[0, i+1].axis('off')\n",
        "    axs[1, i+1].imshow(to_display(comps_nce[i, sample_idx]).numpy())\n",
        "    axs[1, i+1].set_title(f'Token {token_indices[i]} NCE')\n",
        "    axs[1, i+1].axis('off')\n",
        "plt.suptitle('Per-token RGB component (baseline vs NCE): texture vs structure?')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_tdsm_components_baseline_vs_nce.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What changed: visualizing object vs background differentiation\n",
        "\n",
        "NCE adds differential patterns (object vs background) but token-level *class* separation stays weak. Below we visualize **where** the change is (spatial difference) and **which tokens** become more object- vs background-selective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Spatial difference: where did NCE change the per-token reconstructions? (mean over tokens)\n",
        "diff_spatial = (tdsm_nce - tdsm_baseline).abs().mean(dim=1).cpu().numpy()\n",
        "fig, axs = plt.subplots(2, 4, figsize=(14, 6))\n",
        "for i in range(4):\n",
        "    axs[0, i].imshow(imgs_tdsm[i].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
        "    axs[0, i].set_title('Input' if i == 0 else '')\n",
        "    axs[0, i].axis('off')\n",
        "    im = axs[1, i].imshow(diff_spatial[i], cmap='hot')\n",
        "    axs[1, i].set_title('|NCE − Baseline| (mean over tokens)' if i == 0 else '')\n",
        "    axs[1, i].axis('off')\n",
        "plt.colorbar(im, ax=axs[1, :], shrink=0.6, label='Mean |diff|')\n",
        "plt.suptitle('Where NCE changed TDSM: object vs background')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_tdsm_spatial_diff.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Foreground vs background: per-token mean activation in center (object) vs border (background)\n",
        "H, W = IMAGE_SIZE, IMAGE_SIZE\n",
        "margin = 8\n",
        "obj_mask = np.zeros((H, W), dtype=np.float32)\n",
        "obj_mask[margin:H-margin, margin:W-margin] = 1.0\n",
        "bg_mask = 1.0 - obj_mask\n",
        "obj_mask = torch.from_numpy(obj_mask).to(DEVICE).view(1, 1, H, W)\n",
        "bg_mask = torch.from_numpy(bg_mask).to(DEVICE).view(1, 1, H, W)\n",
        "# tdsm: (B, 64, 32, 32)\n",
        "n_tokens = tdsm_baseline.size(1)\n",
        "obj_bl = (tdsm_baseline * obj_mask).sum(dim=(2, 3)) / (obj_mask.sum() + 1e-8)\n",
        "bg_bl = (tdsm_baseline * bg_mask).sum(dim=(2, 3)) / (bg_mask.sum() + 1e-8)\n",
        "obj_nce = (tdsm_nce * obj_mask).sum(dim=(2, 3)) / (obj_mask.sum() + 1e-8)\n",
        "bg_nce = (tdsm_nce * bg_mask).sum(dim=(2, 3)) / (bg_mask.sum() + 1e-8)\n",
        "sens_bl = (obj_bl - bg_bl).mean(dim=0).cpu().numpy()\n",
        "sens_nce = (obj_nce - bg_nce).mean(dim=0).cpu().numpy()\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axs[0].bar(np.arange(n_tokens), sens_bl, color='steelblue', alpha=0.8, label='Baseline')\n",
        "axs[0].set_xlabel('Token index')\n",
        "axs[0].set_ylabel('Object sensitivity (mean center − mean border)')\n",
        "axs[0].set_title('Baseline: token object vs background')\n",
        "axs[0].legend()\n",
        "axs[1].bar(np.arange(n_tokens), sens_nce, color='green', alpha=0.8, label='NCE')\n",
        "axs[1].set_xlabel('Token index')\n",
        "axs[1].set_ylabel('Object sensitivity')\n",
        "axs[1].set_title('NCE: token object vs background')\n",
        "axs[1].legend()\n",
        "plt.suptitle('Per-token object sensitivity (higher = more object-focused)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_tdsm_object_sensitivity.png', dpi=100)\n",
        "plt.show()\n",
        "# Scatter: baseline sensitivity vs NCE sensitivity per token (above diagonal = NCE more object-focused)\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(sens_bl, sens_nce, alpha=0.7)\n",
        "plt.plot([sens_bl.min(), sens_bl.max()], [sens_bl.min(), sens_bl.max()], 'r--', label='y=x')\n",
        "plt.xlabel('Baseline object sensitivity')\n",
        "plt.ylabel('NCE object sensitivity')\n",
        "plt.title('Per-token: NCE vs baseline (above line = NCE more object-focused)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('softnce_tdsm_sensitivity_scatter.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to improve semantics on top of NCE\n",
        "\n",
        "NCE gives **geometry and object vs background**; class separation needs an explicit **class signal**. Options:\n",
        "\n",
        "1. **Auxiliary classifier**: Add a small head on pooled φ (or on TDSM pooled) → class logits; train with **λ_cls × cross-entropy** alongside recon + NCE. Keeps NCE dominant but pulls the representation toward class-discriminative.\n",
        "2. **Supervised contrastive**: In the contrastive loss, add **same-class** pairs as extra positives (e.g. in-batch: anchor from image A, positive from image B if class(A)=class(B)), with a smaller weight than the geometric positives. Pushes same-class features closer.\n",
        "3. **Foreground-weighted NCE**: Mask or downweight contrastive pairs where the anchor/candidate fall in \"background\" (e.g. by a simple saliency or center prior), so the loss focuses on object regions; semantics can emerge more from object-level consistency.\n",
        "\n",
        "Below: minimal **auxiliary classifier** snippet you can plug into the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: auxiliary classifier on pooled φ to add class signal (run in training loop with lambda_cls * CE)\n",
        "NUM_CLASSES = 10\n",
        "pooled_dim = PROJ_DIM  # or QUERIES_DIM if you pool phi_raw\n",
        "aux_classifier = nn.Sequential(\n",
        "    nn.Linear(pooled_dim, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, NUM_CLASSES),\n",
        ").to(DEVICE)\n",
        "# In training: get phi_a (B, N_a, PROJ_DIM), pool over coords: phi_pool = phi_a.mean(dim=1)  # (B, PROJ_DIM)\n",
        "# logits_cls = aux_classifier(phi_pool)\n",
        "# loss_cls = F.cross_entropy(logits_cls, labels)\n",
        "# loss = loss_recon + lam_ctr * loss_softNCE + lambda_cls * loss_cls   # e.g. lambda_cls=0.05\n",
        "print('Aux classifier (pooled φ → 10 classes):', aux_classifier)\n",
        "print('To use: pool phi over anchors, add loss_cls with lambda_cls ~ 0.05 to total loss.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TDSM class/semantics: t-SNE (and PCA) colored by CIFAR-10 class\n",
        "\n",
        "Pool TDSM per image (e.g. spatial mean per token → 64-dim), then t-SNE (or PCA) and color by class. Compare **baseline** vs **NCE**: does NCE yield better class separation in the reconstruction space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect TDSM features for many val images (pooled: mean over space per token -> 64-dim per image)\n",
        "N_VAL_TDSM = min(500, len(val_ds))\n",
        "batch_size = 32\n",
        "all_feat_baseline = []\n",
        "all_feat_nce = []\n",
        "all_labels = []\n",
        "n_done = 0\n",
        "baseline_model.eval()\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "baseline_fourier.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        if n_done >= N_VAL_TDSM:\n",
        "            break\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        input_data, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "        tdsm_bl = get_tdsm(baseline_model, baseline_fourier, input_data, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "        tdsm_n = get_tdsm(model, fourier_encoder, input_data, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "        feat_bl = tdsm_bl.mean(dim=(2, 3)).cpu().numpy()\n",
        "        feat_n = tdsm_n.mean(dim=(2, 3)).cpu().numpy()\n",
        "        all_feat_baseline.append(feat_bl)\n",
        "        all_feat_nce.append(feat_n)\n",
        "        all_labels.append(labels.numpy())\n",
        "        n_done += imgs.size(0)\n",
        "X_baseline = np.concatenate(all_feat_baseline, axis=0)[:N_VAL_TDSM]\n",
        "X_nce = np.concatenate(all_feat_nce, axis=0)[:N_VAL_TDSM]\n",
        "y_all = np.concatenate(all_labels, axis=0)[:N_VAL_TDSM]\n",
        "print('TDSM features: baseline', X_baseline.shape, 'NCE', X_nce.shape, 'labels', y_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# t-SNE (and PCA) of TDSM features, colored by class; baseline vs NCE\n",
        "try:\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "    n_components_pca = min(50, X_baseline.shape[1], X_baseline.shape[0] - 1)\n",
        "    pca_bl = PCA(n_components=n_components_pca).fit(X_baseline)\n",
        "    pca_nce = PCA(n_components=n_components_pca).fit(X_nce)\n",
        "    X_bl_pca = pca_bl.transform(X_baseline)\n",
        "    X_nce_pca = pca_nce.transform(X_nce)\n",
        "    X_bl_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_bl_pca)\n",
        "    X_nce_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_nce_pca)\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    for ax, X_2d, title in [(axs[0], X_bl_tsne, 'Baseline (no NCE)'), (axs[1], X_nce_tsne, 'NCE-trained')]:\n",
        "        sc = ax.scatter(X_2d[:, 0], X_2d[:, 1], c=y_all, cmap='tab10', s=12, alpha=0.7)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('t-SNE 1')\n",
        "        ax.set_ylabel('t-SNE 2')\n",
        "    plt.colorbar(sc, ax=axs, label='Class', shrink=0.6)\n",
        "    plt.suptitle('TDSM features (pooled): t-SNE colored by CIFAR-10 class')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('softnce_tdsm_tsne_class.png', dpi=100)\n",
        "    plt.show()\n",
        "    # PCA 2D (faster, linear) for comparison\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    X_bl_pca2 = X_bl_pca[:, :2]\n",
        "    X_nce_pca2 = X_nce_pca[:, :2]\n",
        "    for ax, X_2d, title in [(axs[0], X_bl_pca2, 'Baseline PCA'), (axs[1], X_nce_pca2, 'NCE PCA')]:\n",
        "        ax.scatter(X_2d[:, 0], X_2d[:, 1], c=y_all, cmap='tab10', s=12, alpha=0.7)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('PC1')\n",
        "        ax.set_ylabel('PC2')\n",
        "    plt.suptitle('TDSM features: PCA first 2 components (class-colored)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('softnce_tdsm_pca_class.png', dpi=100)\n",
        "    plt.show()\n",
        "except ImportError as e:\n",
        "    print('Install sklearn for PCA/t-SNE:', e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
