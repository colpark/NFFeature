{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft InfoNCE on OmniField (CIFAR-10)\n",
    "\n",
    "**Minimal change** on top of the original OmniField reconstruction: add a **geometry-aware contrastive objective (Soft InfoNCE)** on probe-derived tokens from the neural field.\n",
    "\n",
    "- Sample anchor coords in view A and candidate coords in view B (B = affine augmentation of A with **known T**).\n",
    "- Build L2-normalized feature tokens φ(x) from the field (decoder hidden state → projection head).\n",
    "- Soft positives: \\(w_{ij} \\propto \\exp(-\\|x_j^B - T(x_i^A)\\|^2 / (2\\sigma^2))\\) (Gaussian kernel).\n",
    "- Loss: \\(\\mathcal{L} = \\mathcal{L}_{recon} + \\lambda_{ctr} \\mathcal{L}_{softNCE}\\).\n",
    "\n",
    "Defaults: N_a=256, N_b=1024, d=128, τ=0.1, σ=0.08, λ_ctr=0.1 (ramp 0→0.1 over 500 steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from nf_feature_models import (\n",
    "    CascadedPerceiverIO,\n",
    "    GaussianFourierFeatures,\n",
    "    create_coordinate_grid,\n",
    "    prepare_model_input,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "CKPT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint_best.pt')\n",
    "if not os.path.isfile(CKPT_PATH):\n",
    "    CKPT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint_last.pt')\n",
    "assert os.path.isfile(CKPT_PATH), f'No checkpoint in {CHECKPOINT_DIR}. Train AblationCIFAR10 first.'\n",
    "print(f'Device: {DEVICE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OmniField config (match checkpoint)\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "FOURIER_MAPPING_SIZE = 96\n",
    "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
    "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
    "QUERIES_DIM = POS_EMBED_DIM\n",
    "LOGITS_DIM = CHANNELS\n",
    "\n",
    "fourier_encoder = GaussianFourierFeatures(in_features=2, mapping_size=FOURIER_MAPPING_SIZE, scale=15.0).to(DEVICE)\n",
    "model = CascadedPerceiverIO(\n",
    "    input_dim=INPUT_DIM,\n",
    "    queries_dim=QUERIES_DIM,\n",
    "    logits_dim=LOGITS_DIM,\n",
    "    latent_dims=(256, 384, 512),\n",
    "    num_latents=(256, 256, 256),\n",
    "    decoder_ff=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "fourier_encoder.load_state_dict(ckpt['fourier_encoder_state_dict'], strict=False)\n",
    "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
    "print(f'Loaded {CKPT_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual(model, data):\n",
    "    residual = None\n",
    "    for block in model.encoder_blocks:\n",
    "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
    "    for sa_block in model.self_attn_blocks:\n",
    "        residual = sa_block[0](residual) + residual\n",
    "        residual = sa_block[1](residual) + residual\n",
    "    return residual\n",
    "\n",
    "def get_rgb_and_phi_raw(model, queries, residual):\n",
    "    x = model.decoder_cross_attn(queries, context=residual)\n",
    "    x = x + queries\n",
    "    if model.decoder_ff is not None:\n",
    "        x = x + model.decoder_ff(x)\n",
    "    phi_raw = x\n",
    "    rgb = model.to_logits(x)\n",
    "    return rgb, phi_raw\n",
    "\n",
    "def sample_gt_at_coords(images, coords):\n",
    "    B, C, H, W = images.shape\n",
    "    N = coords.shape[1]\n",
    "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
    "    sampled = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    return sampled.squeeze(2).permute(0, 2, 1)\n",
    "\n",
    "def make_grid_2d(h, w, device):\n",
    "    y = torch.linspace(-1, 1, h, device=device)\n",
    "    x = torch.linspace(-1, 1, w, device=device)\n",
    "    g = torch.stack(torch.meshgrid(y, x, indexing='ij'), dim=-1)\n",
    "    return g.reshape(-1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine augmentation with known T (A → B)\n",
    "\n",
    "We apply an affine transform to get view B and store the forward map T so that for any coord in A we have T(x) in B space. Coords are in [-1, 1]²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_affine_params(batch_size, device, scale_range=(0.8, 1.0), max_translate=0.1, max_angle_deg=15):\n",
    "    '''Returns T (A→B): R (B,2,2), t (B,2). T(p) = p @ R.T + t (row vectors).'''\n",
    "    angle = (torch.rand(batch_size, device=device) * 2 - 1) * (max_angle_deg * math.pi / 180)\n",
    "    scale = scale_range[0] + torch.rand(batch_size, device=device) * (scale_range[1] - scale_range[0])\n",
    "    tx = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
    "    ty = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
    "    c, s = torch.cos(angle), torch.sin(angle)\n",
    "    R = torch.stack([c * scale, -s * scale, s * scale, c * scale], dim=-1).view(batch_size, 2, 2)\n",
    "    t = torch.stack([tx, ty], dim=1)\n",
    "    return R, t\n",
    "\n",
    "def apply_affine_to_coords(coords, R, t):\n",
    "    '''coords (B, N, 2), R (B, 2, 2), t (B, 2). T(p)=R@p+t. Returns (B, N, 2) in B space.'''\n",
    "    return torch.einsum('bed,bnd->bne', R, coords) + t.unsqueeze(1)\n",
    "\n",
    "def apply_affine_to_image(images, R, t, align_corners=True):\n",
    "    '''View B = T(A). We need grid such that B is sampled from A at T_inv(B_coords). R,t = T.'''\n",
    "    B, C, H, W = images.shape\n",
    "    R_inv = torch.inverse(R)\n",
    "    t_exp = t.unsqueeze(1)\n",
    "    theta = torch.cat([R_inv, -(R_inv @ t.unsqueeze(2))], dim=2)\n",
    "    grid = F.affine_grid(theta, images.size(), align_corners=align_corners)\n",
    "    out = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=align_corners)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward(self, z):\n",
    "        return F.normalize(self.proj(z), dim=-1)\n",
    "\n",
    "PROJ_DIM = 128\n",
    "projection_head = ProjectionHead(QUERIES_DIM, PROJ_DIM).to(DEVICE)\n",
    "print(projection_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_infonce_loss(phi_a, phi_b, coords_a, coords_b, R, t, tau=0.1, sigma=0.08):\n",
    "    '''\n",
    "    phi_a (B, N_a, d), phi_b (B, N_b, d) L2-normalized.\n",
    "    coords_a (B, N_a, 2), coords_b (B, N_b, 2). T: A->B given by R (B,2,2), t (B,2).\n",
    "    Soft weights w_ij = exp(-||x_j^B - T(x_i^A)||^2 / (2*sigma^2)), normalized over j.\n",
    "    '''\n",
    "    B, N_a, _ = phi_a.shape\n",
    "    N_b = phi_b.size(1)\n",
    "    logits = torch.bmm(phi_a, phi_b.transpose(1, 2)) / tau\n",
    "    xi_mapped = apply_affine_to_coords(coords_a, R, t)\n",
    "    sqd = ((coords_b.unsqueeze(1) - xi_mapped.unsqueeze(2)) ** 2).sum(-1)\n",
    "    w = torch.exp(-sqd / (2 * sigma ** 2))\n",
    "    w = w / (w.sum(dim=2, keepdim=True) + 1e-8)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    loss = -(w * log_probs).sum(-1).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'subset_size': 10000,\n",
    "    'batch_size': 32,\n",
    "    'N_a': 256,\n",
    "    'N_b': 1024,\n",
    "    'tau': 0.1,\n",
    "    'sigma': 0.08,\n",
    "    'lambda_ctr': 0.1,\n",
    "    'ramp_steps': 500,\n",
    "    'epochs': 5,\n",
    "    'lr': 1e-3,\n",
    "    'freeze_backbone': False,\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_sub = Subset(train_ds, list(range(min(cfg['subset_size'], len(train_ds)))))\n",
    "train_loader = DataLoader(train_sub, batch_size=cfg['batch_size'], shuffle=True, num_workers=0)\n",
    "val_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop: recon + Soft InfoNCE (λ ramp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(projection_head.parameters())\n",
    "if not cfg.get('freeze_backbone'):\n",
    "    params += list(model.parameters()) + list(fourier_encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=cfg['lr'])\n",
    "global_step = [0]\n",
    "\n",
    "def get_lambda_ctr(step):\n",
    "    ramp = cfg['ramp_steps']\n",
    "    if step >= ramp:\n",
    "        return cfg['lambda_ctr']\n",
    "    return cfg['lambda_ctr'] * (step / ramp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_psnr(model, fourier_encoder, loader, device, grid_size=32):\n",
    "    model.eval()\n",
    "    fourier_encoder.eval()\n",
    "    grid = make_grid_2d(grid_size, grid_size, device)\n",
    "    mse_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            B = imgs.size(0)\n",
    "            input_data, _, _ = prepare_model_input(imgs, grid, fourier_encoder)\n",
    "            residual = get_residual(model, input_data)\n",
    "            coords_batch = grid.unsqueeze(0).expand(B, -1, -1)\n",
    "            queries = fourier_encoder(coords_batch)\n",
    "            rgb, _ = get_rgb_and_phi_raw(model, queries, residual)\n",
    "            gt = sample_gt_at_coords(imgs, coords_batch)\n",
    "            mse_sum += F.mse_loss(rgb, gt, reduction='sum').item()\n",
    "            n += B * grid.size(0)\n",
    "    mse = mse_sum / max(n, 1)\n",
    "    return 10 * math.log10(1.0 / (mse + 1e-10))\n",
    "\n",
    "print(f'Baseline val PSNR: {eval_psnr(model, fourier_encoder, val_loader, DEVICE):.2f} dB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_log = []\n",
    "for ep in range(cfg['epochs']):\n",
    "    model.train()\n",
    "    fourier_encoder.train()\n",
    "    projection_head.train()\n",
    "    total_recon = 0.0\n",
    "    total_ctr = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        B, C, H, W = imgs.shape\n",
    "        R, t = sample_affine_params(B, DEVICE)\n",
    "        imgs_b = apply_affine_to_image(imgs, R, t)\n",
    "        N_a, N_b = cfg['N_a'], cfg['N_b']\n",
    "        anchors_a = (torch.rand(B, N_a, 2, device=DEVICE) * 2 - 1)\n",
    "        candidates_b = (torch.rand(B, N_b, 2, device=DEVICE) * 2 - 1)\n",
    "        input_a, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
    "        input_b, _, _ = prepare_model_input(imgs_b, coords_32, fourier_encoder)\n",
    "        residual_a = get_residual(model, input_a)\n",
    "        residual_b = get_residual(model, input_b)\n",
    "        queries_a = fourier_encoder(anchors_a)\n",
    "        queries_b = fourier_encoder(candidates_b)\n",
    "        rgb_a, phi_raw_a = get_rgb_and_phi_raw(model, queries_a, residual_a)\n",
    "        _, phi_raw_b = get_rgb_and_phi_raw(model, queries_b, residual_b)\n",
    "        phi_a = projection_head(phi_raw_a)\n",
    "        phi_b = projection_head(phi_raw_b)\n",
    "        loss_ctr = soft_infonce_loss(phi_a, phi_b, anchors_a, candidates_b, R, t, tau=cfg['tau'], sigma=cfg['sigma'])\n",
    "        coords_full = coords_32.unsqueeze(0).expand(B, -1, -1)\n",
    "        queries_full = fourier_encoder(coords_full)\n",
    "        rgb_full, _ = get_rgb_and_phi_raw(model, queries_full, residual_a)\n",
    "        gt_full = sample_gt_at_coords(imgs, coords_full)\n",
    "        loss_recon = F.mse_loss(rgb_full, gt_full)\n",
    "        lam = get_lambda_ctr(global_step[0])\n",
    "        loss = loss_recon + lam * loss_ctr\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step[0] += 1\n",
    "        total_recon += loss_recon.item()\n",
    "        total_ctr += loss_ctr.item()\n",
    "    psnr = eval_psnr(model, fourier_encoder, val_loader, DEVICE)\n",
    "    psnr_log.append(psnr)\n",
    "    print(f'Epoch {ep+1} recon: {total_recon/len(train_loader):.4f} ctr: {total_ctr/len(train_loader):.4f} val PSNR: {psnr:.2f} dB')\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(psnr_log, 'o-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Val PSNR (dB)')\n",
    "plt.title('Reconstruction PSNR (baseline vs +Soft InfoNCE)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('softnce_psnr.png', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "1. **Attention heatmap (soft weights)** for a few anchors: w_j over B's grid.\n",
    "2. **Coordinate error**: for each anchor, argmax_j similarity → coord; distance to T(x_i^A).\n",
    "3. **2D embedding** (PCA/t-SNE) of tokens colored by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "fourier_encoder.eval()\n",
    "projection_head.eval()\n",
    "imgs, labels = next(iter(val_loader))\n",
    "imgs = imgs[:4].to(DEVICE)\n",
    "labels = labels[:4]\n",
    "B = 4\n",
    "R, t = sample_affine_params(B, DEVICE)\n",
    "imgs_b = apply_affine_to_image(imgs, R, t)\n",
    "grid_b = make_grid_2d(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
    "N_a_vis = 64\n",
    "anchors_a = (torch.rand(B, N_a_vis, 2, device=DEVICE) * 2 - 1)\n",
    "with torch.no_grad():\n",
    "    input_a, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
    "    input_b, _, _ = prepare_model_input(imgs_b, coords_32, fourier_encoder)\n",
    "    residual_a = get_residual(model, input_a)\n",
    "    residual_b = get_residual(model, input_b)\n",
    "    coords_b_batch = grid_b.unsqueeze(0).expand(B, -1, -1)\n",
    "    queries_a = fourier_encoder(anchors_a)\n",
    "    queries_b = fourier_encoder(coords_b_batch)\n",
    "    _, phi_raw_a = get_rgb_and_phi_raw(model, queries_a, residual_a)\n",
    "    _, phi_raw_b = get_rgb_and_phi_raw(model, queries_b, residual_b)\n",
    "    phi_a = projection_head(phi_raw_a)\n",
    "    phi_b = projection_head(phi_raw_b)\n",
    "    logits = torch.bmm(phi_a, phi_b.transpose(1, 2)) / cfg['tau']\n",
    "    xi_mapped = apply_affine_to_coords(anchors_a, R, t)\n",
    "    sqd = ((coords_b_batch.unsqueeze(1) - xi_mapped.unsqueeze(2)) ** 2).sum(-1)\n",
    "    w = torch.exp(-sqd / (2 * cfg['sigma'] ** 2))\n",
    "    w = w / (w.sum(dim=2, keepdim=True) + 1e-8)\n",
    "b_show = 0\n",
    "n_anchors_show = 4\n",
    "fig, axs = plt.subplots(2, n_anchors_show, figsize=(12, 5))\n",
    "for i in range(n_anchors_show):\n",
    "    heat = w[b_show, i].cpu().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "    axs[0, i].imshow(heat, cmap='hot')\n",
    "    axs[0, i].set_title(f'Anchor {i} soft weights')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].imshow(imgs_b[b_show].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
    "    axs[1, i].axis('off')\n",
    "plt.suptitle('Soft positive weights w_j over view B (one image)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('softnce_heatmap.png', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    best_j = logits[b_show].argmax(dim=1)\n",
    "    pred_coords = coords_b_batch[b_show][best_j]\n",
    "    gt_coords = xi_mapped[b_show]\n",
    "    err = (pred_coords - gt_coords).norm(dim=-1).cpu().numpy()\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.violinplot([err], positions=[0], showmeans=True)\n",
    "plt.ylabel('Coord error (argmax candidate vs T(x_i^A))')\n",
    "plt.title('Coordinate error (normalized space)')\n",
    "plt.xticks([0], ['Soft InfoNCE'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('softnce_coord_error.png', dpi=100)\n",
    "plt.show()\n",
    "print(f'Mean coord error: {err.mean():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 500\n",
    "imgs_e, labels_e = next(iter(val_loader))\n",
    "imgs_e = imgs_e[:n_embed].to(DEVICE)\n",
    "labels_e = labels_e[:n_embed].cpu().numpy()\n",
    "grid_flat = make_grid_2d(8, 8, DEVICE)\n",
    "coords_embed = grid_flat.unsqueeze(0).expand(n_embed, -1, -1)\n",
    "with torch.no_grad():\n",
    "    input_embed, _, _ = prepare_model_input(imgs_e, coords_32, fourier_encoder)\n",
    "    residual_embed = get_residual(model, input_embed)\n",
    "    queries_embed = fourier_encoder(coords_embed)\n",
    "    _, phi_raw_embed = get_rgb_and_phi_raw(model, queries_embed, residual_embed)\n",
    "    phi_embed = projection_head(phi_raw_embed)\n",
    "feats = phi_embed.cpu().numpy().reshape(-1, PROJ_DIM)\n",
    "label_tile = np.repeat(labels_e[:, None], 64, axis=1).reshape(-1)\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    feats_2d = pca.fit_transform(feats)\n",
    "except ImportError:\n",
    "    feats_2d = np.random.randn(feats.shape[0], 2)\n",
    "    print('Install sklearn for PCA; using random 2D for demo.')\n",
    "plt.figure(figsize=(6, 5))\n",
    "sc = plt.scatter(feats_2d[:, 0], feats_2d[:, 1], c=label_tile, cmap='tab10', s=1, alpha=0.6)\n",
    "plt.colorbar(sc, label='Class')\n",
    "plt.title('Token embedding PCA (colored by CIFAR-10 class)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('softnce_embedding.png', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Single intervention: **Soft InfoNCE** on probe-derived tokens (decoder hidden → projection head).\n",
    "- Geometry-aware soft positives with known affine T; λ_ctr ramped to avoid destabilizing recon.\n",
    "- Visualizations: PSNR curve, attention heatmap, coordinate error, 2D embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
