{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sparse Context: Evaluation & Visualizations\n",
        "\n",
        "Load **checkpoint_sparse_best.pt** (or _last), evaluate PSNR under **sparse** vs **full** context, and visualize reconstructions and context-fraction sensitivity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import CascadedPerceiverIO, GaussianFourierFeatures, create_coordinate_grid\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT_DIR = 'checkpoints'\n",
        "CKPT_PREFIX = 'checkpoint_sparse'\n",
        "CKPT_PATH = os.path.join(CHECKPOINT_DIR, CKPT_PREFIX + '_best.pt')\n",
        "if not os.path.isfile(CKPT_PATH):\n",
        "    CKPT_PATH = os.path.join(CHECKPOINT_DIR, CKPT_PREFIX + '_last.pt')\n",
        "assert os.path.isfile(CKPT_PATH), f'No {CKPT_PREFIX}_*.pt in {CHECKPOINT_DIR}. Run SparseContext_CIFAR10.ipynb first.'\n",
        "print('Device:', DEVICE, 'Checkpoint:', CKPT_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "CONTEXT_FRAC_TRAIN = 0.2\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "N_FULL = coords_32.size(0)\n",
        "N_SPARSE = max(64, int(N_FULL * CONTEXT_FRAC_TRAIN))\n",
        "\n",
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    return F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True).squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def prepare_sparse_context(images, coords_full, fourier_encoder, num_sparse, device):\n",
        "    B = images.size(0)\n",
        "    idx = torch.randperm(coords_full.size(0), device=device)[:num_sparse]\n",
        "    coords_sparse = coords_full[idx]\n",
        "    pixels_sparse = sample_gt_at_coords(images, coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos_sparse = fourier_encoder(coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels_sparse, pos_sparse], dim=-1)\n",
        "\n",
        "def prepare_full_context(images, coords_full, fourier_encoder):\n",
        "    input_full, _, _ = prepare_model_input(images, coords_full, fourier_encoder)\n",
        "    return input_full\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nf_feature_models import prepare_model_input\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
        "fourier_encoder.load_state_dict(ckpt['fourier_encoder_state_dict'], strict=False)\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "print('Loaded', CKPT_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def eval_psnr(model, fourier_encoder, loader, device, use_sparse=True, num_sparse=None):\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    n_sparse = num_sparse if num_sparse is not None else N_SPARSE\n",
        "    mse_sum, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            if use_sparse:\n",
        "                input_data = prepare_sparse_context(imgs, coords_32, fourier_encoder, n_sparse, device)\n",
        "            else:\n",
        "                input_data = prepare_full_context(imgs, coords_32, fourier_encoder)\n",
        "            target_pixels = rearrange(imgs, 'b c h w -> b (h w) c')\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            reconstructed = model(input_data, queries=queries_full)\n",
        "            mse_sum += F.mse_loss(reconstructed, target_pixels, reduction='sum').item()\n",
        "            n += B * N_FULL\n",
        "    mse = mse_sum / max(n, 1)\n",
        "    return 10 * math.log10(1.0 / (mse + 1e-10))\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "psnr_sparse = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=True)\n",
        "psnr_full = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=False)\n",
        "print(f'PSNR (sparse context {N_SPARSE}): {psnr_sparse:.2f} dB')\n",
        "print(f'PSNR (full context): {psnr_full:.2f} dB')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reconstruction gallery: sparse vs full context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "imgs, _ = next(iter(test_loader))\n",
        "imgs = imgs[:8].to(DEVICE)\n",
        "B = imgs.size(0)\n",
        "with torch.no_grad():\n",
        "    input_sparse = prepare_sparse_context(imgs, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
        "    input_full = prepare_full_context(imgs, coords_32, fourier_encoder)\n",
        "    queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "    recon_sparse = model(input_sparse, queries=queries_full)\n",
        "    recon_full = model(input_full, queries=queries_full)\n",
        "recon_sparse = rearrange(recon_sparse, 'b (h w) c -> b c h w', h=IMAGE_SIZE, w=IMAGE_SIZE)\n",
        "recon_full = rearrange(recon_full, 'b (h w) c -> b c h w', h=IMAGE_SIZE, w=IMAGE_SIZE)\n",
        "\n",
        "def to_vis(x):\n",
        "    return (x.cpu().clamp(0, 1).permute(0, 2, 3, 1).numpy())\n",
        "fig, axs = plt.subplots(3, 8, figsize=(16, 6))\n",
        "for i in range(8):\n",
        "    axs[0, i].imshow(to_vis(imgs[i])); axs[0, i].set_title('GT' if i==0 else ''); axs[0, i].axis('off')\n",
        "    axs[1, i].imshow(to_vis(recon_sparse[i])); axs[1, i].set_title('Sparse ctx' if i==0 else ''); axs[1, i].axis('off')\n",
        "    axs[2, i].imshow(to_vis(recon_full[i])); axs[2, i].set_title('Full ctx' if i==0 else ''); axs[2, i].axis('off')\n",
        "plt.suptitle('Sparse-context model: recon with sparse vs full context')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_gallery.png', dpi=100)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PSNR vs context fraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fracs = [0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\n",
        "psnrs = []\n",
        "for f in fracs:\n",
        "    n = max(64, int(N_FULL * f))\n",
        "    if f >= 1.0:\n",
        "        p = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=False)\n",
        "    else:\n",
        "        p = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=True, num_sparse=n)\n",
        "    psnrs.append(p)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fracs, psnrs, 'o-')\n",
        "plt.xlabel('Context fraction')\n",
        "plt.ylabel('PSNR (dB)')\n",
        "plt.title('Sparse-context model: PSNR vs context fraction at eval')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_psnr_vs_frac.png', dpi=100)\n",
        "plt.show()\n",
        "print('Context frac -> PSNR:', list(zip(fracs, [round(p,2) for p in psnrs])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where the model \"looks\": sparse context positions (one sample)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(123)\n",
        "img_one = imgs[:1]\n",
        "n_vis = 128\n",
        "# Use fixed indices so mask matches the context we feed to the model\n",
        "idx_vis = torch.randperm(coords_32.size(0), device=DEVICE)[:n_vis]\n",
        "coords_sparse = coords_32[idx_vis]\n",
        "pixels_sparse = sample_gt_at_coords(img_one, coords_sparse.unsqueeze(0).expand(1, -1, -1))\n",
        "pos_sparse = fourier_encoder(coords_sparse.unsqueeze(0).expand(1, -1, -1))\n",
        "input_vis = torch.cat([pixels_sparse, pos_sparse], dim=-1)\n",
        "# Mask: linear index -> (row, col) for 32x32 row-major grid\n",
        "mask = torch.zeros(1, 1, IMAGE_SIZE, IMAGE_SIZE, device=DEVICE)\n",
        "for i in range(n_vis):\n",
        "    row, col = idx_vis[i].item() // IMAGE_SIZE, idx_vis[i].item() % IMAGE_SIZE\n",
        "    mask[0, 0, row, col] = 1.0\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
        "axs[0].imshow(to_vis(img_one[0])); axs[0].set_title('Image'); axs[0].axis('off')\n",
        "axs[1].imshow(mask[0, 0].cpu().numpy(), cmap='hot'); axs[1].set_title(f'Sparse context positions (n={n_vis})'); axs[1].axis('off')\n",
        "with torch.no_grad():\n",
        "    q = fourier_encoder(coords_32.unsqueeze(0))\n",
        "    recon_one = model(input_vis, queries=q)\n",
        "recon_one = rearrange(recon_one, 'b (h w) c -> b c h w', h=IMAGE_SIZE, w=IMAGE_SIZE)\n",
        "axs[2].imshow(to_vis(recon_one[0])); axs[2].set_title('Reconstruction'); axs[2].axis('off')\n",
        "plt.suptitle('Example: which positions were given as context')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_positions.png', dpi=100)\n",
        "plt.show()\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}