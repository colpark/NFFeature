{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sparse Context: Evaluation & Visualizations\n",
        "\n",
        "Load **checkpoint_sparse_best.pt** (or _last), evaluate PSNR under **sparse** vs **full** context, and visualize reconstructions and context-fraction sensitivity.\n",
        "\n",
        "Comparison is **sparse-context recon** (partial image → full recon) vs **full-context recon** (whole image → full recon). If you trained with USE_NCE=False, both are recon-only; the extra plots (View A/B, Retrieval@ε, TDSM, etc.) still contrast the two setups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import CascadedPerceiverIO, GaussianFourierFeatures, create_coordinate_grid, prepare_model_input\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT_DIR = 'checkpoints'\n",
        "CKPT_PREFIX = 'checkpoint_sparse'\n",
        "CKPT_PATH = os.path.join(CHECKPOINT_DIR, CKPT_PREFIX + '_best.pt')\n",
        "if not os.path.isfile(CKPT_PATH):\n",
        "    CKPT_PATH = os.path.join(CHECKPOINT_DIR, CKPT_PREFIX + '_last.pt')\n",
        "assert os.path.isfile(CKPT_PATH), f'No {CKPT_PREFIX}_*.pt in {CHECKPOINT_DIR}. Run SparseContext_CIFAR10.ipynb first.'\n",
        "print('Device:', DEVICE, 'Checkpoint:', CKPT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "CONTEXT_FRAC_TRAIN = 0.2\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "N_FULL = coords_32.size(0)\n",
        "N_SPARSE = max(64, int(N_FULL * CONTEXT_FRAC_TRAIN))\n",
        "\n",
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    return F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True).squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def prepare_sparse_context(images, coords_full, fourier_encoder, num_sparse, device):\n",
        "    B = images.size(0)\n",
        "    idx = torch.randperm(coords_full.size(0), device=device)[:num_sparse]\n",
        "    coords_sparse = coords_full[idx]\n",
        "    pixels_sparse = sample_gt_at_coords(images, coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos_sparse = fourier_encoder(coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels_sparse, pos_sparse], dim=-1)\n",
        "\n",
        "def prepare_full_context(images, coords_full, fourier_encoder):\n",
        "    input_full, _, _ = prepare_model_input(images, coords_full, fourier_encoder)\n",
        "    return input_full\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "PROJ_DIM = 128\n",
        "projection_head = nn.Linear(QUERIES_DIM, PROJ_DIM).to(DEVICE)\n",
        "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
        "fourier_encoder.load_state_dict(ckpt['fourier_encoder_state_dict'], strict=False)\n",
        "if 'projection_head_state_dict' in ckpt:\n",
        "    projection_head.load_state_dict(ckpt['projection_head_state_dict'], strict=False)\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "projection_head.eval()\n",
        "print('Loaded', CKPT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_psnr(model, fourier_encoder, loader, device, use_sparse=True, num_sparse=None):\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    n_sparse = num_sparse if num_sparse is not None else N_SPARSE\n",
        "    mse_sum, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            if use_sparse:\n",
        "                input_data = prepare_sparse_context(imgs, coords_32, fourier_encoder, n_sparse, device)\n",
        "            else:\n",
        "                input_data = prepare_full_context(imgs, coords_32, fourier_encoder)\n",
        "            target_pixels = rearrange(imgs, 'b c h w -> b (h w) c')\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            reconstructed = model(input_data, queries=queries_full)\n",
        "            mse_sum += F.mse_loss(reconstructed, target_pixels, reduction='sum').item()\n",
        "            n += B * N_FULL\n",
        "    mse = mse_sum / max(n, 1)\n",
        "    return 10 * math.log10(1.0 / (mse + 1e-10))\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "psnr_sparse = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=True)\n",
        "psnr_full = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=False)\n",
        "print(f'PSNR (sparse context {N_SPARSE}): {psnr_sparse:.2f} dB')\n",
        "print(f'PSNR (full context): {psnr_full:.2f} dB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reconstruction gallery: sparse vs full context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs, _ = next(iter(test_loader))\n",
        "imgs = imgs[:8].to(DEVICE)\n",
        "B = imgs.size(0)\n",
        "with torch.no_grad():\n",
        "    input_sparse = prepare_sparse_context(imgs, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
        "    input_full = prepare_full_context(imgs, coords_32, fourier_encoder)\n",
        "    queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "    recon_sparse = model(input_sparse, queries=queries_full)\n",
        "    recon_full = model(input_full, queries=queries_full)\n",
        "recon_sparse = rearrange(recon_sparse, 'b (h w) c -> b c h w', h=IMAGE_SIZE, w=IMAGE_SIZE)\n",
        "recon_full = rearrange(recon_full, 'b (h w) c -> b c h w', h=IMAGE_SIZE, w=IMAGE_SIZE)\n",
        "\n",
        "def to_vis(x):\n",
        "    if x.dim() == 3:\n",
        "        x = x.unsqueeze(0)\n",
        "    out = x.cpu().clamp(0, 1).permute(0, 2, 3, 1).numpy()\n",
        "    return out[0] if out.shape[0] == 1 else out\n",
        "fig, axs = plt.subplots(3, 8, figsize=(16, 6))\n",
        "for i in range(8):\n",
        "    axs[0, i].imshow(to_vis(imgs[i])); axs[0, i].set_title('GT' if i==0 else ''); axs[0, i].axis('off')\n",
        "    axs[1, i].imshow(to_vis(recon_sparse[i])); axs[1, i].set_title('Sparse ctx' if i==0 else ''); axs[1, i].axis('off')\n",
        "    axs[2, i].imshow(to_vis(recon_full[i])); axs[2, i].set_title('Full ctx' if i==0 else ''); axs[2, i].axis('off')\n",
        "plt.suptitle('Sparse-context model: recon with sparse vs full context')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_gallery.png', dpi=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PSNR vs context fraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fracs = [0.1, 0.15, 0.2, 0.3, 0.5, 1.0]\n",
        "psnrs = []\n",
        "for f in fracs:\n",
        "    n = max(64, int(N_FULL * f))\n",
        "    if f >= 1.0:\n",
        "        p = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=False)\n",
        "    else:\n",
        "        p = eval_psnr(model, fourier_encoder, test_loader, DEVICE, use_sparse=True, num_sparse=n)\n",
        "    psnrs.append(p)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fracs, psnrs, 'o-')\n",
        "plt.xlabel('Context fraction')\n",
        "plt.ylabel('PSNR (dB)')\n",
        "plt.title('Sparse-context model: PSNR vs context fraction at eval')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_psnr_vs_frac.png', dpi=100)\n",
        "plt.show()\n",
        "print('Context frac -> PSNR:', list(zip(fracs, [round(p,2) for p in psnrs])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where the model \"looks\": sparse context positions (one sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "img_one = imgs[:1]\n",
        "n_vis = 128\n",
        "# Use fixed indices so mask matches the context we feed to the model\n",
        "idx_vis = torch.randperm(coords_32.size(0), device=DEVICE)[:n_vis]\n",
        "coords_sparse = coords_32[idx_vis]\n",
        "pixels_sparse = sample_gt_at_coords(img_one, coords_sparse.unsqueeze(0).expand(1, -1, -1))\n",
        "pos_sparse = fourier_encoder(coords_sparse.unsqueeze(0).expand(1, -1, -1))\n",
        "input_vis = torch.cat([pixels_sparse, pos_sparse], dim=-1)\n",
        "# Mask: linear index -> (row, col) for 32x32 row-major grid\n",
        "mask = torch.zeros(1, 1, IMAGE_SIZE, IMAGE_SIZE, device=DEVICE)\n",
        "for i in range(n_vis):\n",
        "    row, col = idx_vis[i].item() // IMAGE_SIZE, idx_vis[i].item() % IMAGE_SIZE\n",
        "    mask[0, 0, row, col] = 1.0\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
        "axs[0].imshow(to_vis(img_one[0])); axs[0].set_title('Image'); axs[0].axis('off')\n",
        "axs[1].imshow(mask[0, 0].cpu().numpy(), cmap='hot'); axs[1].set_title(f'Sparse context positions (n={n_vis})'); axs[1].axis('off')\n",
        "with torch.no_grad():\n",
        "    q = fourier_encoder(coords_32.unsqueeze(0))\n",
        "    recon_one = model(input_vis, queries=q)\n",
        "recon_one = rearrange(recon_one, 'b (h w) c -> b c h w', h=IMAGE_SIZE, w=IMAGE_SIZE)\n",
        "axs[2].imshow(to_vis(recon_one[0])); axs[2].set_title('Reconstruction'); axs[2].axis('off')\n",
        "plt.suptitle('Example: which positions were given as context')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_positions.png', dpi=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full-context recon (baseline) vs Sparse-context recon\n",
        "\n",
        "Load the **full-image recon** baseline (checkpoint_best.pt), then run the same correspondence and TDSM visualizations to contrast with the **sparse-context** model (partial image as input). No NCE required: this is partial vs full recon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load full-context recon baseline (whole image as input)\n",
        "CKPT_BASELINE = os.path.join(CHECKPOINT_DIR, 'checkpoint_best.pt')\n",
        "if not os.path.isfile(CKPT_BASELINE):\n",
        "    CKPT_BASELINE = os.path.join(CHECKPOINT_DIR, 'checkpoint_last.pt')\n",
        "baseline_model = baseline_fourier = None\n",
        "if os.path.isfile(CKPT_BASELINE):\n",
        "    baseline_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    baseline_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_bl = torch.load(CKPT_BASELINE, map_location=DEVICE)\n",
        "    baseline_model.load_state_dict(ckpt_bl['model_state_dict'], strict=False)\n",
        "    baseline_fourier.load_state_dict(ckpt_bl['fourier_encoder_state_dict'], strict=False)\n",
        "    baseline_model.eval()\n",
        "    baseline_fourier.eval()\n",
        "    print('Loaded baseline (full recon):', CKPT_BASELINE)\n",
        "else:\n",
        "    print('No baseline checkpoint found; comparison plots will be sparse-context only.')\n",
        "\n",
        "# Optional: Full-context + NCE-trained (from SoftInfoNCE_OmniField_CIFAR10.ipynb; save that model to checkpoint_nce_best.pt)\n",
        "CKPT_NCE = os.path.join(CHECKPOINT_DIR, 'checkpoint_nce_best.pt')\n",
        "if not os.path.isfile(CKPT_NCE):\n",
        "    CKPT_NCE = os.path.join(CHECKPOINT_DIR, 'softnce_best.pt')\n",
        "nce_model = nce_fourier = None\n",
        "if os.path.isfile(CKPT_NCE):\n",
        "    nce_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    nce_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_nce = torch.load(CKPT_NCE, map_location=DEVICE)\n",
        "    nce_model.load_state_dict(ckpt_nce['model_state_dict'], strict=False)\n",
        "    nce_fourier.load_state_dict(ckpt_nce['fourier_encoder_state_dict'], strict=False)\n",
        "    nce_model.eval()\n",
        "    nce_fourier.eval()\n",
        "    print('Loaded Full context + NCE:', CKPT_NCE)\n",
        "else:\n",
        "    print('No NCE checkpoint (checkpoint_nce_best.pt / softnce_best.pt); run SoftInfoNCE notebook and save to include.')\n",
        "\n",
        "def get_residual(model, data):\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def get_rgb_and_phi_raw(model, queries, residual):\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return model.to_logits(x), x\n",
        "\n",
        "def decoder_forward(model, queries, context):\n",
        "    x = model.decoder_cross_attn(queries, context=context)\n",
        "    x = x + queries\n",
        "    if getattr(model, 'decoder_ff', None) is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return model.to_logits(x)\n",
        "\n",
        "def sample_affine_params(batch_size, device, scale_range=(0.85, 1.0), max_translate=0.1, max_angle_deg=12):\n",
        "    angle = (torch.rand(batch_size, device=device) * 2 - 1) * (max_angle_deg * math.pi / 180)\n",
        "    scale = scale_range[0] + torch.rand(batch_size, device=device) * (scale_range[1] - scale_range[0])\n",
        "    tx = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
        "    ty = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
        "    c, s = torch.cos(angle), torch.sin(angle)\n",
        "    R = torch.stack([c*scale, -s*scale, s*scale, c*scale], dim=-1).view(batch_size, 2, 2)\n",
        "    t = torch.stack([tx, ty], dim=1)\n",
        "    return R, t\n",
        "\n",
        "def apply_affine_to_coords(coords, R, t):\n",
        "    return torch.einsum('bed,bnd->bne', R, coords) + t.unsqueeze(1)\n",
        "\n",
        "def apply_affine_to_image(images, R, t):\n",
        "    R_inv = torch.inverse(R)\n",
        "    theta = torch.cat([R_inv, -(R_inv @ t.unsqueeze(2))], dim=2)\n",
        "    grid = F.affine_grid(theta, images.size(), align_corners=True)\n",
        "    return F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
        "\n",
        "def norm_to_pixel(coords_norm, h, w):\n",
        "    y, x = coords_norm[..., 0], coords_norm[..., 1]\n",
        "    row = (y + 1) / 2 * (h - 1)\n",
        "    col = (x + 1) / 2 * (w - 1)\n",
        "    return col.cpu().numpy(), row.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two-view batch and correspondence quantities (sparse-context vs full-context baseline)\n",
        "N_ANCHORS, N_CANDIDATES = 128, 512\n",
        "TAU, SIGMA = 0.1, 0.08\n",
        "b_show = 0\n",
        "n_anchors_show = 4\n",
        "\n",
        "imgs_viz, _ = next(iter(test_loader))\n",
        "imgs_viz = imgs_viz[:4].to(DEVICE)\n",
        "B = imgs_viz.size(0)\n",
        "R, t = sample_affine_params(B, DEVICE)\n",
        "imgs_b = apply_affine_to_image(imgs_viz, R, t)\n",
        "\n",
        "# Sparse model: sparse context for A and B\n",
        "input_sparse_a = prepare_sparse_context(imgs_viz, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
        "input_sparse_b = prepare_sparse_context(imgs_b, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
        "residual_s_a = get_residual(model, input_sparse_a)\n",
        "residual_s_b = get_residual(model, input_sparse_b)\n",
        "anchors_a = (torch.rand(B, N_ANCHORS, 2, device=DEVICE) * 2 - 1)\n",
        "candidates_b = (torch.rand(B, N_CANDIDATES, 2, device=DEVICE) * 2 - 1)\n",
        "q_a = fourier_encoder(anchors_a)\n",
        "q_b = fourier_encoder(candidates_b)\n",
        "_, phi_raw_s_a = get_rgb_and_phi_raw(model, q_a, residual_s_a)\n",
        "_, phi_raw_s_b = get_rgb_and_phi_raw(model, q_b, residual_s_b)\n",
        "phi_s_a = F.normalize(projection_head(phi_raw_s_a), dim=-1)\n",
        "phi_s_b = F.normalize(projection_head(phi_raw_s_b), dim=-1)\n",
        "logits_s = torch.bmm(phi_s_a, phi_s_b.transpose(1, 2)) / TAU\n",
        "xi_mapped = apply_affine_to_coords(anchors_a, R, t)\n",
        "sqd = ((candidates_b.unsqueeze(1) - xi_mapped.unsqueeze(2)) ** 2).sum(-1)\n",
        "w_s = torch.exp(-sqd / (2 * SIGMA ** 2))\n",
        "w_s = w_s / (w_s.sum(dim=2, keepdim=True) + 1e-8)\n",
        "pred_coords_s = candidates_b[b_show][logits_s[b_show].argmax(dim=1)]\n",
        "coords_b_batch = candidates_b\n",
        "\n",
        "# Baseline: full context for A and B; random projection for phi (same arch, no contrastive training)\n",
        "if baseline_model is not None:\n",
        "    input_full_a = prepare_full_context(imgs_viz, coords_32, fourier_encoder)\n",
        "    input_full_b = prepare_full_context(imgs_b, coords_32, baseline_fourier)\n",
        "    residual_b_a = get_residual(baseline_model, input_full_a)\n",
        "    residual_b_b = get_residual(baseline_model, input_full_b)\n",
        "    proj_bl = nn.Linear(QUERIES_DIM, PROJ_DIM).to(DEVICE)\n",
        "    _, phi_raw_b_a = get_rgb_and_phi_raw(baseline_model, q_a, residual_b_a)\n",
        "    _, phi_raw_b_b = get_rgb_and_phi_raw(baseline_model, q_b, residual_b_b)\n",
        "    phi_b_a = F.normalize(proj_bl(phi_raw_b_a), dim=-1)\n",
        "    phi_b_b = F.normalize(proj_bl(phi_raw_b_b), dim=-1)\n",
        "    logits_b = torch.bmm(phi_b_a, phi_b_b.transpose(1, 2)) / TAU\n",
        "    w_b = torch.exp(-sqd / (2 * SIGMA ** 2))\n",
        "    w_b = w_b / (w_b.sum(dim=2, keepdim=True) + 1e-8)\n",
        "    pred_coords_b = candidates_b[b_show][logits_b[b_show].argmax(dim=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View A vs View B: anchors on A, GT and predicted match on B (Sparse context vs Full-context baseline)\n",
        "n_show = min(8, anchors_a.size(1))\n",
        "fig, axs = plt.subplots(2 if baseline_model is not None else 1, 2, figsize=(10, 5 if baseline_model is not None else 2.5))\n",
        "if baseline_model is None:\n",
        "    axs = axs.reshape(1, -1)\n",
        "rows_data = [('Sparse context', pred_coords_s)]\n",
        "if baseline_model is not None:\n",
        "    rows_data.append(('Full context (baseline)', pred_coords_b))\n",
        "for row, (name, pred_coords) in enumerate(rows_data):\n",
        "    axs[row, 0].imshow(to_vis(imgs_viz[b_show]))\n",
        "    axs[row, 0].set_title(f'{name}: View A (anchors)')\n",
        "    axs[row, 0].axis('off')\n",
        "    cx_a, cy_a = norm_to_pixel(anchors_a[b_show, :n_show], IMAGE_SIZE, IMAGE_SIZE)\n",
        "    axs[row, 0].scatter(cx_a, cy_a, c='lime', s=40, marker='o', edgecolors='black', linewidths=0.5)\n",
        "    axs[row, 1].imshow(to_vis(imgs_b[b_show]))\n",
        "    axs[row, 1].set_title(f'{name}: View B (GT vs pred)')\n",
        "    axs[row, 1].axis('off')\n",
        "    cx_gt, cy_gt = norm_to_pixel(xi_mapped[b_show, :n_show], IMAGE_SIZE, IMAGE_SIZE)\n",
        "    cx_pr, cy_pr = norm_to_pixel(pred_coords[:n_show], IMAGE_SIZE, IMAGE_SIZE)\n",
        "    axs[row, 1].scatter(cx_gt, cy_gt, c='lime', s=60, marker='+', linewidths=2, label='GT T(x)')\n",
        "    axs[row, 1].scatter(cx_pr, cy_pr, c='cyan', s=40, marker='x', linewidths=1.5, label='pred')\n",
        "    axs[row, 1].legend(loc='upper right', fontsize=8)\n",
        "plt.suptitle('Correspondence: Sparse context vs Full-context baseline')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_viewA_viewB_vs_baseline.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieval @ ε: % of anchors with error < ε (Sparse context vs Full-context baseline)\n",
        "eps_values = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "err_s = (pred_coords_s - xi_mapped[b_show]).norm(dim=-1).cpu().numpy()\n",
        "acc_s = [(err_s < eps).mean() * 100 for eps in eps_values]\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "x = np.arange(len(eps_values))\n",
        "w_bar = 0.35\n",
        "ax.bar(x - w_bar/2, acc_s, width=w_bar, label='Sparse context', color='steelblue')\n",
        "if baseline_model is not None:\n",
        "    err_b = (pred_coords_b - xi_mapped[b_show]).norm(dim=-1).cpu().numpy()\n",
        "    acc_b = [(err_b < eps).mean() * 100 for eps in eps_values]\n",
        "    ax.bar(x + w_bar/2, acc_b, width=w_bar, label='Baseline', color='coral', alpha=0.8)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([str(e) for e in eps_values])\n",
        "ax.set_xlabel('ε (normalized coord distance)')\n",
        "ax.set_ylabel('% anchors with error < ε')\n",
        "ax.set_title('Retrieval accuracy: Sparse context vs Full-context baseline')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_retrieval_at_eps_vs_baseline.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Soft weight concentration (Sparse context vs Full-context baseline)\n",
        "fig, axs = plt.subplots(1, 2 if baseline_model is not None else 1, figsize=(6 if baseline_model else 3, 3))\n",
        "if baseline_model is None:\n",
        "    axs = [axs]\n",
        "w_max_s = w_s.max(dim=2).values.flatten().cpu().numpy()\n",
        "axs[0].hist(w_max_s, bins=30, color='steelblue', edgecolor='black', alpha=0.8)\n",
        "axs[0].set_xlabel('max_j w_ij')\n",
        "axs[0].set_ylabel('Count (anchors)')\n",
        "axs[0].set_title('Sparse context: weight concentration')\n",
        "axs[0].axvline(w_max_s.mean(), color='red', linestyle='--', label=f'mean={w_max_s.mean():.3f}')\n",
        "axs[0].legend()\n",
        "if baseline_model is not None:\n",
        "    w_max_b = w_b.max(dim=2).values.flatten().cpu().numpy()\n",
        "    axs[1].hist(w_max_b, bins=30, color='coral', edgecolor='black', alpha=0.8)\n",
        "    axs[1].set_xlabel('max_j w_ij')\n",
        "    axs[1].set_ylabel('Count (anchors)')\n",
        "    axs[1].set_title('Baseline: weight concentration')\n",
        "    axs[1].axvline(w_max_b.mean(), color='red', linestyle='--', label=f'mean={w_max_b.mean():.3f}')\n",
        "    axs[1].legend()\n",
        "plt.suptitle('Peaked weights = confident correspondence')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_weight_concentration_vs_baseline.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap overlay: soft weights with GT (+) and predicted (x) — Sparse context\n",
        "fig, axs = plt.subplots(2, n_anchors_show, figsize=(12, 5))\n",
        "for i in range(n_anchors_show):\n",
        "    heat = w_s[b_show, i].cpu().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
        "    axs[0, i].imshow(heat, cmap='hot')\n",
        "    axs[0, i].set_title(f'Anchor {i}')\n",
        "    cx_gt, cy_gt = norm_to_pixel(xi_mapped[b_show, i:i+1], IMAGE_SIZE, IMAGE_SIZE)\n",
        "    cx_pr, cy_pr = norm_to_pixel(pred_coords_s[i:i+1], IMAGE_SIZE, IMAGE_SIZE)\n",
        "    axs[0, i].scatter(cx_gt, cy_gt, c='lime', s=80, marker='+', linewidths=2)\n",
        "    axs[0, i].scatter(cx_pr, cy_pr, c='cyan', s=50, marker='x', linewidths=1.5)\n",
        "    axs[0, i].axis('off')\n",
        "    axs[1, i].imshow(to_vis(imgs_b[b_show]))\n",
        "    axs[1, i].scatter(cx_gt, cy_gt, c='lime', s=80, marker='+', linewidths=2)\n",
        "    axs[1, i].scatter(cx_pr, cy_pr, c='cyan', s=50, marker='x', linewidths=1.5)\n",
        "    axs[1, i].axis('off')\n",
        "plt.suptitle('Sparse context: soft weights — lime=GT, cyan=argmax pred')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_heatmap_overlay.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature-level: pos vs neg cosine similarity (Sparse context vs Full-context baseline)\n",
        "S_s = (logits_s[b_show] * TAU).detach().cpu().numpy()\n",
        "N_a, N_b = S_s.shape\n",
        "sqd_b = ((coords_b_batch[b_show].unsqueeze(0) - xi_mapped[b_show].unsqueeze(1)) ** 2).sum(-1).cpu().numpy()\n",
        "j_gt = np.argmin(sqd_b, axis=1)\n",
        "pos_sims_s = S_s[np.arange(N_a), j_gt]\n",
        "neg_mask = np.ones((N_a, N_b), dtype=bool)\n",
        "neg_mask[np.arange(N_a), j_gt] = False\n",
        "neg_sims_s = S_s[neg_mask]\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "ax.hist(neg_sims_s, bins=40, alpha=0.5, color='coral', label='Sparse context neg', density=True)\n",
        "ax.hist(pos_sims_s, bins=30, alpha=0.5, color='green', label='Sparse context pos', density=True)\n",
        "if baseline_model is not None:\n",
        "    S_b = (logits_b[b_show] * TAU).detach().cpu().numpy()\n",
        "    pos_sims_b = S_b[np.arange(N_a), j_gt]\n",
        "    neg_sims_b = S_b[neg_mask]\n",
        "    ax.hist(neg_sims_b, bins=40, alpha=0.4, color='gray', histtype='step', linewidth=2, label='Baseline neg', density=True)\n",
        "    ax.hist(pos_sims_b, bins=30, alpha=0.5, color='blue', histtype='step', linewidth=2, label='Baseline pos', density=True)\n",
        "ax.set_xlabel('Cosine similarity φ(anchor)·φ(candidate)')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Feature-level: Sparse context (filled) vs Full-context baseline (outline)')\n",
        "ax.legend(loc='upper left', fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_feature_pos_neg_vs_baseline.png', dpi=100)\n",
        "plt.show()\n",
        "print('Sparse context: pos mean={:.4f} neg mean={:.4f} gap={:.4f}'.format(pos_sims_s.mean(), neg_sims_s.mean(), pos_sims_s.mean()-neg_sims_s.mean()))\n",
        "if baseline_model is not None:\n",
        "    print('Baseline:   pos mean={:.4f} neg mean={:.4f} gap={:.4f}'.format(pos_sims_b.mean(), neg_sims_b.mean(), pos_sims_b.mean()-neg_sims_b.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TDSM: per-token decoded spatial map (Full context vs Sparse context)\n",
        "\n",
        "Same images; baseline gets **full context**, sparse model gets **sparse context**. Compare per-token reconstructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tdsm(model, fourier_enc, data, coords, device, num_tokens=256, token_step=4):\n",
        "    with torch.no_grad():\n",
        "        residual = get_residual(model, data)\n",
        "        B = data.size(0)\n",
        "        queries_32 = fourier_enc(coords.unsqueeze(0).expand(B, -1, -1)).to(device)\n",
        "        component_images = []\n",
        "        for k in range(0, num_tokens, token_step):\n",
        "            ctx_k = residual[:, k:k+1, :]\n",
        "            logits_k = decoder_forward(model, queries_32, ctx_k)\n",
        "            img_k = logits_k.reshape(B, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "            component_images.append(img_k)\n",
        "        component_images = torch.stack(component_images, dim=1)\n",
        "        tdsm = component_images.mean(dim=-1)\n",
        "    return tdsm\n",
        "\n",
        "TDSM_TOKEN_STEP = 4\n",
        "imgs_tdsm = imgs_viz[:4]\n",
        "input_tdsm_full = prepare_full_context(imgs_tdsm, coords_32, fourier_encoder)\n",
        "input_tdsm_sparse = prepare_sparse_context(imgs_tdsm, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
        "tdsm_sparse = get_tdsm(model, fourier_encoder, input_tdsm_sparse, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "if baseline_model is not None:\n",
        "    tdsm_baseline = get_tdsm(baseline_model, baseline_fourier, input_tdsm_full, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "else:\n",
        "    tdsm_baseline = None\n",
        "\n",
        "token_indices = [0, 16, 32, 48]\n",
        "tdsm_slice_idx = [k // TDSM_TOKEN_STEP for k in token_indices]\n",
        "n_show_t = len(token_indices)\n",
        "sample_idx = 0\n",
        "n_rows = 2 if baseline_model is not None else 1\n",
        "fig, axs = plt.subplots(n_rows, n_show_t, figsize=(12, 5 if n_rows == 2 else 2.5))\n",
        "if n_rows == 1:\n",
        "    axs = axs.reshape(1, -1)\n",
        "for i, (k, sk) in enumerate(zip(token_indices, tdsm_slice_idx)):\n",
        "    if baseline_model is not None:\n",
        "        axs[0, i].imshow(tdsm_baseline[sample_idx, sk].cpu().numpy(), cmap='viridis')\n",
        "        axs[0, i].set_title('Token %d Baseline' % k)\n",
        "        axs[0, i].axis('off')\n",
        "    axs[n_rows-1, i].imshow(tdsm_sparse[sample_idx, sk].cpu().numpy(), cmap='viridis')\n",
        "    axs[n_rows-1, i].set_title('Token %d Sparse context' % k)\n",
        "    axs[n_rows-1, i].axis('off')\n",
        "plt.suptitle('TDSM: Full context vs Sparse context')\n",
        "plt.tight_layout()\n",
        "plt.savefig('sparse_context_tdsm_vs_baseline.png', dpi=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TDSM t-SNE: class-colored token-level reconstruction (no pooling)\n",
        "\n",
        "Each point = one token's **full spatial map** (H×W flattened). All token maps from all images; color by image class. PCA + t-SNE. Compare **Sparse context**, **Full context** (baseline), and **Full context + NCE** (if checkpoint_nce_best.pt exists; save from SoftInfoNCE notebook)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No pooling: each point = one token's full spatial map (H*W), color by image class\n",
        "N_VAL_TDSM = min(400, len(test_loader.dataset))\n",
        "all_feat_sparse, all_feat_baseline, all_feat_nce, all_labels = [], [], [], []\n",
        "n_done = 0\n",
        "with torch.no_grad():\n",
        "    for imgs_batch, labels_batch in test_loader:\n",
        "        if n_done >= N_VAL_TDSM:\n",
        "            break\n",
        "        imgs_batch = imgs_batch.to(DEVICE)\n",
        "        input_full = prepare_full_context(imgs_batch, coords_32, fourier_encoder)\n",
        "        input_sparse = prepare_sparse_context(imgs_batch, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
        "        tdsm_s = get_tdsm(model, fourier_encoder, input_sparse, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "        # (B, num_tokens, H, W) -> (B*num_tokens, H*W)\n",
        "        B, nt, h, w = tdsm_s.shape\n",
        "        all_feat_sparse.append(tdsm_s.reshape(B * nt, h * w).cpu().numpy())\n",
        "        all_labels.append(np.repeat(labels_batch.numpy(), nt))\n",
        "        if baseline_model is not None:\n",
        "            tdsm_b = get_tdsm(baseline_model, baseline_fourier, input_full, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "            all_feat_baseline.append(tdsm_b.reshape(B * nt, h * w).cpu().numpy())\n",
        "        if nce_model is not None:\n",
        "            tdsm_n = get_tdsm(nce_model, nce_fourier, input_full, coords_32, DEVICE, token_step=TDSM_TOKEN_STEP)\n",
        "            all_feat_nce.append(tdsm_n.reshape(B * nt, h * w).cpu().numpy())\n",
        "        n_done += imgs_batch.size(0)\n",
        "\n",
        "X_sparse = np.concatenate(all_feat_sparse, axis=0)\n",
        "y_all = np.concatenate(all_labels, axis=0)\n",
        "if baseline_model is not None:\n",
        "    X_baseline = np.concatenate(all_feat_baseline, axis=0)\n",
        "if nce_model is not None:\n",
        "    X_nce = np.concatenate(all_feat_nce, axis=0)\n",
        "print('TDSM points (no pooling):', X_sparse.shape[0], 'features dim', X_sparse.shape[1])\n",
        "\n",
        "try:\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "    n_comp = min(50, X_sparse.shape[1], X_sparse.shape[0] - 1)\n",
        "    X_sparse_pca = PCA(n_components=n_comp).fit_transform(X_sparse)\n",
        "    X_sparse_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_sparse_pca)\n",
        "    n_plots = 1 + (1 if baseline_model is not None else 0) + (1 if nce_model is not None else 0)\n",
        "    fig, axs = plt.subplots(1, n_plots, figsize=(6 * n_plots, 5))\n",
        "    if n_plots == 1:\n",
        "        axs = [axs]\n",
        "    idx = 0\n",
        "    sc = axs[idx].scatter(X_sparse_tsne[:, 0], X_sparse_tsne[:, 1], c=y_all, cmap='tab10', s=8, alpha=0.6)\n",
        "    axs[idx].set_title('Sparse context (each point = one token map)')\n",
        "    axs[idx].set_xlabel('t-SNE 1'); axs[idx].set_ylabel('t-SNE 2')\n",
        "    idx += 1\n",
        "    if baseline_model is not None:\n",
        "        X_base_pca = PCA(n_components=n_comp).fit_transform(X_baseline)\n",
        "        X_base_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_base_pca)\n",
        "        axs[idx].scatter(X_base_tsne[:, 0], X_base_tsne[:, 1], c=y_all, cmap='tab10', s=8, alpha=0.6)\n",
        "        axs[idx].set_title('Full context (each point = one token map)')\n",
        "        axs[idx].set_xlabel('t-SNE 1'); axs[idx].set_ylabel('t-SNE 2')\n",
        "        idx += 1\n",
        "    if nce_model is not None:\n",
        "        X_nce_pca = PCA(n_components=n_comp).fit_transform(X_nce)\n",
        "        X_nce_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_nce_pca)\n",
        "        axs[idx].scatter(X_nce_tsne[:, 0], X_nce_tsne[:, 1], c=y_all, cmap='tab10', s=8, alpha=0.6)\n",
        "        axs[idx].set_title('Full context + NCE (each point = one token map)')\n",
        "        axs[idx].set_xlabel('t-SNE 1'); axs[idx].set_ylabel('t-SNE 2')\n",
        "    plt.colorbar(sc, ax=axs, label='Class', shrink=0.6)\n",
        "    plt.suptitle('TDSM: all token maps (no pooling), colored by CIFAR-10 class')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sparse_context_tdsm_tsne_class.png', dpi=100)\n",
        "    plt.show()\n",
        "except ImportError as e:\n",
        "    print('Install sklearn for t-SNE/PCA:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Where Sparse context differs from Full-context baseline: spatial diff and object sensitivity\n",
        "if baseline_model is not None and tdsm_baseline is not None:\n",
        "    diff_spatial = (tdsm_sparse - tdsm_baseline).abs().mean(dim=1).cpu().numpy()\n",
        "    fig, axs = plt.subplots(2, 4, figsize=(14, 6))\n",
        "    for i in range(4):\n",
        "        axs[0, i].imshow(to_vis(imgs_tdsm[i]))\n",
        "        axs[0, i].set_title('Input' if i == 0 else ''); axs[0, i].axis('off')\n",
        "        im = axs[1, i].imshow(diff_spatial[i], cmap='hot')\n",
        "        axs[1, i].set_title('|Sparse − Full ctx|' if i == 0 else ''); axs[1, i].axis('off')\n",
        "    plt.colorbar(im, ax=axs[1, :], shrink=0.6, label='Mean |diff|')\n",
        "    plt.suptitle('Where Sparse context differs from Full-context baseline')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sparse_context_tdsm_spatial_diff.png', dpi=100)\n",
        "    plt.show()\n",
        "\n",
        "    H, W = IMAGE_SIZE, IMAGE_SIZE\n",
        "    margin = 8\n",
        "    obj_mask = np.zeros((H, W), dtype=np.float32)\n",
        "    obj_mask[margin:H-margin, margin:W-margin] = 1.0\n",
        "    bg_mask = 1.0 - obj_mask\n",
        "    obj_mask = torch.from_numpy(obj_mask).to(DEVICE).view(1, 1, H, W)\n",
        "    bg_mask = torch.from_numpy(bg_mask).to(DEVICE).view(1, 1, H, W)\n",
        "    n_tokens = tdsm_baseline.size(1)\n",
        "    obj_bl = (tdsm_baseline * obj_mask).sum(dim=(2, 3)) / (obj_mask.sum() + 1e-8)\n",
        "    bg_bl = (tdsm_baseline * bg_mask).sum(dim=(2, 3)) / (bg_mask.sum() + 1e-8)\n",
        "    obj_nce = (tdsm_sparse * obj_mask).sum(dim=(2, 3)) / (obj_mask.sum() + 1e-8)\n",
        "    bg_nce = (tdsm_sparse * bg_mask).sum(dim=(2, 3)) / (bg_mask.sum() + 1e-8)\n",
        "    sens_bl = (obj_bl - bg_bl).mean(dim=0).cpu().numpy()\n",
        "    sens_nce = (obj_nce - bg_nce).mean(dim=0).cpu().numpy()\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    axs[0].bar(np.arange(n_tokens), sens_bl, color='steelblue', alpha=0.8); axs[0].set_title('Baseline'); axs[0].set_ylabel('Object sensitivity')\n",
        "    axs[1].bar(np.arange(n_tokens), sens_nce, color='green', alpha=0.8); axs[1].set_title('Sparse context'); axs[1].set_ylabel('Object sensitivity')\n",
        "    plt.suptitle('Per-token object vs background (center − border)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sparse_context_tdsm_object_sensitivity.png', dpi=100)\n",
        "    plt.show()\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.scatter(sens_bl, sens_nce, alpha=0.7)\n",
        "    plt.plot([sens_bl.min(), sens_bl.max()], [sens_bl.min(), sens_bl.max()], 'r--', label='y=x')\n",
        "    plt.xlabel('Full-context object sensitivity'); plt.ylabel('Sparse context object sensitivity')\n",
        "    plt.title('Above line = Sparse context more object-focused'); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig('sparse_context_tdsm_sensitivity_scatter.png', dpi=100)\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Skipping spatial diff / object sensitivity (no baseline loaded).')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
