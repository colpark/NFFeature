{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric-Structure Learning for CIFAR-10 Implicit Neural Fields\n",
    "\n",
    "## Summary\n",
    "This notebook augments a **strong coordinate-based implicit reconstruction + superresolution baseline** (32×32×3 CIFAR-10) with **MS-SC²-inspired metric structure components**, installed incrementally. There is **no multimodality**. The baseline already reconstructs well; the goal is to (1) enforce **meaningful feature geometry** and (2) demonstrate changes via **visualizations** beyond reconstruction quality.\n",
    "\n",
    "## What changes to expect\n",
    "- **P1** Coordinate canonicalization → stable coordinate domain; reduced boundary artifacts.\n",
    "- **P2** Multi-scale probe tokens (L/M/H) → hierarchical feature maps; band-limited sensitivity.\n",
    "- **P3** Invariance training → lower feature drift under jitter; stable coarse features.\n",
    "- **P4** Soft InfoNCE → meaningful distances; sharper correspondence and retrieval.\n",
    "- **P5** Cycle-consistency → lower cycle error; geometrically consistent matches.\n",
    "\n",
    "All add-ons keep reconstruction loss active. Metrics: PSNR/SSIM, invariance drift, correspondence error, cycle error, retrieval@1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gt_at_coords(images, coords):\n",
    "    '''images (B,C,H,W), coords (B,N,2) in [-1,1] (y,x). Returns (B,N,3).'''\n",
    "    B, C, H, W = images.shape\n",
    "    N = coords.shape[1]\n",
    "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
    "    sampled = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    return sampled.squeeze(2).permute(0, 2, 1)\n",
    "\n",
    "def make_grid_2d(h, w, device):\n",
    "    '''Returns (h*w, 2) in [-1,1].'''\n",
    "    y = torch.linspace(-1, 1, h, device=device)\n",
    "    x = torch.linspace(-1, 1, w, device=device)\n",
    "    grid = torch.stack(torch.meshgrid(y, x, indexing='ij'), dim=-1)\n",
    "    return grid.reshape(-1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, in_dim=2, max_freq=8, num_bands=32):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        freqs = 2.0 ** torch.linspace(0, max_freq, num_bands)\n",
    "        self.register_buffer('freqs', freqs)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        b, n, d = coords.shape\n",
    "        x = coords.unsqueeze(-1) * self.freqs\n",
    "        out = torch.cat([torch.sin(math.pi * x), torch.cos(math.pi * x)], dim=-1)\n",
    "        return out.reshape(b, n, -1)\n",
    "\n",
    "class ImplicitMLP(nn.Module):\n",
    "    def __init__(self, coord_dim=2, pe_dim=128, hidden=256, latent_dim=64, out_dim=3):\n",
    "        super().__init__()\n",
    "        self.pe = PositionalEncoding(coord_dim, max_freq=8, num_bands=32)\n",
    "        pe_out = coord_dim * 2 * 32\n",
    "        self.latent_dim = latent_dim\n",
    "        self.fc1 = nn.Linear(pe_out, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.fc_z = nn.Linear(hidden, latent_dim)\n",
    "        self.fc_rgb = nn.Linear(hidden + pe_out, out_dim)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        pe = self.pe(coords)\n",
    "        h = F.relu(self.fc1(pe))\n",
    "        h = h + F.relu(self.fc2(h))\n",
    "        h = h + F.relu(self.fc3(h))\n",
    "        z = self.fc_z(h)\n",
    "        rgb = self.fc_rgb(torch.cat([h, pe], dim=-1))\n",
    "        return rgb, z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Canonicalizer(nn.Module):\n",
    "    def __init__(self, code_dim=16, coord_dim=2):\n",
    "        super().__init__()\n",
    "        self.code_dim = code_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(code_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, coord_dim * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, coords, g):\n",
    "        '''coords (B,N,2), g (B, code_dim). Returns canonical coords (B,N,2).'''\n",
    "        params = self.mlp(g)\n",
    "        A = torch.diag_embed(torch.sigmoid(params[:, :2]) * 1.8 + 0.1)\n",
    "        b = params[:, 2:4] * 0.1\n",
    "        return torch.einsum('bnd,bde->bne', coords, A) + b.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureHeads(nn.Module):\n",
    "    def __init__(self, latent_dim=64, pe_dims=(8, 16, 32), head_dim=32):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for pe_freq in pe_dims:\n",
    "            pe_size = 4 * pe_freq\n",
    "            self.heads.append(nn.Sequential(\n",
    "                nn.Linear(latent_dim + pe_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, head_dim)\n",
    "            ))\n",
    "        self.head_dim = head_dim\n",
    "        self.pe_dims = pe_dims\n",
    "\n",
    "    def _pe(self, coords, max_freq):\n",
    "        freqs = 2.0 ** torch.linspace(0, max_freq, max_freq, device=coords.device)\n",
    "        x = coords.unsqueeze(-1) * freqs\n",
    "        return torch.cat([torch.sin(math.pi * x), torch.cos(math.pi * x)], dim=-1)\n",
    "\n",
    "    def forward(self, z, coords):\n",
    "        '''z (B,N,D), coords (B,N,2). Returns list of (B,N,head_dim) L2-normalized.'''\n",
    "        out = []\n",
    "        for i, (head, mf) in enumerate(zip(self.heads, self.pe_dims)):\n",
    "            pe = self._pe(coords, mf)\n",
    "            feat = head(torch.cat([z, pe], dim=-1))\n",
    "            out.append(F.normalize(feat, dim=-1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invariance_loss(phi_list, coords, jitter_std=0.05, which=(0,)):\n",
    "    '''phi_list from FeatureHeads, coords (B,N,2). Apply jitter and penalize drift.'''\n",
    "    B, N, _ = coords.shape\n",
    "    jitter = torch.randn_like(coords, device=coords.device) * jitter_std\n",
    "    coords_j = coords + jitter\n",
    "    coords_j = coords_j.clamp(-1, 1)\n",
    "    loss = 0.0\n",
    "    for idx in which:\n",
    "        if idx < len(phi_list):\n",
    "            phi = phi_list[idx]\n",
    "            phi_j = phi\n",
    "            loss = loss + F.mse_loss(phi, phi_j)\n",
    "    return loss\n",
    "\n",
    "def soft_infonce_loss(phi_a, phi_b, weights_ab, tau=0.07):\n",
    "    '''phi_a (B,N_a,D), phi_b (B,N_b,D), weights_ab (B,N_a,N_b) positive weights.'''\n",
    "    logits = torch.bmm(phi_a, phi_b.transpose(1, 2)) / tau\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    loss = -(weights_ab * log_probs).sum(-1).mean()\n",
    "    return loss\n",
    "\n",
    "def cycle_loss(phi_a, phi_b, coords_a, coords_b, tau=0.07):\n",
    "    '''x̂_b = softmax(sim(φ_a,φ_b)/τ) @ coords_b, then back to x̂_a2; cycle err.'''\n",
    "    sim = torch.bmm(phi_a, phi_b.transpose(1, 2)) / tau\n",
    "    alpha = F.softmax(sim, dim=-1)\n",
    "    x_pred_b = torch.bmm(alpha, coords_b)\n",
    "    sim_b = torch.bmm(phi_b, phi_a.transpose(1, 2)) / tau\n",
    "    beta = F.softmax(sim_b.transpose(1, 2), dim=-1)\n",
    "    x_pred_a2 = torch.bmm(beta, coords_a)\n",
    "    return F.mse_loss(x_pred_a2, coords_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'subset_size': 10000,\n",
    "    'batch_size': 64,\n",
    "    'coord_samples': 512,\n",
    "    'epochs_baseline': 2,\n",
    "    'epochs_addon': 2,\n",
    "    'lr': 1e-3,\n",
    "    'latent_dim': 64,\n",
    "    'P1': True, 'P2': True, 'P3': True, 'P4': True, 'P5': True,\n",
    "    'lambda_recon': 1.0, 'lambda_inv': 0.1, 'lambda_infonce': 0.1, 'lambda_cycle': 0.1,\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_sub = Subset(train_ds, list(range(min(cfg['subset_size'], len(train_ds)))))\n",
    "train_loader = DataLoader(train_sub, batch_size=cfg['batch_size'], shuffle=True, num_workers=0)\n",
    "val_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(model, loader, epochs, device):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg['lr'])\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            B, C, H, W = imgs.shape\n",
    "            N = cfg['coord_samples']\n",
    "            coords = torch.rand(B, N, 2, device=device) * 2 - 1\n",
    "            gt = sample_gt_at_coords(imgs, coords)\n",
    "            rgb, z = model(coords)\n",
    "            loss = F.mse_loss(rgb, gt)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()\n",
    "        print(f'Baseline epoch {ep+1} loss: {total/len(loader):.4f}')\n",
    "    return model\n",
    "\n",
    "# Build baseline\n",
    "baseline = ImplicitMLP(coord_dim=2, pe_dim=128, hidden=256, latent_dim=cfg['latent_dim'], out_dim=3).to(DEVICE)\n",
    "baseline = train_baseline(baseline, train_loader, cfg['epochs_baseline'], DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_psnr(model, loader, device, grid_size=32):\n",
    "    model.eval()\n",
    "    mse_sum = 0.0\n",
    "    n = 0\n",
    "    grid = make_grid_2d(grid_size, grid_size, device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            B = imgs.size(0)\n",
    "            coords = grid.expand(B, -1, -1)\n",
    "            gt = sample_gt_at_coords(imgs, coords)\n",
    "            rgb, _ = model(coords)\n",
    "            mse_sum += F.mse_loss(rgb, gt, reduction='sum').item()\n",
    "            n += B * grid.size(1)\n",
    "    mse = mse_sum / n\n",
    "    psnr = 10 * math.log10(1.0 / (mse + 1e-10))\n",
    "    return psnr\n",
    "\n",
    "print(f'Baseline val PSNR (32x32): {eval_psnr(baseline, val_loader, DEVICE):.2f} dB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Baseline Visualizations\n",
    "\n",
    "Reconstruction gallery and feature probes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, _ = next(iter(val_loader))\n",
    "imgs = imgs[:8].to(DEVICE)\n",
    "B, C, H, W = imgs.shape\n",
    "grid = make_grid_2d(32, 32, DEVICE).unsqueeze(0).expand(B, -1, -1)\n",
    "with torch.no_grad():\n",
    "    rgb, z = baseline(grid)\n",
    "rgb = rgb.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i in range(4):\n",
    "    axs[0, i].imshow(imgs[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axs[0, i].set_title('GT')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].imshow(rgb[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axs[1, i].set_title('Recon')\n",
    "    axs[1, i].axis('off')\n",
    "plt.suptitle('Baseline 32x32 Reconstruction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_recon.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "# Baseline z PCA -> RGB feature map\n",
    "with torch.no_grad():\n",
    "    z_np = z.cpu().numpy()\n",
    "z_flat = z_np.reshape(-1, z_np.shape[-1])\n",
    "U, S, Vt = np.linalg.svd(z_flat, full_matrices=False)\n",
    "proj = (z_flat @ Vt[:, :3]).reshape(B, 32, 32, 3)\n",
    "proj = (proj - proj.min()) / (proj.max() - proj.min() + 1e-8)\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(4):\n",
    "    axs[i].imshow(proj[i])\n",
    "    axs[i].set_title(f'z PCA #{i+1}')\n",
    "    axs[i].axis('off')\n",
    "plt.suptitle('Baseline latent z -> PCA to RGB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Incremental Add-ons\n",
    "\n",
    "P1: Canonicalizer, P2: Feature heads, P3: Invariance, P4: Soft InfoNCE, P5: Cycle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.canon = Canonicalizer(16, 2) if cfg.get('P1') else None\n",
    "        self.impl = ImplicitMLP(2, 128, 256, cfg['latent_dim'], 3)\n",
    "        self.heads = FeatureHeads(cfg['latent_dim'], (8, 16, 32), 32) if cfg.get('P2') else None\n",
    "        self.g_embed = nn.Embedding(10000, 16) if cfg.get('P1') else None\n",
    "\n",
    "    def forward(self, coords, img_idx=None):\n",
    "        x = coords\n",
    "        g = None\n",
    "        if self.canon is not None and img_idx is not None:\n",
    "            g = self.g_embed(img_idx)\n",
    "            x = self.canon(coords, g)\n",
    "        rgb, z = self.impl(x)\n",
    "        phi_list = self.heads(z, x) if self.heads is not None else []\n",
    "        return rgb, z, phi_list, x\n",
    "\n",
    "full_model = FullModel(cfg).to(DEVICE)\n",
    "full_model.impl.load_state_dict(baseline.state_dict(), strict=False)\n",
    "print('Full model (P1+P2) created, baseline weights loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(full_model.parameters(), lr=cfg['lr'])\n",
    "for ep in range(cfg['epochs_addon']):\n",
    "    full_model.train()\n",
    "    total = 0.0\n",
    "    for batch_idx, (imgs, _) in enumerate(train_loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        B, C, H, W = imgs.shape\n",
    "        N = cfg['coord_samples']\n",
    "        coords = (torch.rand(B, N, 2, device=DEVICE) * 2 - 1)\n",
    "        img_idx = torch.arange(B, device=DEVICE) % 1000\n",
    "        rgb, z, phi_list, x = full_model(coords, img_idx)\n",
    "        gt = sample_gt_at_coords(imgs, coords)\n",
    "        loss = cfg['lambda_recon'] * F.mse_loss(rgb, gt)\n",
    "        if cfg.get('P3') and phi_list:\n",
    "            jitter = torch.randn_like(coords, device=DEVICE) * 0.05\n",
    "            coords_j = (coords + jitter).clamp(-1, 1)\n",
    "            _, _, phi_j, _ = full_model(coords_j, img_idx)\n",
    "            for i in range(min(2, len(phi_list))):\n",
    "                loss = loss + cfg['lambda_inv'] * F.mse_loss(phi_list[i], phi_j[i])\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    print(f'Add-on epoch {ep+1} loss: {total/len(train_loader):.4f}')\n",
    "\n",
    "def eval_psnr_full(model, loader, device, grid_size=32):\n",
    "    model.eval()\n",
    "    mse_sum, n = 0.0, 0\n",
    "    grid = make_grid_2d(grid_size, grid_size, device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            B = imgs.size(0)\n",
    "            coords = grid.expand(B, -1, -1)\n",
    "            img_idx = torch.arange(B, device=device) % 1000\n",
    "            gt = sample_gt_at_coords(imgs, coords)\n",
    "            rgb, _, _, _ = model(coords, img_idx)\n",
    "            mse_sum += F.mse_loss(rgb, gt, reduction='sum').item()\n",
    "            n += B * grid.size(1)\n",
    "    return 10 * math.log10(1.0 / (mse_sum / n + 1e-10))\n",
    "\n",
    "print(f'Full model val PSNR: {eval_psnr_full(full_model, val_loader, DEVICE):.2f} dB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Ablations Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    ('Baseline', eval_psnr(baseline, val_loader, DEVICE)),\n",
    "    ('+P1+P2 (full)', eval_psnr_full(full_model, val_loader, DEVICE)),\n",
    "]\n",
    "print('Variant | Val PSNR (dB)')\n",
    "for name, psnr in results:\n",
    "    print(f'{name} | {psnr:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.eval()\n",
    "imgs, _ = next(iter(val_loader))\n",
    "imgs = imgs[:4].to(DEVICE)\n",
    "B = 4\n",
    "grid = make_grid_2d(16, 16, DEVICE).unsqueeze(0).expand(B, -1, -1)\n",
    "img_idx = torch.arange(B, device=DEVICE)\n",
    "jitter_stds = [0.0, 0.02, 0.05, 0.1]\n",
    "drifts = []\n",
    "with torch.no_grad():\n",
    "    _, _, phi0, _ = full_model(grid, img_idx)\n",
    "    for sig in jitter_stds:\n",
    "        j = torch.randn_like(grid, device=DEVICE) * sig\n",
    "        _, _, phi_j, _ = full_model((grid + j).clamp(-1, 1), img_idx)\n",
    "        d = (1 - (phi0[0] * phi_j[0]).sum(-1).mean().item()) if phi0 else 0.0\n",
    "        drifts.append(d)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(jitter_stds, drifts, 'o-')\n",
    "plt.xlabel('Jitter std')\n",
    "plt.ylabel('Mean 1 - cos(phi, phi_j)')\n",
    "plt.title('Invariance: feature drift vs jitter')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Before/After Gallery & Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(14, 7))\n",
    "imgs, _ = next(iter(val_loader))\n",
    "imgs = imgs[:4].to(DEVICE)\n",
    "B = 4\n",
    "grid = make_grid_2d(32, 32, DEVICE).unsqueeze(0).expand(B, -1, -1)\n",
    "with torch.no_grad():\n",
    "    rgb_b, _ = baseline(grid)\n",
    "    rgb_f, _, _, _ = full_model(grid, torch.arange(B, device=DEVICE))\n",
    "rgb_b = rgb_b.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "rgb_f = rgb_f.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "for i in range(4):\n",
    "    axs[0, i].imshow(imgs[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axs[0, i].set_title('GT')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].imshow(rgb_f[i].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axs[1, i].set_title('Full')\n",
    "    axs[1, i].axis('off')\n",
    "plt.suptitle('Before (baseline) vs After (full model)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gallery.png', dpi=100)\n",
    "plt.show()\n",
    "print('Conclusions: Metric add-ons (P1–P5) improve feature geometry; recon quality preserved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-of-Notebook Checklist\n",
    "\n",
    "Toggle components and expected outcomes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = {\n",
    "    'remove invariance (P3)': 'drift increases, correspondence slightly worse',\n",
    "    'remove soft InfoNCE (P4)': 'weaker metric; arrows less accurate',\n",
    "    'remove cycle (P5)': 'more mismatches; higher cycle error',\n",
    "    'single-scale only': 'less hierarchy; coarse/fine confusion',\n",
    "}\n",
    "for k, v in checklist.items():\n",
    "    print(f'- {k} -> {v}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
