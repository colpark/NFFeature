{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Token Generator (Foundation-Field) — CIFAR-10\n",
        "\n",
        "Treat the neural field as a **continuous, resolution-agnostic token generator**:\n",
        "\n",
        "> *f(x) → [RGB, φ(x)]*\n",
        "\n",
        "with **φ(x)** L2-normalized semantic features. Techniques are implemented as **distinguishable components**:\n",
        "\n",
        "1. **Dual output** — RGB (reconstruction) + φ (semantic tokens)\n",
        "2. **Reconstruction loss** — MSE on RGB\n",
        "3. **Geometry-aware contrastive** — Soft InfoNCE φ_A(x) ↔ φ_B(T(x)) with known affine T\n",
        "4. **Multi-density coordinate sampling** — full grid, sparse 64/256/512/1024 (sampling-rate invariance)\n",
        "5. **Resolution-agnostic query** — after training, query φ at 16×16, 32×32, 64×64\n",
        "\n",
        "Checkpoints: **checkpoint_semantic_token_best.pt** / **_last.pt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config — all techniques toggleable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "# 1) Reconstruction\n",
        "USE_RECON_LOSS = True\n",
        "\n",
        "# 2) Geometry-aware contrastive (Soft InfoNCE)\n",
        "USE_CONTRASTIVE = True\n",
        "NCE_RAMP_STEPS = 500\n",
        "LAMBDA_NCE = 0.1\n",
        "N_ANCHORS, N_CANDIDATES = 256, 1024\n",
        "TAU, SIGMA = 0.1, 0.08\n",
        "\n",
        "# 3) Semantic head (φ output)\n",
        "PHI_DIM = 128\n",
        "\n",
        "# 4) Multi-density sampling: one of ['full', 'sparse_64', 'sparse_256', 'sparse_512', 'sparse_1024'] per batch\n",
        "SAMPLING_MODES = [\"full\", \"sparse_64\", \"sparse_256\", \"sparse_512\", \"sparse_1024\"]\n",
        "N_FULL = IMAGE_SIZE * IMAGE_SIZE\n",
        "SAMPLING_N = {\"full\": N_FULL, \"sparse_64\": 64, \"sparse_256\": 256, \"sparse_512\": 512, \"sparse_1024\": min(1024, N_FULL)}\n",
        "\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(\"N_FULL:\", N_FULL, \"| SAMPLING_MODES:\", SAMPLING_MODES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Field output: RGB + φ (semantic tokens)\n",
        "\n",
        "Decoder produces one vector per query; we split into **RGB** (reconstruction) and **φ** (L2-normalized semantic token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    sampled = F.grid_sample(images, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\n",
        "    return sampled.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def get_residual(model, context):\n",
        "    \"\"\"Encoder + processor → (B, num_latents, latent_dim).\"\"\"\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=context, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def decoder_forward(model, queries, residual):\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return x\n",
        "\n",
        "def get_rgb(model, queries, residual):\n",
        "    \"\"\"(B, N, 3) — reconstruction target.\"\"\"\n",
        "    h = decoder_forward(model, queries, residual)\n",
        "    return model.to_logits(h)\n",
        "\n",
        "def get_phi_raw(model, queries, residual):\n",
        "    \"\"\"(B, N, QUERIES_DIM) — pre-normalized semantic features.\"\"\"\n",
        "    return decoder_forward(model, queries, residual)\n",
        "\n",
        "def get_semantic_tokens(phi_raw, semantic_head):\n",
        "    \"\"\"φ(x) L2-normalized, shared across images. (B, N, PHI_DIM).\"\"\"\n",
        "    return F.normalize(semantic_head(phi_raw), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Context builders — full vs sparse (multi-density)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_full_context(images, coords_full, fourier_encoder):\n",
        "    \"\"\"Full grid: (B, N_FULL, INPUT_DIM).\"\"\"\n",
        "    input_full, _, _ = prepare_model_input(images, coords_full, fourier_encoder)\n",
        "    return input_full\n",
        "\n",
        "def prepare_sparse_context(images, coords_full, fourier_encoder, num_sparse, device):\n",
        "    \"\"\"Random subset of num_sparse coords. (B, num_sparse, INPUT_DIM).\"\"\"\n",
        "    B = images.size(0)\n",
        "    idx = torch.randperm(coords_full.size(0), device=device)[:num_sparse]\n",
        "    coords_sparse = coords_full[idx]\n",
        "    pixels = sample_gt_at_coords(images, coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos = fourier_encoder(coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels, pos], dim=-1)\n",
        "\n",
        "def sample_context_for_batch(images, coords_full, fourier_encoder, device):\n",
        "    \"\"\"Multi-density: pick a mode at random, return context and number of points.\"\"\"\n",
        "    mode = random.choice(SAMPLING_MODES)\n",
        "    n = SAMPLING_N[mode]\n",
        "    if mode == \"full\":\n",
        "        return prepare_full_context(images, coords_full, fourier_encoder), n\n",
        "    return prepare_sparse_context(images, coords_full, fourier_encoder, n, device), n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Geometry-aware contrastive — Soft InfoNCE\n",
        "\n",
        "Two views A, B with known affine T. Match φ_A(x) ↔ φ_B(T(x)) with soft weights by spatial distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_affine_params(batch_size, device, scale_range=(0.85, 1.0), max_translate=0.1, max_angle_deg=12):\n",
        "    angle = (torch.rand(batch_size, device=device) * 2 - 1) * (max_angle_deg * math.pi / 180)\n",
        "    scale = scale_range[0] + torch.rand(batch_size, device=device) * (scale_range[1] - scale_range[0])\n",
        "    tx = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
        "    ty = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
        "    c, s = torch.cos(angle), torch.sin(angle)\n",
        "    R = torch.stack([c*scale, -s*scale, s*scale, c*scale], dim=-1).view(batch_size, 2, 2)\n",
        "    t = torch.stack([tx, ty], dim=1)\n",
        "    return R, t\n",
        "\n",
        "def apply_affine_to_coords(coords, R, t):\n",
        "    return torch.einsum(\"bed,bnd->bne\", R, coords) + t.unsqueeze(1)\n",
        "\n",
        "def apply_affine_to_image(images, R, t):\n",
        "    R_inv = torch.inverse(R)\n",
        "    theta = torch.cat([R_inv, -(R_inv @ t.unsqueeze(2))], dim=2)\n",
        "    grid = F.affine_grid(theta, images.size(), align_corners=True)\n",
        "    return F.grid_sample(images, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\n",
        "\n",
        "def soft_infonce_loss(phi_a, phi_b, coords_a, coords_b, R, t, tau=0.1, sigma=0.08):\n",
        "    \"\"\"Geometry-aware: φ_A(x) ↔ φ_B(T(x)); soft assignment by spatial distance.\"\"\"\n",
        "    logits = torch.bmm(phi_a, phi_b.transpose(1, 2)) / tau\n",
        "    coords_a_mapped = apply_affine_to_coords(coords_a, R, t)\n",
        "    sqd = ((coords_b.unsqueeze(1) - coords_a_mapped.unsqueeze(2)) ** 2).sum(-1)\n",
        "    w = torch.exp(-sqd / (2 * sigma ** 2))\n",
        "    w = w / (w.sum(dim=2, keepdim=True) + 1e-8)\n",
        "    return -(w * F.log_softmax(logits, dim=-1)).sum(-1).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model + semantic head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM,\n",
        "    queries_dim=QUERIES_DIM,\n",
        "    logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512),\n",
        "    num_latents=(256, 256, 256),\n",
        "    decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "semantic_head = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "print(\"Model + semantic_head (φ dim =\", PHI_DIM, \")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "print(\"Train batches:\", len(train_loader), \"Test batches:\", len(test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop — reconstruction + contrastive, multi-density sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = list(model.parameters()) + list(fourier_encoder.parameters()) + list(semantic_head.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "step = [0]\n",
        "best_val_loss = float(\"inf\")\n",
        "CKPT_BEST = os.path.join(CHECKPOINT_DIR, \"checkpoint_semantic_token_best.pt\")\n",
        "CKPT_LAST = os.path.join(CHECKPOINT_DIR, \"checkpoint_semantic_token_last.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    fourier_encoder.train()\n",
        "    semantic_head.train()\n",
        "    total_loss = 0.0\n",
        "    total_recon = 0.0\n",
        "    total_nce = 0.0\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B = imgs.size(0)\n",
        "\n",
        "        # 4) Multi-density sampling: random mode per batch\n",
        "        context_a, n_ctx = sample_context_for_batch(imgs, coords_32, fourier_encoder, DEVICE)\n",
        "        residual_a = get_residual(model, context_a)\n",
        "\n",
        "        queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "        target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "\n",
        "        # 1) RGB output + 2) Reconstruction loss\n",
        "        rgb = get_rgb(model, queries_full, residual_a)\n",
        "        loss_recon = F.mse_loss(rgb, target_pixels) if USE_RECON_LOSS else torch.tensor(0.0, device=DEVICE)\n",
        "        loss = loss_recon\n",
        "\n",
        "        # 3) Geometry-aware contrastive (Soft InfoNCE)\n",
        "        if USE_CONTRASTIVE:\n",
        "            R, t = sample_affine_params(B, DEVICE)\n",
        "            imgs_b = apply_affine_to_image(imgs, R, t)\n",
        "            context_b, _ = sample_context_for_batch(imgs_b, coords_32, fourier_encoder, DEVICE)\n",
        "            residual_b = get_residual(model, context_b)\n",
        "            anchors_a = torch.rand(B, N_ANCHORS, 2, device=DEVICE) * 2 - 1\n",
        "            candidates_b = torch.rand(B, N_CANDIDATES, 2, device=DEVICE) * 2 - 1\n",
        "            q_a = fourier_encoder(anchors_a)\n",
        "            q_b = fourier_encoder(candidates_b)\n",
        "            phi_raw_a = get_phi_raw(model, q_a, residual_a)\n",
        "            phi_raw_b = get_phi_raw(model, q_b, residual_b)\n",
        "            phi_a = get_semantic_tokens(phi_raw_a, semantic_head)\n",
        "            phi_b = get_semantic_tokens(phi_raw_b, semantic_head)\n",
        "            lam = LAMBDA_NCE if step[0] >= NCE_RAMP_STEPS else LAMBDA_NCE * (step[0] / NCE_RAMP_STEPS)\n",
        "            loss_nce = soft_infonce_loss(phi_a, phi_b, anchors_a, candidates_b, R, t, TAU, SIGMA)\n",
        "            loss = loss + lam * loss_nce\n",
        "            total_nce += loss_nce.item()\n",
        "            step[0] += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_recon += loss_recon.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_recon = total_recon / len(train_loader)\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    semantic_head.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in test_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            B = imgs.size(0)\n",
        "            context = prepare_full_context(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, context)\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            rgb = get_rgb(model, queries_full, residual)\n",
        "            target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            val_loss += F.mse_loss(rgb, target_pixels).item()\n",
        "    val_loss /= len(test_loader)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "            \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "            \"best_val_loss\": best_val_loss,\n",
        "        }, CKPT_BEST)\n",
        "    torch.save({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "        \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "        \"best_val_loss\": best_val_loss,\n",
        "    }, CKPT_LAST)\n",
        "    nce_str = f\" NCE: {total_nce/len(train_loader):.4f}\" if USE_CONTRASTIVE else \"\"\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} recon: {avg_recon:.4f}{nce_str} val_loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Resolution-agnostic query — extract φ at arbitrary coordinates\n",
        "\n",
        "Same model; query semantic tokens at 16×16, 32×32, 64×64 (no retraining)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, res_h, res_w, device):\n",
        "    \"\"\"Query φ at (res_h, res_w) grid. residual: (B, num_latents, latent_dim) from one encoder pass.\"\"\"\n",
        "    coords = create_coordinate_grid(res_h, res_w, device)\n",
        "    B = residual.size(0)\n",
        "    queries = fourier_encoder(coords.unsqueeze(0).expand(B, -1, -1))\n",
        "    phi_raw = get_phi_raw(model, queries, residual)\n",
        "    return get_semantic_tokens(phi_raw, semantic_head)\n",
        "\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "semantic_head.eval()\n",
        "imgs, _ = next(iter(test_loader))\n",
        "imgs = imgs[:4].to(DEVICE)\n",
        "context = prepare_full_context(imgs, coords_32, fourier_encoder)\n",
        "with torch.no_grad():\n",
        "    residual = get_residual(model, context)\n",
        "    phi_16 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 16, 16, DEVICE)\n",
        "    phi_32 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 32, 32, DEVICE)\n",
        "    phi_64 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 64, 64, DEVICE)\n",
        "\n",
        "print(\"φ at 16×16:\", phi_16.shape, \"| 32×32:\", phi_32.shape, \"| 64×64:\", phi_64.shape)\n",
        "print(\"Same model, one encoder pass; resolution-agnostic token extraction.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
