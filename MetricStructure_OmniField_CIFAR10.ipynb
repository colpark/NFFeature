{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric-Structure Learning on OmniField (CIFAR-10)\n",
    "\n",
    "Same **P1–P5** metric-structure add-ons as in `MetricStructure_CIFAR10.ipynb`, but built on the **well-working OmniField** model (pretrained from `AblationCIFAR10.ipynb` / `nf_feature_models.py`) instead of the Linear/ImplicitMLP baseline. The field representation **φ** is the decoder hidden state (before RGB logits) at query coordinates.\n",
    "\n",
    "- **P1** Coordinate canonicalization (g from pooled residual)\n",
    "- **P2** Multi-scale feature heads on φ\n",
    "- **P3** Invariance under coord jitter\n",
    "- **P4** Soft InfoNCE (optional)\n",
    "- **P5** Cycle-consistency (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from nf_feature_models import (\n",
    "    CascadedPerceiverIO,\n",
    "    GaussianFourierFeatures,\n",
    "    create_coordinate_grid,\n",
    "    prepare_model_input,\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "CKPT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint_best.pt')\n",
    "if not os.path.isfile(CKPT_PATH):\n",
    "    CKPT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint_last.pt')\n",
    "assert os.path.isfile(CKPT_PATH), f'No checkpoint in {CHECKPOINT_DIR}. Train AblationCIFAR10 first.'\n",
    "print(f'Device: {DEVICE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gt_at_coords(images, coords):\n",
    "    '''images (B,C,H,W), coords (B,N,2) in [-1,1] (y,x). Returns (B,N,3).'''\n",
    "    B, C, H, W = images.shape\n",
    "    N = coords.shape[1]\n",
    "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
    "    sampled = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    return sampled.squeeze(2).permute(0, 2, 1)\n",
    "\n",
    "def make_grid_2d(h, w, device):\n",
    "    '''Returns (h*w, 2) in [-1,1].'''\n",
    "    y = torch.linspace(-1, 1, h, device=device)\n",
    "    x = torch.linspace(-1, 1, w, device=device)\n",
    "    grid = torch.stack(torch.meshgrid(y, x, indexing='ij'), dim=-1)\n",
    "    return grid.reshape(-1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OmniField config (match AblationCIFAR10 / checkpoint)\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "FOURIER_MAPPING_SIZE = 96\n",
    "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
    "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
    "QUERIES_DIM = POS_EMBED_DIM\n",
    "LOGITS_DIM = CHANNELS\n",
    "\n",
    "fourier_encoder = GaussianFourierFeatures(in_features=2, mapping_size=FOURIER_MAPPING_SIZE, scale=15.0).to(DEVICE)\n",
    "model = CascadedPerceiverIO(\n",
    "    input_dim=INPUT_DIM,\n",
    "    queries_dim=QUERIES_DIM,\n",
    "    logits_dim=LOGITS_DIM,\n",
    "    latent_dims=(256, 384, 512),\n",
    "    num_latents=(256, 256, 256),\n",
    "    decoder_ff=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "fourier_encoder.load_state_dict(ckpt['fourier_encoder_state_dict'], strict=False)\n",
    "model.eval()\n",
    "fourier_encoder.eval()\n",
    "\n",
    "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
    "print(f'Loaded {CKPT_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual(model, data):\n",
    "    '''Encoder + processor only -> (B, num_latents, latent_dim).'''\n",
    "    residual = None\n",
    "    for block in model.encoder_blocks:\n",
    "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
    "    for sa_block in model.self_attn_blocks:\n",
    "        residual = sa_block[0](residual) + residual\n",
    "        residual = sa_block[1](residual) + residual\n",
    "    return residual\n",
    "\n",
    "def get_rgb_and_phi(model, queries, residual):\n",
    "    '''Decoder up to logits; return rgb (B,N,3) and phi (B,N,QUERIES_DIM) for metric learning.'''\n",
    "    x = model.decoder_cross_attn(queries, context=residual)\n",
    "    x = x + queries\n",
    "    if model.decoder_ff is not None:\n",
    "        x = x + model.decoder_ff(x)\n",
    "    phi = x\n",
    "    rgb = model.to_logits(x)\n",
    "    return rgb, phi\n",
    "\n",
    "def omnifield_forward(images, coords, model, fourier_encoder, coords_full, device):\n",
    "    '''\n",
    "    images (B,C,H,W), coords (B,N,2). coords_full: full grid for context (e.g. 32*32, 2).\n",
    "    Returns rgb (B,N,3), phi (B,N,QUERIES_DIM), residual (B,L,D).\n",
    "    '''\n",
    "    input_data, _, _ = prepare_model_input(images, coords_full, fourier_encoder)\n",
    "    residual = get_residual(model, input_data)\n",
    "    B = coords.shape[0]\n",
    "    if coords.ndim == 2:\n",
    "        coords = coords.unsqueeze(0).expand(residual.size(0), -1, -1)\n",
    "    queries = fourier_encoder(coords)\n",
    "    rgb, phi = get_rgb_and_phi(model, queries, residual)\n",
    "    return rgb, phi, residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Canonicalizer(nn.Module):\n",
    "    def __init__(self, code_dim=16, coord_dim=2):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(code_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, coord_dim * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, coords, g):\n",
    "        '''coords (B,N,2), g (B, code_dim). Returns canonical coords (B,N,2).'''\n",
    "        params = self.mlp(g)\n",
    "        A = torch.diag_embed(torch.sigmoid(params[:, :2]) * 1.8 + 0.1)\n",
    "        b = params[:, 2:4] * 0.1\n",
    "        return torch.einsum('bnd,bde->bne', coords, A) + b.unsqueeze(1)\n",
    "\n",
    "class FeatureHeads(nn.Module):\n",
    "    def __init__(self, latent_dim, pe_dims=(8, 16, 32), head_dim=32):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for pe_freq in pe_dims:\n",
    "            pe_size = 4 * pe_freq\n",
    "            self.heads.append(nn.Sequential(\n",
    "                nn.Linear(latent_dim + pe_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, head_dim)\n",
    "            ))\n",
    "        self.head_dim = head_dim\n",
    "        self.pe_dims = pe_dims\n",
    "\n",
    "    def _pe(self, coords, max_freq):\n",
    "        b, n, _ = coords.shape\n",
    "        freqs = 2.0 ** torch.linspace(0, max_freq, max_freq, device=coords.device)\n",
    "        x = coords.unsqueeze(-1) * freqs\n",
    "        out = torch.cat([torch.sin(math.pi * x), torch.cos(math.pi * x)], dim=-1)\n",
    "        return out.reshape(b, n, -1)\n",
    "\n",
    "    def forward(self, z, coords):\n",
    "        '''z (B,N,D), coords (B,N,2). Returns list of (B,N,head_dim) L2-normalized.'''\n",
    "        out = []\n",
    "        for head, mf in zip(self.heads, self.pe_dims):\n",
    "            pe = self._pe(coords, mf)\n",
    "            feat = head(torch.cat([z, pe], dim=-1))\n",
    "            out.append(F.normalize(feat, dim=-1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'subset_size': 10000,\n",
    "    'batch_size': 32,\n",
    "    'coord_samples': 512,\n",
    "    'epochs_addon': 3,\n",
    "    'lr': 1e-3,\n",
    "    'P1': True, 'P2': True, 'P3': True, 'P4': False, 'P5': False,\n",
    "    'lambda_recon': 1.0, 'lambda_inv': 0.1, 'lambda_infonce': 0.1, 'lambda_cycle': 0.1,\n",
    "    'freeze_omnifield': True,\n",
    "}\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_sub = Subset(train_ds, list(range(min(cfg['subset_size'], len(train_ds)))))\n",
    "train_loader = DataLoader(train_sub, batch_size=cfg['batch_size'], shuffle=True, num_workers=0)\n",
    "val_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniFieldMetricWrapper(nn.Module):\n",
    "    '''Wraps pretrained OmniField + optional P1 canonicalizer and P2 feature heads.'''\n",
    "    def __init__(self, model, fourier_encoder, coords_full, cfg, device):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.fourier_encoder = fourier_encoder\n",
    "        self.coords_full = coords_full\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        self.residual_dim = 512\n",
    "        self.queries_dim = QUERIES_DIM\n",
    "        self.canon = Canonicalizer(code_dim=16, coord_dim=2) if cfg.get('P1') else None\n",
    "        self.g_proj = nn.Linear(self.residual_dim, 16) if cfg.get('P1') else None\n",
    "        self.heads = FeatureHeads(self.queries_dim, pe_dims=(8, 16, 32), head_dim=32) if cfg.get('P2') else None\n",
    "\n",
    "    def forward(self, images, coords):\n",
    "        B, N = coords.shape[0], coords.shape[1]\n",
    "        input_data, _, _ = prepare_model_input(images, self.coords_full, self.fourier_encoder)\n",
    "        residual = get_residual(self.model, input_data)\n",
    "        g = self.g_proj(residual.mean(1)) if self.g_proj is not None else None\n",
    "        x = coords\n",
    "        if self.canon is not None and g is not None:\n",
    "            x = self.canon(coords, g)\n",
    "        queries = self.fourier_encoder(x)\n",
    "        rgb, phi = get_rgb_and_phi(self.model, queries, residual)\n",
    "        phi_list = self.heads(phi, x) if self.heads is not None else []\n",
    "        return rgb, phi, phi_list, x\n",
    "}\n",
    "\n",
    "wrapper = OmniFieldMetricWrapper(model, fourier_encoder, coords_32, cfg, DEVICE).to(DEVICE)\n",
    "if cfg.get('freeze_omnifield'):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in fourier_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "params = [p for p in wrapper.parameters() if p.requires_grad]\n",
    "print(f'Trainable params: {sum(p.numel() for p in params)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_psnr_omnifield(wrapper, loader, device, grid_size=32):\n",
    "    wrapper.eval()\n",
    "    model.eval()\n",
    "    fourier_encoder.eval()\n",
    "    grid = make_grid_2d(grid_size, grid_size, device).unsqueeze(0)\n",
    "    mse_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            B = imgs.size(0)\n",
    "            coords = grid.expand(B, -1, -1)\n",
    "            rgb, _, _, x = wrapper(imgs, coords)\n",
    "            gt = sample_gt_at_coords(imgs, x)\n",
    "            mse_sum += F.mse_loss(rgb, gt, reduction='sum').item()\n",
    "            n += B * grid.size(1)\n",
    "    mse = mse_sum / max(n, 1)\n",
    "    return 10 * math.log10(1.0 / (mse + 1e-10))\n",
    "\n",
    "print(f'OmniField (no add-ons) val PSNR: {eval_psnr_omnifield(wrapper, val_loader, DEVICE):.2f} dB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train add-ons (P1–P3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(params, lr=cfg['lr'])\n",
    "for ep in range(cfg['epochs_addon']):\n",
    "    wrapper.train()\n",
    "    if not cfg.get('freeze_omnifield'):\n",
    "        model.train()\n",
    "        fourier_encoder.train()\n",
    "    total = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        B, C, H, W = imgs.shape\n",
    "        N = cfg['coord_samples']\n",
    "        coords = (torch.rand(B, N, 2, device=DEVICE) * 2 - 1)\n",
    "        rgb, phi, phi_list, x = wrapper(imgs, coords)\n",
    "        gt = sample_gt_at_coords(imgs, x)\n",
    "        loss = cfg['lambda_recon'] * F.mse_loss(rgb, gt)\n",
    "        if cfg.get('P3') and phi_list:\n",
    "            jitter = torch.randn_like(coords, device=DEVICE) * 0.05\n",
    "            coords_j = (coords + jitter).clamp(-1, 1)\n",
    "            _, _, phi_j, _ = wrapper(imgs, coords_j)\n",
    "            for i in range(min(2, len(phi_list))):\n",
    "                loss = loss + cfg['lambda_inv'] * F.mse_loss(phi_list[i], phi_j[i])\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    print(f'Add-on epoch {ep+1} loss: {total/len(train_loader):.4f}')\n",
    "\n",
    "print(f'Full (OmniField + P1+P2+P3) val PSNR: {eval_psnr_omnifield(wrapper, val_loader, DEVICE):.2f} dB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.eval()\n",
    "imgs, _ = next(iter(val_loader))\n",
    "imgs = imgs[:8].to(DEVICE)\n",
    "B, C, H, W = imgs.shape\n",
    "grid = make_grid_2d(32, 32, DEVICE).unsqueeze(0).expand(B, -1, -1)\n",
    "with torch.no_grad():\n",
    "    rgb, phi, phi_list, x = wrapper(imgs, grid)\n",
    "rgb = rgb.view(B, 32, 32, 3).permute(0, 3, 1, 2)\n",
    "recon_vis = rgb.cpu().float()\n",
    "for b in range(B):\n",
    "    rb = recon_vis[b]\n",
    "    lo, hi = rb.min().item(), rb.max().item()\n",
    "    if hi > lo:\n",
    "        recon_vis[b] = (rb - lo) / (hi - lo)\n",
    "    else:\n",
    "        recon_vis[b] = rb\n",
    "recon_vis = recon_vis.clamp(0, 1)\n",
    "fig, axs = plt.subplots(3, 4, figsize=(12, 9))\n",
    "for i in range(4):\n",
    "    axs[0, i].imshow(imgs[i].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
    "    axs[0, i].set_title('GT')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[1, i].imshow(recon_vis[i].permute(1, 2, 0).numpy())\n",
    "    axs[1, i].set_title('Recon (norm)')\n",
    "    axs[1, i].axis('off')\n",
    "    diff = (imgs[i].cpu() - recon_vis[i]).abs().permute(1, 2, 0).numpy()\n",
    "    axs[2, i].imshow(diff)\n",
    "    axs[2, i].set_title('|GT - Recon|')\n",
    "    axs[2, i].axis('off')\n",
    "plt.suptitle('OmniField + metric add-ons: 32x32 Reconstruction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('omnifield_metric_recon.png', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if phi_list:\n",
    "    z_np = phi_list[0].cpu().numpy()\n",
    "    z_flat = z_np.reshape(-1, z_np.shape[-1])\n",
    "    U, S, Vt = np.linalg.svd(z_flat, full_matrices=False)\n",
    "    proj = (z_flat @ Vt[:, :3]).reshape(B, 32, 32, 3)\n",
    "    p1, p99 = np.percentile(proj, [1, 99])\n",
    "    proj = np.clip((proj - p1) / (p99 - p1 + 1e-8), 0, 1)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    for i in range(4):\n",
    "        axs[i].imshow(proj[i])\n",
    "        axs[i].set_title(f'φ_L PCA #{i+1}')\n",
    "        axs[i].axis('off')\n",
    "    plt.suptitle('OmniField φ (coarse head) -> PCA')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if phi_list:\n",
    "    jitter_stds = [0.0, 0.02, 0.05, 0.1]\n",
    "    drifts = []\n",
    "    with torch.no_grad():\n",
    "        _, _, phi0, _ = wrapper(imgs[:4], grid[:4])\n",
    "        for sig in jitter_stds:\n",
    "            j = torch.randn_like(grid[:4], device=DEVICE) * sig\n",
    "            _, _, phi_j, _ = wrapper(imgs[:4], (grid[:4] + j).clamp(-1, 1))\n",
    "            d = (1 - (phi0[0] * phi_j[0]).sum(-1).mean().item()) if phi0 else 0.0\n",
    "            drifts.append(d)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(jitter_stds, drifts, 'o-')\n",
    "    plt.xlabel('Jitter std')\n",
    "    plt.ylabel('Mean 1 - cos(φ, φ_j)')\n",
    "    plt.title('Invariance: feature drift vs jitter (OmniField φ)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist (P1–P5 on OmniField)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = {\n",
    "    'P1 canonicalizer': 'stable coordinate domain; g from pooled residual',\n",
    "    'P2 feature heads': 'φ = decoder hidden state; L/M/H band-limited heads',\n",
    "    'P3 invariance': 'drift under jitter penalized',\n",
    "    'P4 Soft InfoNCE': 'optional; enable with P4: True and two-view batch',\n",
    "    'P5 cycle': 'optional; enable with P5: True',\n",
    "}\n",
    "for k, v in checklist.items():\n",
    "    print(f'- {k} -> {v}')\n",
    "print('\\nBaseline: OmniField (pretrained). Add-ons trained on top; recon preserved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
