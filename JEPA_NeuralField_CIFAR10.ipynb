{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JEPA (Joint-Embedding Predictive Architecture) — Neural Field on CIFAR-10\n",
        "\n",
        "Learn representations by **predicting target embeddings from context embeddings** under masking, **no contrastive negatives**. Continuity-aware field gives domain-continuous tokens φ(x); JEPA predicts φ at masked coordinates from visible ones.\n",
        "\n",
        "**Distinguishable components:**\n",
        "\n",
        "1. **Dual output** — f(x) → [RGB, φ(x)] (same as semantic-token notebook)\n",
        "2. **Context / target split** — mask coords into visible (context) and predicted (target)\n",
        "3. **EMA target encoder** — φ_target = stop_grad(φ_θ̄(X_t)); θ̄ updated by momentum\n",
        "4. **Cross-attention predictor** — ẑ_t = P(z_c, X_c, X_t); predicts target embeddings from context\n",
        "5. **JEPA loss** — cosine or L2 on norm(ẑ_t) vs norm(z_t); no negatives\n",
        "6. **Optional VICReg** — variance + covariance on φ_online to avoid collapse\n",
        "7. **Optional RGB auxiliary** — small λ reconstruction so semantics aren't dominated by pixels\n",
        "\n",
        "Checkpoints: **checkpoint_jepa_best.pt** / **_last.pt**\n",
        "\n",
        "**I-JEPA faithful**: With **TARGET_FROM_FULL_CONTEXT=True** (default), target = φ_EMA(**full image**) at target coords, matching `ijepa/src/train.py` (target_encoder sees full image; we use full grid). Context branch unchanged: online encoder sees context only; predictor predicts from context tokens. Loss in representation space; EMA update of target encoder.\n",
        "\n",
        "**Speed**: Use `num_workers=2` and **RGB_QUERY_RES=16** to avoid >5 min/epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(\"Device:\", DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "N_FULL = IMAGE_SIZE * IMAGE_SIZE\n",
        "\n",
        "# JEPA: context vs target coord split\n",
        "# Mask style: \"ijepa_block\" = I-JEPA-style block-level (patch grid, pred blocks + enc blocks in complement); \"random\" = random pixel split\n",
        "MASK_STYLE = \"ijepa_block\"\n",
        "N_CONTEXT = 512\n",
        "N_TARGET = 256\n",
        "USE_BLOCK_MASK = MASK_STYLE == \"ijepa_block\"  # legacy flag for viz\n",
        "\n",
        "# I-JEPA-style block masking (see ijepa/src/masks/multiblock.py)\n",
        "PATCH_SIZE = 4\n",
        "ENC_MASK_SCALE = (0.85, 1.0)\n",
        "PRED_MASK_SCALE = (0.15, 0.25)\n",
        "ASPECT_RATIO = (0.75, 1.5)\n",
        "N_ENC_MASKS = 1\n",
        "N_PRED_MASKS = 2\n",
        "ALLOW_OVERLAP = False\n",
        "MIN_KEEP = 4\n",
        "\n",
        "# Verbose training: log every N batches (0 = only epoch summary)\n",
        "VERBOSE = True\n",
        "LOG_EVERY = 20\n",
        "\n",
        "# I-JEPA faithful: target = EMA(full image) at target coords (True). Legacy: target = EMA(context) at target (False).\n",
        "TARGET_FROM_FULL_CONTEXT = True\n",
        "\n",
        "# EMA target: official I-JEPA ramps 0.996 -> 1.0 so target *slows down* and stabilizes\n",
        "EMA_MOMENTUM = 0.996\n",
        "EMA_MOMENTUM_RAMP = True\n",
        "EMA_MOMENTUM_START = 0.996\n",
        "EMA_MOMENTUM_END = 0.999\n",
        "\n",
        "# Losses\n",
        "USE_JEPA_LOSS = True\n",
        "# I-JEPA geometry: LayerNorm on target and predictor output, then smooth_l1 (no cosine)\n",
        "USE_LAYERNORM_JEPA = True\n",
        "JEPA_LOSS_TYPE = \"smooth_l1\"\n",
        "USE_RGB_AUX = True\n",
        "LAMBDA_RGB = 0.1\n",
        "RGB_QUERY_RES = 16\n",
        "USE_VICREG = False\n",
        "LAMBDA_VICREG = 0.01\n",
        "VICREG_SIM_WEIGHT = 25.0\n",
        "VICREG_VAR_WEIGHT = 25.0\n",
        "VICREG_COV_WEIGHT = 1.0\n",
        "\n",
        "PHI_DIM = 128\n",
        "PREDICTOR_DIM = 256\n",
        "PREDICTOR_HEADS = 4\n",
        "\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(\"N_FULL:\", N_FULL, \"| MASK_STYLE:\", MASK_STYLE, \"| TARGET_FROM_FULL_CONTEXT:\", TARGET_FROM_FULL_CONTEXT)\n",
        "print(\"JEPA_LOSS_TYPE:\", JEPA_LOSS_TYPE, \"| USE_LAYERNORM_JEPA:\", USE_LAYERNORM_JEPA, \"| USE_VICREG:\", USE_VICREG, \"| EMA_ramp:\", EMA_MOMENTUM_RAMP, end=\"\")\n",
        "if EMA_MOMENTUM_RAMP:\n",
        "    print(\" [%s, %s]\" % (EMA_MOMENTUM_START, EMA_MOMENTUM_END))\n",
        "else:\n",
        "    print()\n",
        "if MASK_STYLE == \"ijepa_block\":\n",
        "    print(\"  I-JEPA blocks: patch_size=%d enc_scale=%s pred_scale=%s aspect=%s n_enc=%d n_pred=%d\" % (PATCH_SIZE, ENC_MASK_SCALE, PRED_MASK_SCALE, ASPECT_RATIO, N_ENC_MASKS, N_PRED_MASKS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Field output: RGB + φ (shared with semantic-token design)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    sampled = F.grid_sample(images, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\n",
        "    return sampled.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def get_residual(model, context):\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=context, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def decoder_forward(model, queries, residual):\n",
        "    \"\"\"Each query attends only to residual (context); no query-to-query attention, so neural field stays alive: φ(x) depends only on (x, residual), not on other query positions.\"\"\"\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return x\n",
        "\n",
        "def get_rgb(model, queries, residual):\n",
        "    return model.to_logits(decoder_forward(model, queries, residual))\n",
        "\n",
        "def get_phi_raw(model, queries, residual):\n",
        "    return decoder_forward(model, queries, residual)\n",
        "\n",
        "def get_semantic_tokens(phi_raw, semantic_head):\n",
        "    return F.normalize(semantic_head(phi_raw), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Context/target split — I-JEPA-style block masking or random\n",
        "\n",
        "**Encoder sees where to decode:** We feed the encoder `[context tokens | target-position placeholders]` (placeholders = target coords + learnable `mask_rgb`). So the encoder knows which positions will be decoded, matching I-JEPA's predictor receiving mask tokens with position embeddings for target positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _patch_to_pixel_indices(patch_indices_flat, h_patch, w_patch, patch_size, image_size, device):\n",
        "    \"\"\"Convert flat patch indices to flat pixel indices (C order: row, then col).\"\"\"\n",
        "    pixels_per_patch = patch_size * patch_size\n",
        "    pixel_indices = []\n",
        "    for p_flat in patch_indices_flat:\n",
        "        pi, pj = p_flat // w_patch, p_flat % w_patch\n",
        "        for di in range(patch_size):\n",
        "            for dj in range(patch_size):\n",
        "                i, j = pi * patch_size + di, pj * patch_size + dj\n",
        "                pixel_indices.append(i * image_size + j)\n",
        "    return torch.tensor(pixel_indices, device=device, dtype=torch.long)\n",
        "\n",
        "def sample_ijepa_block_indices(image_size, patch_size, enc_mask_scale, pred_mask_scale, aspect_ratio,\n",
        "                               n_enc_masks, n_pred_masks, allow_overlap, min_keep, device, generator=None):\n",
        "    \"\"\"\n",
        "    I-JEPA-style block masking (ijepa/src/masks/multiblock.py).\n",
        "    Returns (idx_c, idx_t) as flat pixel indices: context = encoder sees, target = predictor predicts.\n",
        "    \"\"\"\n",
        "    h_patch = image_size // patch_size\n",
        "    w_patch = image_size // patch_size\n",
        "    n_patches = h_patch * w_patch\n",
        "    if generator is None:\n",
        "        generator = torch.Generator(device=device)\n",
        "\n",
        "    def sample_block_size(scale_lo, scale_hi, aspect_lo, aspect_hi):\n",
        "        s = scale_lo + torch.rand(1, device=device, generator=generator).item() * (scale_hi - scale_lo)\n",
        "        ar = aspect_lo + torch.rand(1, device=device, generator=generator).item() * (aspect_hi - aspect_lo)\n",
        "        max_keep = int(n_patches * s)\n",
        "        h = max(1, min(h_patch - 1, int(round((max_keep * ar) ** 0.5))))\n",
        "        w = max(1, min(w_patch - 1, int(round((max_keep / ar) ** 0.5))))\n",
        "        return h, w\n",
        "\n",
        "    def sample_one_block(bh, bw, acceptable_region=None):\n",
        "        \"\"\"acceptable_region: (H, W) bool, True = can place block. None = anywhere.\"\"\"\n",
        "        for _ in range(30):\n",
        "            top = torch.randint(0, h_patch - bh + 1, (1,), device=device, generator=generator).item()\n",
        "            left = torch.randint(0, w_patch - bw + 1, (1,), device=device, generator=generator).item()\n",
        "            block = torch.zeros(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "            block[top : top + bh, left : left + bw] = True\n",
        "            if acceptable_region is not None:\n",
        "                if not (block & acceptable_region == block).all():\n",
        "                    continue\n",
        "            idx = torch.nonzero(block.flatten(), as_tuple=False).squeeze(-1)\n",
        "            if len(idx) >= min_keep:\n",
        "                complement = torch.ones(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "                complement[top : top + bh, left : left + bw] = False\n",
        "                return idx, complement\n",
        "        return None, None\n",
        "\n",
        "    # 1) Target (pred) blocks\n",
        "    ph, pw = sample_block_size(pred_mask_scale[0], pred_mask_scale[1], aspect_ratio[0], aspect_ratio[1])\n",
        "    all_pred_patches = set()\n",
        "    pred_complements = []\n",
        "    for _ in range(n_pred_masks):\n",
        "        idx, comp = sample_one_block(ph, pw, None)\n",
        "        if idx is None:\n",
        "            continue\n",
        "        pred_complements.append(comp)\n",
        "        for i in idx.cpu().tolist():\n",
        "            all_pred_patches.add(i)\n",
        "    pred_patches_flat = list(all_pred_patches)\n",
        "    if len(pred_patches_flat) < min_keep:\n",
        "        pred_patches_flat = list(range(n_patches))[: min_keep + 1]\n",
        "    idx_t = _patch_to_pixel_indices(pred_patches_flat, h_patch, w_patch, patch_size, image_size, device)\n",
        "\n",
        "    # 2) Context (enc) blocks: only in complement of target\n",
        "    acceptable = None if allow_overlap else torch.ones(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "    if not allow_overlap and pred_complements:\n",
        "        for c in pred_complements:\n",
        "            acceptable = acceptable & c\n",
        "    eh, ew = sample_block_size(enc_mask_scale[0], enc_mask_scale[1], 1.0, 1.0)\n",
        "    all_enc_patches = set()\n",
        "    for _ in range(n_enc_masks):\n",
        "        idx, _ = sample_one_block(eh, ew, acceptable)\n",
        "        if idx is not None:\n",
        "            for i in idx.cpu().tolist():\n",
        "                all_enc_patches.add(i)\n",
        "    enc_patches_flat = list(all_enc_patches)\n",
        "    if len(enc_patches_flat) < min_keep:\n",
        "        enc_patches_flat = [k for k in range(n_patches) if k not in all_pred_patches][: min_keep + 1]\n",
        "    if not enc_patches_flat:\n",
        "        enc_patches_flat = [k for k in range(n_patches) if k not in all_pred_patches]\n",
        "    idx_c = _patch_to_pixel_indices(enc_patches_flat, h_patch, w_patch, patch_size, image_size, device)\n",
        "\n",
        "    return idx_c, idx_t\n",
        "\n",
        "def sample_context_target_indices(coords_full, n_context, n_target, device, block_mask=False,\n",
        "                                  mask_style=\"random\", image_size=32, patch_size=4,\n",
        "                                  enc_mask_scale=(0.85, 1.0), pred_mask_scale=(0.15, 0.25),\n",
        "                                  aspect_ratio=(0.75, 1.5), n_enc_masks=1, n_pred_masks=2,\n",
        "                                  allow_overlap=False, min_keep=4):\n",
        "    \"\"\"Return idx_c, idx_t disjoint. mask_style='ijepa_block' uses I-JEPA block masking.\"\"\"\n",
        "    n_total = coords_full.size(0)\n",
        "    if mask_style == \"ijepa_block\":\n",
        "        return sample_ijepa_block_indices(\n",
        "            image_size, patch_size, enc_mask_scale, pred_mask_scale, aspect_ratio,\n",
        "            n_enc_masks, n_pred_masks, allow_overlap, min_keep, device)\n",
        "    if block_mask:\n",
        "        h, w = int(math.sqrt(n_total)), int(math.sqrt(n_total))\n",
        "        nh, nw = max(1, h // 4), max(1, w // 4)\n",
        "        top = random.randint(0, h - nh)\n",
        "        left = random.randint(0, w - nw)\n",
        "        target_flat = []\n",
        "        for i in range(top, top + nh):\n",
        "            for j in range(left, left + nw):\n",
        "                target_flat.append(i * w + j)\n",
        "        idx_t = torch.tensor(target_flat, device=device, dtype=torch.long)\n",
        "        idx_c = torch.tensor([k for k in range(n_total) if k not in set(target_flat)], device=device)\n",
        "        if idx_c.size(0) > n_context:\n",
        "            idx_c = idx_c[torch.randperm(idx_c.size(0), device=device)[:n_context]]\n",
        "    else:\n",
        "        perm = torch.randperm(n_total, device=device)\n",
        "        idx_c = perm[:n_context]\n",
        "        idx_t = perm[n_context : n_context + n_target]\n",
        "    return idx_c, idx_t\n",
        "\n",
        "def prepare_context_input(images, coords_full, fourier_encoder, idx_c, device):\n",
        "    \"\"\"Build (B, len(idx_c), INPUT_DIM) from pixels at context coords only.\"\"\"\n",
        "    B = images.size(0)\n",
        "    coords_c = coords_full[idx_c]\n",
        "    pixels = sample_gt_at_coords(images, coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels, pos], dim=-1)\n",
        "\n",
        "def prepare_jepa_encoder_input(images, coords_full, fourier_encoder, idx_c, idx_t, device, mask_rgb):\n",
        "    \"\"\"\n",
        "    Build encoder input as [context tokens | target-position placeholders].\n",
        "    So the encoder knows WHERE it will be decoded (target coords), matching I-JEPA's\n",
        "    predictor receiving mask tokens with position embeddings for target positions.\n",
        "    Returns (B, N_c + N_t, INPUT_DIM); mask_rgb is (1, 1, CHANNELS) learnable placeholder for RGB at target.\n",
        "    \"\"\"\n",
        "    B = images.size(0)\n",
        "    context_input = prepare_context_input(images, coords_full, fourier_encoder, idx_c, device)\n",
        "    coords_t = coords_full[idx_t]\n",
        "    pos_t = fourier_encoder(coords_t.unsqueeze(0).expand(B, -1, -1))\n",
        "    rgb_t = mask_rgb.expand(B, coords_t.size(0), -1)\n",
        "    target_placeholder = torch.cat([rgb_t, pos_t], dim=-1)\n",
        "    return torch.cat([context_input, target_placeholder], dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Cross-attention predictor — P(z_c, X_c, X_t) → ẑ_t\n",
        "\n",
        "Query from target coords; key/value from context tokens. Field-native."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class JEPAPredictor(nn.Module):\n",
        "    \"\"\"Predict target embeddings from context: ẑ_t = Attn(q=embed(X_t), k=z_c, v=z_c).\"\"\"\n",
        "\n",
        "    def __init__(self, coord_embed_dim, phi_dim, pred_dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.to_q = nn.Linear(coord_embed_dim, pred_dim)\n",
        "        self.to_kv = nn.Linear(phi_dim, pred_dim * 2)\n",
        "        self.to_out = nn.Linear(pred_dim, phi_dim)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = pred_dim // num_heads\n",
        "\n",
        "    def forward(self, coords_t_embed, z_c):\n",
        "        B, N_t, _ = coords_t_embed.shape\n",
        "        _, N_c, _ = z_c.shape\n",
        "        q = self.to_q(coords_t_embed).view(B, N_t, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        kv = self.to_kv(z_c).view(B, N_c, 2, self.num_heads, self.d_head)\n",
        "        k, v = kv[:, :, 0].transpose(1, 2), kv[:, :, 1].transpose(1, 2)\n",
        "        scale = self.d_head ** -0.5\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(B, N_t, -1)\n",
        "        return self.to_out(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) JEPA loss (no negatives) + optional VICReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def jepa_cosine_loss(z_pred, z_target):\n",
        "    \"\"\"1 - cos(norm(z_pred), z_target); z_target already normalized.\"\"\"\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    return (1 - (z_pred * z_target).sum(dim=-1)).mean()\n",
        "\n",
        "def jepa_l2_loss(z_pred, z_target):\n",
        "    \"\"\"L2 in normalized space = 2*(1-cos); z_target already normalized.\"\"\"\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    return (2 - 2 * (z_pred * z_target).sum(dim=-1)).mean()\n",
        "\n",
        "def jepa_smooth_l1_loss(z_pred, z_target_raw):\n",
        "    \"\"\"Smooth L1 between predictor output and unnormalized target (I-JEPA paper).\"\"\"\n",
        "    return F.smooth_l1_loss(z_pred, z_target_raw).mean()\n",
        "\n",
        "def vicreg_loss(z, sim_weight=25.0, var_weight=25.0, cov_weight=1.0):\n",
        "    \"\"\"Variance + covariance regularization to avoid collapse (no negatives).\"\"\"\n",
        "    B, N, D = z.shape\n",
        "    z = z.reshape(B * N, D)\n",
        "    std = z.std(dim=0) + 1e-4\n",
        "    var_loss = torch.mean(F.relu(1 - std))\n",
        "    z_centered = z - z.mean(dim=0)\n",
        "    cov = (z_centered.T @ z_centered) / (z.size(0) - 1)\n",
        "    cov_loss = (cov.pow(2).sum() - cov.diag().pow(2).sum()) / D\n",
        "    return var_weight * var_loss + cov_weight * cov_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Online encoder + EMA target + predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def copy_ema(source, target, momentum=0.996):\n",
        "    for p_s, p_t in zip(source.parameters(), target.parameters()):\n",
        "        p_t.data.mul_(momentum).add_(p_s.data, alpha=1 - momentum)\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "semantic_head = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "\n",
        "model_ema = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "fourier_ema = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "semantic_head_ema = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "model_ema.load_state_dict(model.state_dict())\n",
        "fourier_ema.load_state_dict(fourier_encoder.state_dict())\n",
        "semantic_head_ema.load_state_dict(semantic_head.state_dict())\n",
        "\n",
        "# Learnable [MASK] for target positions: encoder sees (coord_t, mask_rgb) so it knows WHERE to decode (I-JEPA-style).\n",
        "mask_rgb = nn.Parameter(torch.zeros(1, 1, CHANNELS).to(DEVICE))\n",
        "\n",
        "predictor = JEPAPredictor(POS_EMBED_DIM, PHI_DIM, PREDICTOR_DIM, num_heads=PREDICTOR_HEADS).to(DEVICE)\n",
        "print(\"Online + EMA target + predictor; encoder gets target-position placeholders (mask_rgb)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize inputs: full image, context encoder, target encoder, and masks\n",
        "\n",
        "Below we take one batch and show **what each branch sees** so the data flow is transparent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One batch, one fixed mask for clarity\n",
        "imgs_viz, _ = next(iter(train_loader))\n",
        "imgs_viz = imgs_viz.to(DEVICE)\n",
        "B_viz = min(4, imgs_viz.size(0))\n",
        "imgs_viz = imgs_viz[:B_viz]\n",
        "H_viz, W_viz = IMAGE_SIZE, IMAGE_SIZE\n",
        "N_full = H_viz * W_viz\n",
        "\n",
        "idx_c, idx_t = sample_context_target_indices(\n",
        "    coords_32, N_CONTEXT, N_TARGET, DEVICE, block_mask=USE_BLOCK_MASK, mask_style=MASK_STYLE,\n",
        "    image_size=IMAGE_SIZE, patch_size=PATCH_SIZE, enc_mask_scale=ENC_MASK_SCALE,\n",
        "    pred_mask_scale=PRED_MASK_SCALE, aspect_ratio=ASPECT_RATIO, n_enc_masks=N_ENC_MASKS,\n",
        "    n_pred_masks=N_PRED_MASKS, allow_overlap=ALLOW_OVERLAP, min_keep=MIN_KEEP)\n",
        "# Build mask images: green = context only, red = target only\n",
        "mask_c_2d = torch.zeros(N_full, device=DEVICE, dtype=torch.bool)\n",
        "mask_c_2d[idx_c] = True\n",
        "mask_t_2d = torch.zeros(N_full, device=DEVICE, dtype=torch.bool)\n",
        "mask_t_2d[idx_t] = True\n",
        "mask_c_2d = mask_c_2d.view(H_viz, W_viz).cpu().numpy()\n",
        "mask_t_2d = mask_t_2d.view(H_viz, W_viz).cpu().numpy()\n",
        "\n",
        "# Full image: what the target encoder (EMA) sees when TARGET_FROM_FULL_CONTEXT=True\n",
        "full_imgs = imgs_viz  # (B, 3, H, W)\n",
        "\n",
        "# Context-only \"image\": pixels at context coords, gray (0.5) at target coords (what online encoder sees)\n",
        "pixels_flat = rearrange(imgs_viz, \"b c h w -> b (h w) c\")\n",
        "context_only = torch.ones(B_viz, N_full, 3, device=DEVICE) * 0.5  # gray\n",
        "context_only[:, idx_c] = pixels_flat[:, idx_c]\n",
        "context_only = context_only.view(B_viz, H_viz, W_viz, 3).permute(0, 3, 1, 2)\n",
        "\n",
        "# Target-positions \"image\": pixels only at target coords, gray elsewhere (where we predict φ)\n",
        "target_only = torch.ones(B_viz, N_full, 3, device=DEVICE) * 0.5\n",
        "target_only[:, idx_t] = pixels_flat[:, idx_t]\n",
        "target_only = target_only.view(B_viz, H_viz, W_viz, 3).permute(0, 3, 1, 2)\n",
        "\n",
        "# Mask overlay: green = context, red = target (same mask for all samples in batch)\n",
        "overlay = torch.zeros(B_viz, 3, H_viz, W_viz, device=DEVICE)\n",
        "m_t = torch.from_numpy(mask_t_2d).float().to(DEVICE).unsqueeze(0).expand(B_viz, -1, -1)\n",
        "m_c = torch.from_numpy(mask_c_2d).float().to(DEVICE).unsqueeze(0).expand(B_viz, -1, -1)\n",
        "overlay[:, 0] = m_t  # R where target\n",
        "overlay[:, 1] = m_c  # G where context\n",
        "\n",
        "def to_display(img_bchw):\n",
        "    \"\"\"(B,C,H,W) in [-1,1] -> (H,W,C) in [0,1] for one image.\"\"\"\n",
        "    x = img_bchw[0].cpu().permute(1, 2, 0).numpy()\n",
        "    return np.clip(x * 0.5 + 0.5, 0, 1)\n",
        "\n",
        "fig, axes = plt.subplots(5, B_viz, figsize=(3 * B_viz, 14))\n",
        "if B_viz == 1:\n",
        "    axes = axes[:, np.newaxis]\n",
        "titles_row = [\n",
        "    \"Full image (input to target encoder / EMA when TARGET_FROM_FULL_CONTEXT=True)\",\n",
        "    \"Context only (input to online context encoder)\",\n",
        "    \"Target positions (pixels at coords where we predict φ)\",\n",
        "    \"Mask: green = context, red = target\",\n",
        "    \"Context (green) & target (red) coords on full image\",\n",
        "]\n",
        "# Coords in [-1,1] -> display: (x+1)/2 * W for col, (1-y)/2 * H for row (y up)\n",
        "coords_c_np = coords_32[idx_c].cpu().numpy()\n",
        "coords_t_np = coords_32[idx_t].cpu().numpy()\n",
        "for b in range(B_viz):\n",
        "    axes[0, b].imshow(to_display(full_imgs[b : b + 1]))\n",
        "    axes[0, b].set_title(f\"Sample {b+1}\" if b == 0 else \"\")\n",
        "    axes[0, b].axis(\"off\")\n",
        "    axes[1, b].imshow(to_display(context_only[b : b + 1]))\n",
        "    axes[1, b].axis(\"off\")\n",
        "    axes[2, b].imshow(to_display(target_only[b : b + 1]))\n",
        "    axes[2, b].axis(\"off\")\n",
        "    axes[3, b].imshow(to_display(overlay[b : b + 1]))\n",
        "    axes[3, b].axis(\"off\")\n",
        "    # Scatter: coords (x,y) in [-1,1], imshow has (0,0) top-left, x=col, y=row\n",
        "    axes[4, b].imshow(to_display(full_imgs[b : b + 1]))\n",
        "    col_c = (coords_c_np[:, 0] + 1) / 2 * (W_viz - 1)\n",
        "    row_c = (1 - coords_c_np[:, 1]) / 2 * (H_viz - 1)\n",
        "    col_t = (coords_t_np[:, 0] + 1) / 2 * (W_viz - 1)\n",
        "    row_t = (1 - coords_t_np[:, 1]) / 2 * (H_viz - 1)\n",
        "    axes[4, b].scatter(col_c, row_c, c=\"lime\", s=8, alpha=0.8, label=\"context\")\n",
        "    axes[4, b].scatter(col_t, row_t, c=\"red\", s=8, alpha=0.8, label=\"target\")\n",
        "    axes[4, b].axis(\"off\")\n",
        "for r in range(5):\n",
        "    axes[r, 0].set_ylabel(titles_row[r], fontsize=10)\n",
        "plt.suptitle(\"JEPA inputs: what each branch sees (one batch)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print shapes and counts\n",
        "context_input_viz = prepare_context_input(imgs_viz, coords_32, fourier_encoder, idx_c, DEVICE)\n",
        "full_input_viz, _, _ = prepare_model_input(imgs_viz, coords_32, fourier_encoder)\n",
        "print(\"Shapes:\")\n",
        "print(\"  Full image (batch):\", tuple(full_imgs.shape))\n",
        "print(\"  full_input (for EMA/target encoder when TARGET_FROM_FULL_CONTEXT):\", tuple(full_input_viz.shape), \"→ [B, N_FULL, CHANNELS+POS]\")\n",
        "print(\"  context_input (for online encoder):\", tuple(context_input_viz.shape), \"→ [B, N_CONTEXT, CHANNELS+POS]\")\n",
        "print(\"  Context coords:\", len(idx_c), \"| Target coords:\", len(idx_t))\n",
        "print(\"  TARGET_FROM_FULL_CONTEXT:\", TARGET_FROM_FULL_CONTEXT)\n",
        "if TARGET_FROM_FULL_CONTEXT:\n",
        "    print(\"    → Target encoder (EMA) sees: full image. Target = φ_EMA(full image) at target coords.\")\n",
        "else:\n",
        "    print(\"    → Target encoder (EMA) sees: context only (same as context encoder input). Target = φ_EMA(context) at target coords.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Target of the network (loss):** The predictor outputs **z_pred** (predicted φ at target coords); the target is **z_t_target** = φ_EMA at those same coords (no grad). Loss = cosine(z_pred, z_t_target). So we do *not* supervise pixels at target positions—we supervise **representation space** at target coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OmniField + JEPA: what the predictor and target encoder do\n",
        "\n",
        "**Neural field (OmniField):** One encoder consumes a set of (coord, value) tokens and produces a **residual**; a **decoder** is queried at arbitrary coordinates: `φ(x) = decoder(x, residual)`. So the field is the map **x ↦ φ(x)**. Each decode is independent (query attends only to residual, not to other queries).\n",
        "\n",
        "---\n",
        "\n",
        "### Target encoder (EMA branch)\n",
        "\n",
        "- **Who:** `model_ema` + `fourier_ema` + `semantic_head_ema` (no grad; updated by EMA copy from online).\n",
        "- **Input:**  \n",
        "  - If **TARGET_FROM_FULL_CONTEXT**: full image → `full_input` = [all pixels at full grid, fourier(coords_32)] → encoder sees **entire image**.  \n",
        "  - Else: same as online encoder input (context + target placeholders).\n",
        "- **Computation:**  \n",
        "  `residual_ema = get_residual(model_ema, full_input)`  \n",
        "  then **query the field at target coords only:**  \n",
        "  `queries_t = fourier_ema(coords_t)`  \n",
        "  `phi_t_raw = decoder(model_ema, queries_t, residual_ema)`  \n",
        "  `z_t_target_raw = semantic_head_ema(phi_t_raw)`  \n",
        "  (optional LayerNorm for loss).\n",
        "- **Role:** Produces the **target representation** at the positions we want to predict: “what would the (slowly updated) field output at those coordinates if it had seen the full image?” So the target encoder **defines the regression target** in representation space; the online branch is trained to match it.\n",
        "\n",
        "---\n",
        "\n",
        "### Predictor\n",
        "\n",
        "- **Who:** `JEPAPredictor` (cross-attention: q from target coord embeddings, k/v from context tokens).\n",
        "- **Input:**  \n",
        "  - `queries_t` = Fourier embedding of **target coordinates** (where we must predict).  \n",
        "  - `z_c` = φ at **context coordinates** from the **online** encoder (context tokens).\n",
        "- **Computation:**  \n",
        "  `q = to_q(queries_t)` (one query per target coord)  \n",
        "  `k, v = to_kv(z_c)` (keys/values from context tokens)  \n",
        "  `z_pred = to_out(softmax(q k^T / √d) v)`  \n",
        "  So each target position gets a prediction by attending over the **context** tokens only.\n",
        "- **Role:** From **partial** information (context φ and target coord embeddings), predict **what the target encoder would output** at those target coords. No access to target pixels or to the full-image residual; only to online context tokens and positions. So the predictor implements “predict φ at target positions from context.”\n",
        "\n",
        "---\n",
        "\n",
        "### End-to-end\n",
        "\n",
        "1. **Online:** Encoder sees [context tokens | target-position placeholders] → residual. Decoder(residual, context coords) → φ at context → `z_c`. Decoder is **not** queried at target coords on the online side for the JEPA loss.\n",
        "2. **Target (EMA):** Encoder sees full image → residual_ema. Decoder(residual_ema, **target** coords) → φ at target → `z_t_target` (no grad).\n",
        "3. **Predictor:** `z_pred = predictor(queries_t, z_c)`.\n",
        "4. **Loss:** smooth_l1 (or cosine/L2) between `z_pred` and `z_t_target` (e.g. after LayerNorm). So we train: **from context, predict the full-view field output at target coordinates** in representation space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Representation quality monitoring (every epoch)\n",
        "\n",
        "We compute **collapse metrics** (per-dimension std of mean-pooled φ) and **k-NN accuracy** on a fixed subset to track whether representations stay useful and non-collapsed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# Fixed subsets for reproducible monitoring (same samples every epoch)\n",
        "N_MONITOR_TRAIN = 1000\n",
        "N_MONITOR_TEST = 500\n",
        "MONITOR_GRID = 16\n",
        "K_NN = 5\n",
        "\n",
        "monitor_train_indices = list(range(min(N_MONITOR_TRAIN, len(train_ds))))\n",
        "monitor_test_indices = list(range(min(N_MONITOR_TEST, len(test_ds))))\n",
        "monitor_train_subset = Subset(train_ds, monitor_train_indices)\n",
        "monitor_test_subset = Subset(test_ds, monitor_test_indices)\n",
        "monitor_train_loader = DataLoader(monitor_train_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "monitor_test_loader = DataLoader(monitor_test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "def compute_representation_metrics(model, semantic_head, fourier_encoder, coords_32, device,\n",
        "                                  monitor_train_loader, monitor_test_loader, grid_size=16, k=5):\n",
        "    \"\"\"\n",
        "    Compute (1) per-dimension std of mean-pooled φ (collapse: low = bad), (2) k-NN accuracy on labels.\n",
        "    Uses full-image forward, mean-pool φ over grid_size x grid_size.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    semantic_head.eval()\n",
        "    coords_monitor = create_coordinate_grid(grid_size, grid_size, device)\n",
        "    feats_train, labels_train = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in monitor_train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries = fourier_encoder(coords_monitor.unsqueeze(0).expand(B, -1, -1))\n",
        "            phi = get_semantic_tokens(get_phi_raw(model, queries, residual), semantic_head)\n",
        "            phi_pooled = phi.mean(dim=1)\n",
        "            feats_train.append(phi_pooled.cpu())\n",
        "            labels_train.append(labels)\n",
        "    feats_train = torch.cat(feats_train, dim=0)\n",
        "    labels_train = torch.cat(labels_train, dim=0)\n",
        "    feats_test, labels_test = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in monitor_test_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries = fourier_encoder(coords_monitor.unsqueeze(0).expand(B, -1, -1))\n",
        "            phi = get_semantic_tokens(get_phi_raw(model, queries, residual), semantic_head)\n",
        "            phi_pooled = phi.mean(dim=1)\n",
        "            feats_test.append(phi_pooled.cpu())\n",
        "            labels_test.append(labels)\n",
        "    feats_test = torch.cat(feats_test, dim=0)\n",
        "    labels_test = torch.cat(labels_test, dim=0)\n",
        "\n",
        "    all_feats = torch.cat([feats_train, feats_test], dim=0)\n",
        "    std_per_dim = all_feats.std(dim=0)\n",
        "    phi_std_mean = std_per_dim.mean().item()\n",
        "    phi_std_min = std_per_dim.min().item()\n",
        "\n",
        "    # k-NN: L2 distance, majority vote\n",
        "    dist = torch.cdist(feats_test, feats_train)\n",
        "    _, idx = dist.topk(k, dim=1, largest=False)\n",
        "    neighbor_labels = labels_train[idx]\n",
        "    votes = torch.mode(neighbor_labels, dim=1).values\n",
        "    knn_acc = (votes == labels_test).float().mean().item()\n",
        "    return {\"phi_std_mean\": phi_std_mean, \"phi_std_min\": phi_std_min, \"knn_acc\": knn_acc}\n",
        "\n",
        "# Optional: run once before training to see initial metrics\n",
        "# metrics_0 = compute_representation_metrics(model, semantic_head, fourier_encoder, coords_32, DEVICE,\n",
        "#     monitor_train_loader, monitor_test_loader, grid_size=MONITOR_GRID, k=K_NN)\n",
        "# print(\"Initial repr metrics:\", metrics_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop — JEPA + RGB aux + VICReg + EMA update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = list(model.parameters()) + list(fourier_encoder.parameters()) + list(semantic_head.parameters()) + list(predictor.parameters()) + [mask_rgb]\n",
        "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "best_val_loss = float(\"inf\")\n",
        "CKPT_BEST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_best.pt\")\n",
        "CKPT_LAST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_last.pt\")\n",
        "repr_history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    fourier_encoder.train()\n",
        "    semantic_head.train()\n",
        "    predictor.train()\n",
        "    model_ema.eval()\n",
        "    fourier_ema.eval()\n",
        "    semantic_head_ema.eval()\n",
        "    total_loss = 0.0\n",
        "    total_jepa = 0.0\n",
        "    total_rgb = 0.0\n",
        "    total_vic = 0.0\n",
        "    if VERBOSE:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} (batches: {len(train_loader)})\")\n",
        "    for batch_ix, (imgs, _) in enumerate(train_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B = imgs.size(0)\n",
        "\n",
        "        idx_c, idx_t = sample_context_target_indices(\n",
        "            coords_32, N_CONTEXT, N_TARGET, DEVICE, block_mask=USE_BLOCK_MASK, mask_style=MASK_STYLE,\n",
        "            image_size=IMAGE_SIZE, patch_size=PATCH_SIZE, enc_mask_scale=ENC_MASK_SCALE,\n",
        "            pred_mask_scale=PRED_MASK_SCALE, aspect_ratio=ASPECT_RATIO, n_enc_masks=N_ENC_MASKS,\n",
        "            n_pred_masks=N_PRED_MASKS, allow_overlap=ALLOW_OVERLAP, min_keep=MIN_KEEP)\n",
        "        coords_c = coords_32[idx_c]\n",
        "        coords_t = coords_32[idx_t]\n",
        "\n",
        "        if VERBOSE and batch_ix == 0:\n",
        "            print(f\"  [epoch {epoch+1}] batch 0: N_context={len(idx_c)}, N_target={len(idx_t)} (mask_style={MASK_STYLE})\")\n",
        "\n",
        "        encoder_input = prepare_jepa_encoder_input(imgs, coords_32, fourier_encoder, idx_c, idx_t, DEVICE, mask_rgb)\n",
        "        residual = get_residual(model, encoder_input)\n",
        "\n",
        "        queries_c = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "        queries_t = fourier_encoder(coords_t.unsqueeze(0).expand(B, -1, -1))\n",
        "\n",
        "        z_c = get_semantic_tokens(get_phi_raw(model, queries_c, residual), semantic_head)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if TARGET_FROM_FULL_CONTEXT:\n",
        "                full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_ema)\n",
        "                residual_ema = get_residual(model_ema, full_input)\n",
        "            else:\n",
        "                residual_ema = get_residual(model_ema, encoder_input)\n",
        "            phi_t_raw = get_phi_raw(model_ema, queries_t, residual_ema)\n",
        "            z_t_target = get_semantic_tokens(phi_t_raw, semantic_head_ema)\n",
        "            if JEPA_LOSS_TYPE == \"smooth_l1\" or USE_LAYERNORM_JEPA:\n",
        "                z_t_target_raw = semantic_head_ema(phi_t_raw)\n",
        "\n",
        "        z_pred = predictor(queries_t, z_c)\n",
        "        if not USE_JEPA_LOSS:\n",
        "            loss_jepa = torch.tensor(0.0, device=DEVICE)\n",
        "        elif USE_LAYERNORM_JEPA:\n",
        "            z_pred_ln = F.layer_norm(z_pred, (z_pred.size(-1),))\n",
        "            z_t_ln = F.layer_norm(z_t_target_raw, (z_t_target_raw.size(-1),))\n",
        "            loss_jepa = F.smooth_l1_loss(z_pred_ln, z_t_ln).mean()\n",
        "        elif JEPA_LOSS_TYPE == \"cosine\":\n",
        "            loss_jepa = jepa_cosine_loss(z_pred, z_t_target)\n",
        "        elif JEPA_LOSS_TYPE == \"l2\":\n",
        "            loss_jepa = jepa_l2_loss(z_pred, z_t_target)\n",
        "        else:\n",
        "            loss_jepa = jepa_smooth_l1_loss(z_pred, z_t_target_raw)\n",
        "        loss = loss_jepa\n",
        "        loss_rgb = torch.tensor(0.0, device=DEVICE)\n",
        "        loss_vic = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        if USE_VICREG:\n",
        "            loss_vic = vicreg_loss(z_c, VICREG_SIM_WEIGHT, VICREG_VAR_WEIGHT, VICREG_COV_WEIGHT)\n",
        "            loss = loss + LAMBDA_VICREG * loss_vic\n",
        "            total_vic += loss_vic.item()\n",
        "\n",
        "        if USE_RGB_AUX:\n",
        "            if RGB_QUERY_RES >= IMAGE_SIZE:\n",
        "                queries_rgb = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "                rgb = get_rgb(model, queries_rgb, residual)\n",
        "                target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            else:\n",
        "                coords_rgb = create_coordinate_grid(RGB_QUERY_RES, RGB_QUERY_RES, DEVICE)\n",
        "                queries_rgb = fourier_encoder(coords_rgb.unsqueeze(0).expand(B, -1, -1))\n",
        "                rgb = get_rgb(model, queries_rgb, residual)\n",
        "                target_pixels = sample_gt_at_coords(imgs, coords_rgb.unsqueeze(0).expand(B, -1, -1))\n",
        "            loss_rgb = F.mse_loss(rgb, target_pixels) * LAMBDA_RGB\n",
        "            loss = loss + loss_rgb\n",
        "            total_rgb += loss_rgb.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_jepa += loss_jepa.item()\n",
        "\n",
        "        if VERBOSE and LOG_EVERY > 0 and (batch_ix + 1) % LOG_EVERY == 0:\n",
        "            rgb_val = total_rgb / (batch_ix + 1) if USE_RGB_AUX else 0.0\n",
        "            vic_val = total_vic / (batch_ix + 1) if USE_VICREG else 0.0\n",
        "            print(f\"  [epoch {epoch+1}] batch {batch_ix+1}/{len(train_loader)}: loss={loss.item():.4f} jepa={loss_jepa.item():.4f}\" +\n",
        "                  (f\" rgb={loss_rgb.item():.4f}\" if USE_RGB_AUX else \"\") +\n",
        "                  (f\" vic={loss_vic.item():.4f}\" if USE_VICREG else \"\") +\n",
        "                  f\" (avg so far: loss={total_loss/(batch_ix+1):.4f})\")\n",
        "\n",
        "        if EMA_MOMENTUM_RAMP:\n",
        "            total_steps = EPOCHS * len(train_loader)\n",
        "            step = epoch * len(train_loader) + batch_ix\n",
        "            current_momentum = EMA_MOMENTUM_START + (EMA_MOMENTUM_END - EMA_MOMENTUM_START) * min(1.0, step / total_steps)\n",
        "        else:\n",
        "            current_momentum = EMA_MOMENTUM\n",
        "        copy_ema(model, model_ema, current_momentum)\n",
        "        copy_ema(fourier_encoder, fourier_ema, current_momentum)\n",
        "        copy_ema(semantic_head, semantic_head_ema, current_momentum)\n",
        "\n",
        "    avg = total_loss / len(train_loader)\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in test_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            rgb = get_rgb(model, queries_full, residual)\n",
        "            target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            val_loss += F.mse_loss(rgb, target_pixels).item()\n",
        "    val_loss /= len(test_loader)\n",
        "    repr_metrics = compute_representation_metrics(\n",
        "        model, semantic_head, fourier_encoder, coords_32, DEVICE,\n",
        "        monitor_train_loader, monitor_test_loader, grid_size=MONITOR_GRID, k=K_NN)\n",
        "    repr_history.append(repr_metrics)\n",
        "    if VERBOSE:\n",
        "        print(f\"  repr: phi_std_mean={repr_metrics['phi_std_mean']:.4f} phi_std_min={repr_metrics['phi_std_min']:.4f} knn_acc={repr_metrics['knn_acc']:.4f}\")\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "            \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "            \"predictor_state_dict\": predictor.state_dict(),\n",
        "            \"model_ema_state_dict\": model_ema.state_dict(),\n",
        "            \"fourier_ema_state_dict\": fourier_ema.state_dict(),\n",
        "            \"semantic_head_ema_state_dict\": semantic_head_ema.state_dict(),\n",
        "            \"best_val_loss\": best_val_loss,\n",
        "        }, CKPT_BEST)\n",
        "    torch.save({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "        \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "        \"predictor_state_dict\": predictor.state_dict(),\n",
        "        \"model_ema_state_dict\": model_ema.state_dict(),\n",
        "        \"fourier_ema_state_dict\": fourier_ema.state_dict(),\n",
        "        \"semantic_head_ema_state_dict\": semantic_head_ema.state_dict(),\n",
        "        \"best_val_loss\": best_val_loss,\n",
        "    }, CKPT_LAST)\n",
        "    vic_str = f\" vic: {total_vic/len(train_loader):.4f}\" if USE_VICREG else \"\"\n",
        "    rgb_str = f\" rgb: {total_rgb/len(train_loader):.4f}\" if USE_RGB_AUX else \"\"\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} jepa: {total_jepa/len(train_loader):.4f}{vic_str}{rgb_str} val_loss: {val_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot representation quality over epochs (run after training)\n",
        "if len(repr_history) > 0:\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
        "    epochs = np.arange(1, len(repr_history) + 1)\n",
        "    ax[0].plot(epochs, [m[\"phi_std_mean\"] for m in repr_history], \"o-\")\n",
        "    ax[0].set_title(\"φ std (mean over dims)\")\n",
        "    ax[0].set_xlabel(\"Epoch\")\n",
        "    ax[1].plot(epochs, [m[\"phi_std_min\"] for m in repr_history], \"o-\")\n",
        "    ax[1].set_title(\"φ std (min dim)\")\n",
        "    ax[1].set_xlabel(\"Epoch\")\n",
        "    ax[2].plot(epochs, [m[\"knn_acc\"] for m in repr_history], \"o-\")\n",
        "    ax[2].set_title(\"k-NN accuracy\")\n",
        "    ax[2].set_xlabel(\"Epoch\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resolution-agnostic query (same as semantic-token notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, res_h, res_w, device):\n",
        "    coords = create_coordinate_grid(res_h, res_w, device)\n",
        "    B = residual.size(0)\n",
        "    queries = fourier_encoder(coords.unsqueeze(0).expand(B, -1, -1))\n",
        "    phi_raw = get_phi_raw(model, queries, residual)\n",
        "    return get_semantic_tokens(phi_raw, semantic_head)\n",
        "\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "semantic_head.eval()\n",
        "imgs, _ = next(iter(test_loader))\n",
        "imgs = imgs[:4].to(DEVICE)\n",
        "full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "with torch.no_grad():\n",
        "    residual = get_residual(model, full_input)\n",
        "    phi_16 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 16, 16, DEVICE)\n",
        "    phi_32 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 32, 32, DEVICE)\n",
        "print(\"φ at 16×16:\", phi_16.shape, \"| 32×32:\", phi_32.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}