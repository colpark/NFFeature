{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JEPA (Joint-Embedding Predictive Architecture) — Neural Field on CIFAR-10\n",
        "\n",
        "Learn representations by **predicting target embeddings from context embeddings** under masking, **no contrastive negatives**. Continuity-aware field gives domain-continuous tokens φ(x); JEPA predicts φ at masked coordinates from visible ones.\n",
        "\n",
        "**Distinguishable components:**\n",
        "\n",
        "1. **Dual output** — f(x) → [RGB, φ(x)] (same as semantic-token notebook)\n",
        "2. **Context / target split** — mask coords into visible (context) and predicted (target)\n",
        "3. **EMA target encoder** — φ_target = stop_grad(φ_θ̄(X_t)); θ̄ updated by momentum\n",
        "4. **Cross-attention predictor** — ẑ_t = P(z_c, X_c, X_t); predicts target embeddings from context\n",
        "5. **JEPA loss** — cosine or L2 on norm(ẑ_t) vs norm(z_t); no negatives\n",
        "6. **Optional VICReg** — variance + covariance on φ_online to avoid collapse\n",
        "7. **Optional RGB auxiliary** — small λ reconstruction so semantics aren't dominated by pixels\n",
        "\n",
        "Checkpoints: **checkpoint_jepa_best.pt** / **_last.pt**\n",
        "\n",
        "**I-JEPA faithful**: With **TARGET_FROM_FULL_CONTEXT=True** (default), target = φ_EMA(**full image**) at target coords, matching `ijepa/src/train.py` (target_encoder sees full image; we use full grid). Context branch unchanged: online encoder sees context only; predictor predicts from context tokens. Loss in representation space; EMA update of target encoder.\n",
        "\n",
        "**Speed**: Use `num_workers=2` and **RGB_QUERY_RES=16** to avoid >5 min/epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(\"Device:\", DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "N_FULL = IMAGE_SIZE * IMAGE_SIZE\n",
        "\n",
        "# JEPA: context vs target coord split\n",
        "# Mask style: \"ijepa_block\" = I-JEPA-style block-level (patch grid, pred blocks + enc blocks in complement); \"random\" = random pixel split\n",
        "MASK_STYLE = \"ijepa_block\"\n",
        "N_CONTEXT = 512\n",
        "N_TARGET = 256\n",
        "USE_BLOCK_MASK = MASK_STYLE == \"ijepa_block\"  # legacy flag for viz\n",
        "\n",
        "# I-JEPA-style block masking (see ijepa/src/masks/multiblock.py)\n",
        "PATCH_SIZE = 4\n",
        "ENC_MASK_SCALE = (0.85, 1.0)\n",
        "PRED_MASK_SCALE = (0.15, 0.25)\n",
        "ASPECT_RATIO = (0.75, 1.5)\n",
        "N_ENC_MASKS = 1\n",
        "N_PRED_MASKS = 2\n",
        "ALLOW_OVERLAP = False\n",
        "MIN_KEEP = 4\n",
        "\n",
        "# Verbose training: log every N batches (0 = only epoch summary)\n",
        "VERBOSE = True\n",
        "LOG_EVERY = 20\n",
        "\n",
        "# I-JEPA faithful: target = EMA(full image) at target coords (True). Legacy: target = EMA(context) at target (False).\n",
        "TARGET_FROM_FULL_CONTEXT = True\n",
        "\n",
        "# EMA target (momentum for target encoder; same as ijepa)\n",
        "EMA_MOMENTUM = 0.996\n",
        "\n",
        "# Losses\n",
        "USE_JEPA_LOSS = True\n",
        "USE_RGB_AUX = True\n",
        "LAMBDA_RGB = 0.1\n",
        "RGB_QUERY_RES = 16\n",
        "# RGB aux at 16×16 (256 queries) instead of 32×32 (1024) cuts decoder cost ~4x; set 32 for full-res.\n",
        "USE_VICREG = True\n",
        "LAMBDA_VICREG = 0.1\n",
        "VICREG_SIM_WEIGHT = 25.0\n",
        "VICREG_VAR_WEIGHT = 25.0\n",
        "VICREG_COV_WEIGHT = 1.0\n",
        "\n",
        "PHI_DIM = 128\n",
        "PREDICTOR_DIM = 256\n",
        "PREDICTOR_HEADS = 4\n",
        "\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(\"N_FULL:\", N_FULL, \"| MASK_STYLE:\", MASK_STYLE, \"| TARGET_FROM_FULL_CONTEXT:\", TARGET_FROM_FULL_CONTEXT)\n",
        "if MASK_STYLE == \"ijepa_block\":\n",
        "    print(\"  I-JEPA blocks: patch_size=%d enc_scale=%s pred_scale=%s aspect=%s n_enc=%d n_pred=%d\" % (PATCH_SIZE, ENC_MASK_SCALE, PRED_MASK_SCALE, ASPECT_RATIO, N_ENC_MASKS, N_PRED_MASKS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Field output: RGB + φ (shared with semantic-token design)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    sampled = F.grid_sample(images, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\n",
        "    return sampled.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def get_residual(model, context):\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=context, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def decoder_forward(model, queries, residual):\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return x\n",
        "\n",
        "def get_rgb(model, queries, residual):\n",
        "    return model.to_logits(decoder_forward(model, queries, residual))\n",
        "\n",
        "def get_phi_raw(model, queries, residual):\n",
        "    return decoder_forward(model, queries, residual)\n",
        "\n",
        "def get_semantic_tokens(phi_raw, semantic_head):\n",
        "    return F.normalize(semantic_head(phi_raw), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Context/target split — I-JEPA-style block masking or random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _patch_to_pixel_indices(patch_indices_flat, h_patch, w_patch, patch_size, image_size, device):\n",
        "    \"\"\"Convert flat patch indices to flat pixel indices (C order: row, then col).\"\"\"\n",
        "    pixels_per_patch = patch_size * patch_size\n",
        "    pixel_indices = []\n",
        "    for p_flat in patch_indices_flat:\n",
        "        pi, pj = p_flat // w_patch, p_flat % w_patch\n",
        "        for di in range(patch_size):\n",
        "            for dj in range(patch_size):\n",
        "                i, j = pi * patch_size + di, pj * patch_size + dj\n",
        "                pixel_indices.append(i * image_size + j)\n",
        "    return torch.tensor(pixel_indices, device=device, dtype=torch.long)\n",
        "\n",
        "def sample_ijepa_block_indices(image_size, patch_size, enc_mask_scale, pred_mask_scale, aspect_ratio,\n",
        "                               n_enc_masks, n_pred_masks, allow_overlap, min_keep, device, generator=None):\n",
        "    \"\"\"\n",
        "    I-JEPA-style block masking (ijepa/src/masks/multiblock.py).\n",
        "    Returns (idx_c, idx_t) as flat pixel indices: context = encoder sees, target = predictor predicts.\n",
        "    \"\"\"\n",
        "    h_patch = image_size // patch_size\n",
        "    w_patch = image_size // patch_size\n",
        "    n_patches = h_patch * w_patch\n",
        "    if generator is None:\n",
        "        generator = torch.Generator(device=device)\n",
        "\n",
        "    def sample_block_size(scale_lo, scale_hi, aspect_lo, aspect_hi):\n",
        "        s = scale_lo + torch.rand(1, device=device, generator=generator).item() * (scale_hi - scale_lo)\n",
        "        ar = aspect_lo + torch.rand(1, device=device, generator=generator).item() * (aspect_hi - aspect_lo)\n",
        "        max_keep = int(n_patches * s)\n",
        "        h = max(1, min(h_patch - 1, int(round((max_keep * ar) ** 0.5))))\n",
        "        w = max(1, min(w_patch - 1, int(round((max_keep / ar) ** 0.5))))\n",
        "        return h, w\n",
        "\n",
        "    def sample_one_block(bh, bw, acceptable_region=None):\n",
        "        \"\"\"acceptable_region: (H, W) bool, True = can place block. None = anywhere.\"\"\"\n",
        "        for _ in range(30):\n",
        "            top = torch.randint(0, h_patch - bh + 1, (1,), device=device, generator=generator).item()\n",
        "            left = torch.randint(0, w_patch - bw + 1, (1,), device=device, generator=generator).item()\n",
        "            block = torch.zeros(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "            block[top : top + bh, left : left + bw] = True\n",
        "            if acceptable_region is not None:\n",
        "                if not (block & acceptable_region == block).all():\n",
        "                    continue\n",
        "            idx = torch.nonzero(block.flatten(), as_tuple=False).squeeze(-1)\n",
        "            if len(idx) >= min_keep:\n",
        "                complement = torch.ones(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "                complement[top : top + bh, left : left + bw] = False\n",
        "                return idx, complement\n",
        "        return None, None\n",
        "\n",
        "    # 1) Target (pred) blocks\n",
        "    ph, pw = sample_block_size(pred_mask_scale[0], pred_mask_scale[1], aspect_ratio[0], aspect_ratio[1])\n",
        "    all_pred_patches = set()\n",
        "    pred_complements = []\n",
        "    for _ in range(n_pred_masks):\n",
        "        idx, comp = sample_one_block(ph, pw, None)\n",
        "        if idx is None:\n",
        "            continue\n",
        "        pred_complements.append(comp)\n",
        "        for i in idx.cpu().tolist():\n",
        "            all_pred_patches.add(i)\n",
        "    pred_patches_flat = list(all_pred_patches)\n",
        "    if len(pred_patches_flat) < min_keep:\n",
        "        pred_patches_flat = list(range(n_patches))[: min_keep + 1]\n",
        "    idx_t = _patch_to_pixel_indices(pred_patches_flat, h_patch, w_patch, patch_size, image_size, device)\n",
        "\n",
        "    # 2) Context (enc) blocks: only in complement of target\n",
        "    acceptable = None if allow_overlap else torch.ones(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "    if not allow_overlap and pred_complements:\n",
        "        for c in pred_complements:\n",
        "            acceptable = acceptable & c\n",
        "    eh, ew = sample_block_size(enc_mask_scale[0], enc_mask_scale[1], 1.0, 1.0)\n",
        "    all_enc_patches = set()\n",
        "    for _ in range(n_enc_masks):\n",
        "        idx, _ = sample_one_block(eh, ew, acceptable)\n",
        "        if idx is not None:\n",
        "            for i in idx.cpu().tolist():\n",
        "                all_enc_patches.add(i)\n",
        "    enc_patches_flat = list(all_enc_patches)\n",
        "    if len(enc_patches_flat) < min_keep:\n",
        "        enc_patches_flat = [k for k in range(n_patches) if k not in all_pred_patches][: min_keep + 1]\n",
        "    if not enc_patches_flat:\n",
        "        enc_patches_flat = [k for k in range(n_patches) if k not in all_pred_patches]\n",
        "    idx_c = _patch_to_pixel_indices(enc_patches_flat, h_patch, w_patch, patch_size, image_size, device)\n",
        "\n",
        "    return idx_c, idx_t\n",
        "\n",
        "def sample_context_target_indices(coords_full, n_context, n_target, device, block_mask=False,\n",
        "                                  mask_style=\"random\", image_size=32, patch_size=4,\n",
        "                                  enc_mask_scale=(0.85, 1.0), pred_mask_scale=(0.15, 0.25),\n",
        "                                  aspect_ratio=(0.75, 1.5), n_enc_masks=1, n_pred_masks=2,\n",
        "                                  allow_overlap=False, min_keep=4):\n",
        "    \"\"\"Return idx_c, idx_t disjoint. mask_style='ijepa_block' uses I-JEPA block masking.\"\"\"\n",
        "    n_total = coords_full.size(0)\n",
        "    if mask_style == \"ijepa_block\":\n",
        "        return sample_ijepa_block_indices(\n",
        "            image_size, patch_size, enc_mask_scale, pred_mask_scale, aspect_ratio,\n",
        "            n_enc_masks, n_pred_masks, allow_overlap, min_keep, device)\n",
        "    if block_mask:\n",
        "        h, w = int(math.sqrt(n_total)), int(math.sqrt(n_total))\n",
        "        nh, nw = max(1, h // 4), max(1, w // 4)\n",
        "        top = random.randint(0, h - nh)\n",
        "        left = random.randint(0, w - nw)\n",
        "        target_flat = []\n",
        "        for i in range(top, top + nh):\n",
        "            for j in range(left, left + nw):\n",
        "                target_flat.append(i * w + j)\n",
        "        idx_t = torch.tensor(target_flat, device=device, dtype=torch.long)\n",
        "        idx_c = torch.tensor([k for k in range(n_total) if k not in set(target_flat)], device=device)\n",
        "        if idx_c.size(0) > n_context:\n",
        "            idx_c = idx_c[torch.randperm(idx_c.size(0), device=device)[:n_context]]\n",
        "    else:\n",
        "        perm = torch.randperm(n_total, device=device)\n",
        "        idx_c = perm[:n_context]\n",
        "        idx_t = perm[n_context : n_context + n_target]\n",
        "    return idx_c, idx_t\n",
        "\n",
        "def prepare_context_input(images, coords_full, fourier_encoder, idx_c, device):\n",
        "    \"\"\"Build (B, len(idx_c), INPUT_DIM) from pixels at context coords only.\"\"\"\n",
        "    B = images.size(0)\n",
        "    coords_c = coords_full[idx_c]\n",
        "    pixels = sample_gt_at_coords(images, coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels, pos], dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Cross-attention predictor — P(z_c, X_c, X_t) → ẑ_t\n",
        "\n",
        "Query from target coords; key/value from context tokens. Field-native."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class JEPAPredictor(nn.Module):\n",
        "    \"\"\"Predict target embeddings from context: ẑ_t = Attn(q=embed(X_t), k=z_c, v=z_c).\"\"\"\n",
        "\n",
        "    def __init__(self, coord_embed_dim, phi_dim, pred_dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.to_q = nn.Linear(coord_embed_dim, pred_dim)\n",
        "        self.to_kv = nn.Linear(phi_dim, pred_dim * 2)\n",
        "        self.to_out = nn.Linear(pred_dim, phi_dim)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = pred_dim // num_heads\n",
        "\n",
        "    def forward(self, coords_t_embed, z_c):\n",
        "        B, N_t, _ = coords_t_embed.shape\n",
        "        _, N_c, _ = z_c.shape\n",
        "        q = self.to_q(coords_t_embed).view(B, N_t, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        kv = self.to_kv(z_c).view(B, N_c, 2, self.num_heads, self.d_head)\n",
        "        k, v = kv[:, :, 0].transpose(1, 2), kv[:, :, 1].transpose(1, 2)\n",
        "        scale = self.d_head ** -0.5\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(B, N_t, -1)\n",
        "        return self.to_out(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) JEPA loss (no negatives) + optional VICReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def jepa_cosine_loss(z_pred, z_target):\n",
        "    \"\"\"1 - cos(norm(z_pred), z_target); z_target already normalized.\"\"\"\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    return (1 - (z_pred * z_target).sum(dim=-1)).mean()\n",
        "\n",
        "def vicreg_loss(z, sim_weight=25.0, var_weight=25.0, cov_weight=1.0):\n",
        "    \"\"\"Variance + covariance regularization to avoid collapse (no negatives).\"\"\"\n",
        "    B, N, D = z.shape\n",
        "    z = z.reshape(B * N, D)\n",
        "    std = z.std(dim=0) + 1e-4\n",
        "    var_loss = torch.mean(F.relu(1 - std))\n",
        "    z_centered = z - z.mean(dim=0)\n",
        "    cov = (z_centered.T @ z_centered) / (z.size(0) - 1)\n",
        "    cov_loss = (cov.pow(2).sum() - cov.diag().pow(2).sum()) / D\n",
        "    return var_weight * var_loss + cov_weight * cov_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Online encoder + EMA target + predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def copy_ema(source, target, momentum=0.996):\n",
        "    for p_s, p_t in zip(source.parameters(), target.parameters()):\n",
        "        p_t.data.mul_(momentum).add_(p_s.data, alpha=1 - momentum)\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "semantic_head = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "\n",
        "model_ema = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "fourier_ema = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "semantic_head_ema = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "model_ema.load_state_dict(model.state_dict())\n",
        "fourier_ema.load_state_dict(fourier_encoder.state_dict())\n",
        "semantic_head_ema.load_state_dict(semantic_head.state_dict())\n",
        "\n",
        "predictor = JEPAPredictor(POS_EMBED_DIM, PHI_DIM, PREDICTOR_DIM, num_heads=PREDICTOR_HEADS).to(DEVICE)\n",
        "print(\"Online + EMA target + predictor\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize inputs: full image, context encoder, target encoder, and masks\n",
        "\n",
        "Below we take one batch and show **what each branch sees** so the data flow is transparent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# One batch, one fixed mask for clarity\n",
        "imgs_viz, _ = next(iter(train_loader))\n",
        "imgs_viz = imgs_viz.to(DEVICE)\n",
        "B_viz = min(4, imgs_viz.size(0))\n",
        "imgs_viz = imgs_viz[:B_viz]\n",
        "H_viz, W_viz = IMAGE_SIZE, IMAGE_SIZE\n",
        "N_full = H_viz * W_viz\n",
        "\n",
        "idx_c, idx_t = sample_context_target_indices(\n",
        "    coords_32, N_CONTEXT, N_TARGET, DEVICE, block_mask=USE_BLOCK_MASK, mask_style=MASK_STYLE,\n",
        "    image_size=IMAGE_SIZE, patch_size=PATCH_SIZE, enc_mask_scale=ENC_MASK_SCALE,\n",
        "    pred_mask_scale=PRED_MASK_SCALE, aspect_ratio=ASPECT_RATIO, n_enc_masks=N_ENC_MASKS,\n",
        "    n_pred_masks=N_PRED_MASKS, allow_overlap=ALLOW_OVERLAP, min_keep=MIN_KEEP)\n",
        "# Build mask images: green = context only, red = target only\n",
        "mask_c_2d = torch.zeros(N_full, device=DEVICE, dtype=torch.bool)\n",
        "mask_c_2d[idx_c] = True\n",
        "mask_t_2d = torch.zeros(N_full, device=DEVICE, dtype=torch.bool)\n",
        "mask_t_2d[idx_t] = True\n",
        "mask_c_2d = mask_c_2d.view(H_viz, W_viz).cpu().numpy()\n",
        "mask_t_2d = mask_t_2d.view(H_viz, W_viz).cpu().numpy()\n",
        "\n",
        "# Full image: what the target encoder (EMA) sees when TARGET_FROM_FULL_CONTEXT=True\n",
        "full_imgs = imgs_viz  # (B, 3, H, W)\n",
        "\n",
        "# Context-only \"image\": pixels at context coords, gray (0.5) at target coords (what online encoder sees)\n",
        "pixels_flat = rearrange(imgs_viz, \"b c h w -> b (h w) c\")\n",
        "context_only = torch.ones(B_viz, N_full, 3, device=DEVICE) * 0.5  # gray\n",
        "context_only[:, idx_c] = pixels_flat[:, idx_c]\n",
        "context_only = context_only.view(B_viz, H_viz, W_viz, 3).permute(0, 3, 1, 2)\n",
        "\n",
        "# Target-positions \"image\": pixels only at target coords, gray elsewhere (where we predict φ)\n",
        "target_only = torch.ones(B_viz, N_full, 3, device=DEVICE) * 0.5\n",
        "target_only[:, idx_t] = pixels_flat[:, idx_t]\n",
        "target_only = target_only.view(B_viz, H_viz, W_viz, 3).permute(0, 3, 1, 2)\n",
        "\n",
        "# Mask overlay: green = context, red = target (same mask for all samples in batch)\n",
        "overlay = torch.zeros(B_viz, 3, H_viz, W_viz, device=DEVICE)\n",
        "m_t = torch.from_numpy(mask_t_2d).float().to(DEVICE).unsqueeze(0).expand(B_viz, -1, -1)\n",
        "m_c = torch.from_numpy(mask_c_2d).float().to(DEVICE).unsqueeze(0).expand(B_viz, -1, -1)\n",
        "overlay[:, 0] = m_t  # R where target\n",
        "overlay[:, 1] = m_c  # G where context\n",
        "\n",
        "def to_display(img_bchw):\n",
        "    \"\"\"(B,C,H,W) in [-1,1] -> (H,W,C) in [0,1] for one image.\"\"\"\n",
        "    x = img_bchw[0].cpu().permute(1, 2, 0).numpy()\n",
        "    return np.clip(x * 0.5 + 0.5, 0, 1)\n",
        "\n",
        "fig, axes = plt.subplots(5, B_viz, figsize=(3 * B_viz, 14))\n",
        "if B_viz == 1:\n",
        "    axes = axes[:, np.newaxis]\n",
        "titles_row = [\n",
        "    \"Full image (input to target encoder / EMA when TARGET_FROM_FULL_CONTEXT=True)\",\n",
        "    \"Context only (input to online context encoder)\",\n",
        "    \"Target positions (pixels at coords where we predict φ)\",\n",
        "    \"Mask: green = context, red = target\",\n",
        "    \"Context (green) & target (red) coords on full image\",\n",
        "]\n",
        "# Coords in [-1,1] -> display: (x+1)/2 * W for col, (1-y)/2 * H for row (y up)\n",
        "coords_c_np = coords_32[idx_c].cpu().numpy()\n",
        "coords_t_np = coords_32[idx_t].cpu().numpy()\n",
        "for b in range(B_viz):\n",
        "    axes[0, b].imshow(to_display(full_imgs[b : b + 1]))\n",
        "    axes[0, b].set_title(f\"Sample {b+1}\" if b == 0 else \"\")\n",
        "    axes[0, b].axis(\"off\")\n",
        "    axes[1, b].imshow(to_display(context_only[b : b + 1]))\n",
        "    axes[1, b].axis(\"off\")\n",
        "    axes[2, b].imshow(to_display(target_only[b : b + 1]))\n",
        "    axes[2, b].axis(\"off\")\n",
        "    axes[3, b].imshow(to_display(overlay[b : b + 1]))\n",
        "    axes[3, b].axis(\"off\")\n",
        "    # Scatter: coords (x,y) in [-1,1], imshow has (0,0) top-left, x=col, y=row\n",
        "    axes[4, b].imshow(to_display(full_imgs[b : b + 1]))\n",
        "    col_c = (coords_c_np[:, 0] + 1) / 2 * (W_viz - 1)\n",
        "    row_c = (1 - coords_c_np[:, 1]) / 2 * (H_viz - 1)\n",
        "    col_t = (coords_t_np[:, 0] + 1) / 2 * (W_viz - 1)\n",
        "    row_t = (1 - coords_t_np[:, 1]) / 2 * (H_viz - 1)\n",
        "    axes[4, b].scatter(col_c, row_c, c=\"lime\", s=8, alpha=0.8, label=\"context\")\n",
        "    axes[4, b].scatter(col_t, row_t, c=\"red\", s=8, alpha=0.8, label=\"target\")\n",
        "    axes[4, b].axis(\"off\")\n",
        "for r in range(5):\n",
        "    axes[r, 0].set_ylabel(titles_row[r], fontsize=10)\n",
        "plt.suptitle(\"JEPA inputs: what each branch sees (one batch)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print shapes and counts\n",
        "context_input_viz = prepare_context_input(imgs_viz, coords_32, fourier_encoder, idx_c, DEVICE)\n",
        "full_input_viz, _, _ = prepare_model_input(imgs_viz, coords_32, fourier_encoder)\n",
        "print(\"Shapes:\")\n",
        "print(\"  Full image (batch):\", tuple(full_imgs.shape))\n",
        "print(\"  full_input (for EMA/target encoder when TARGET_FROM_FULL_CONTEXT):\", tuple(full_input_viz.shape), \"→ [B, N_FULL, CHANNELS+POS]\")\n",
        "print(\"  context_input (for online encoder):\", tuple(context_input_viz.shape), \"→ [B, N_CONTEXT, CHANNELS+POS]\")\n",
        "print(\"  Context coords:\", len(idx_c), \"| Target coords:\", len(idx_t))\n",
        "print(\"  TARGET_FROM_FULL_CONTEXT:\", TARGET_FROM_FULL_CONTEXT)\n",
        "if TARGET_FROM_FULL_CONTEXT:\n",
        "    print(\"    → Target encoder (EMA) sees: full image. Target = φ_EMA(full image) at target coords.\")\n",
        "else:\n",
        "    print(\"    → Target encoder (EMA) sees: context only (same as context encoder input). Target = φ_EMA(context) at target coords.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Target of the network (loss):** The predictor outputs **z_pred** (predicted φ at target coords); the target is **z_t_target** = φ_EMA at those same coords (no grad). Loss = cosine(z_pred, z_t_target). So we do *not* supervise pixels at target positions—we supervise **representation space** at target coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop — JEPA + RGB aux + VICReg + EMA update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = list(model.parameters()) + list(fourier_encoder.parameters()) + list(semantic_head.parameters()) + list(predictor.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "best_val_loss = float(\"inf\")\n",
        "CKPT_BEST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_best.pt\")\n",
        "CKPT_LAST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_last.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    fourier_encoder.train()\n",
        "    semantic_head.train()\n",
        "    predictor.train()\n",
        "    model_ema.eval()\n",
        "    fourier_ema.eval()\n",
        "    semantic_head_ema.eval()\n",
        "    total_loss = 0.0\n",
        "    total_jepa = 0.0\n",
        "    total_rgb = 0.0\n",
        "    total_vic = 0.0\n",
        "    if VERBOSE:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} (batches: {len(train_loader)})\")\n",
        "    for batch_ix, (imgs, _) in enumerate(train_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B = imgs.size(0)\n",
        "\n",
        "        idx_c, idx_t = sample_context_target_indices(\n",
        "            coords_32, N_CONTEXT, N_TARGET, DEVICE, block_mask=USE_BLOCK_MASK, mask_style=MASK_STYLE,\n",
        "            image_size=IMAGE_SIZE, patch_size=PATCH_SIZE, enc_mask_scale=ENC_MASK_SCALE,\n",
        "            pred_mask_scale=PRED_MASK_SCALE, aspect_ratio=ASPECT_RATIO, n_enc_masks=N_ENC_MASKS,\n",
        "            n_pred_masks=N_PRED_MASKS, allow_overlap=ALLOW_OVERLAP, min_keep=MIN_KEEP)\n",
        "        coords_c = coords_32[idx_c]\n",
        "        coords_t = coords_32[idx_t]\n",
        "\n",
        "        if VERBOSE and batch_ix == 0:\n",
        "            print(f\"  [epoch {epoch+1}] batch 0: N_context={len(idx_c)}, N_target={len(idx_t)} (mask_style={MASK_STYLE})\")\n",
        "\n",
        "        context_input = prepare_context_input(imgs, coords_32, fourier_encoder, idx_c, DEVICE)\n",
        "        residual = get_residual(model, context_input)\n",
        "\n",
        "        queries_c = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "        queries_t = fourier_encoder(coords_t.unsqueeze(0).expand(B, -1, -1))\n",
        "\n",
        "        z_c = get_semantic_tokens(get_phi_raw(model, queries_c, residual), semantic_head)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if TARGET_FROM_FULL_CONTEXT:\n",
        "                full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_ema)\n",
        "                residual_ema = get_residual(model_ema, full_input)\n",
        "            else:\n",
        "                residual_ema = get_residual(model_ema, context_input)\n",
        "            z_t_target = get_semantic_tokens(get_phi_raw(model_ema, queries_t, residual_ema), semantic_head_ema)\n",
        "\n",
        "        z_pred = predictor(queries_t, z_c)\n",
        "        loss_jepa = jepa_cosine_loss(z_pred, z_t_target) if USE_JEPA_LOSS else torch.tensor(0.0, device=DEVICE)\n",
        "        loss = loss_jepa\n",
        "        loss_rgb = torch.tensor(0.0, device=DEVICE)\n",
        "        loss_vic = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        if USE_VICREG:\n",
        "            loss_vic = vicreg_loss(z_c, VICREG_SIM_WEIGHT, VICREG_VAR_WEIGHT, VICREG_COV_WEIGHT)\n",
        "            loss = loss + LAMBDA_VICREG * loss_vic\n",
        "            total_vic += loss_vic.item()\n",
        "\n",
        "        if USE_RGB_AUX:\n",
        "            if RGB_QUERY_RES >= IMAGE_SIZE:\n",
        "                queries_rgb = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "                rgb = get_rgb(model, queries_rgb, residual)\n",
        "                target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            else:\n",
        "                coords_rgb = create_coordinate_grid(RGB_QUERY_RES, RGB_QUERY_RES, DEVICE)\n",
        "                queries_rgb = fourier_encoder(coords_rgb.unsqueeze(0).expand(B, -1, -1))\n",
        "                rgb = get_rgb(model, queries_rgb, residual)\n",
        "                target_pixels = sample_gt_at_coords(imgs, coords_rgb.unsqueeze(0).expand(B, -1, -1))\n",
        "            loss_rgb = F.mse_loss(rgb, target_pixels) * LAMBDA_RGB\n",
        "            loss = loss + loss_rgb\n",
        "            total_rgb += loss_rgb.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_jepa += loss_jepa.item()\n",
        "\n",
        "        if VERBOSE and LOG_EVERY > 0 and (batch_ix + 1) % LOG_EVERY == 0:\n",
        "            rgb_val = total_rgb / (batch_ix + 1) if USE_RGB_AUX else 0.0\n",
        "            vic_val = total_vic / (batch_ix + 1) if USE_VICREG else 0.0\n",
        "            print(f\"  [epoch {epoch+1}] batch {batch_ix+1}/{len(train_loader)}: loss={loss.item():.4f} jepa={loss_jepa.item():.4f}\" +\n",
        "                  (f\" rgb={loss_rgb.item():.4f}\" if USE_RGB_AUX else \"\") +\n",
        "                  (f\" vic={loss_vic.item():.4f}\" if USE_VICREG else \"\") +\n",
        "                  f\" (avg so far: loss={total_loss/(batch_ix+1):.4f})\")\n",
        "\n",
        "        copy_ema(model, model_ema, EMA_MOMENTUM)\n",
        "        copy_ema(fourier_encoder, fourier_ema, EMA_MOMENTUM)\n",
        "        copy_ema(semantic_head, semantic_head_ema, EMA_MOMENTUM)\n",
        "\n",
        "    avg = total_loss / len(train_loader)\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in test_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            rgb = get_rgb(model, queries_full, residual)\n",
        "            target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            val_loss += F.mse_loss(rgb, target_pixels).item()\n",
        "    val_loss /= len(test_loader)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "            \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "            \"predictor_state_dict\": predictor.state_dict(),\n",
        "            \"model_ema_state_dict\": model_ema.state_dict(),\n",
        "            \"fourier_ema_state_dict\": fourier_ema.state_dict(),\n",
        "            \"semantic_head_ema_state_dict\": semantic_head_ema.state_dict(),\n",
        "            \"best_val_loss\": best_val_loss,\n",
        "        }, CKPT_BEST)\n",
        "    torch.save({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "        \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "        \"predictor_state_dict\": predictor.state_dict(),\n",
        "        \"model_ema_state_dict\": model_ema.state_dict(),\n",
        "        \"fourier_ema_state_dict\": fourier_ema.state_dict(),\n",
        "        \"semantic_head_ema_state_dict\": semantic_head_ema.state_dict(),\n",
        "        \"best_val_loss\": best_val_loss,\n",
        "    }, CKPT_LAST)\n",
        "    vic_str = f\" vic: {total_vic/len(train_loader):.4f}\" if USE_VICREG else \"\"\n",
        "    rgb_str = f\" rgb: {total_rgb/len(train_loader):.4f}\" if USE_RGB_AUX else \"\"\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} jepa: {total_jepa/len(train_loader):.4f}{vic_str}{rgb_str} val_loss: {val_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resolution-agnostic query (same as semantic-token notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, res_h, res_w, device):\n",
        "    coords = create_coordinate_grid(res_h, res_w, device)\n",
        "    B = residual.size(0)\n",
        "    queries = fourier_encoder(coords.unsqueeze(0).expand(B, -1, -1))\n",
        "    phi_raw = get_phi_raw(model, queries, residual)\n",
        "    return get_semantic_tokens(phi_raw, semantic_head)\n",
        "\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "semantic_head.eval()\n",
        "imgs, _ = next(iter(test_loader))\n",
        "imgs = imgs[:4].to(DEVICE)\n",
        "full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "with torch.no_grad():\n",
        "    residual = get_residual(model, full_input)\n",
        "    phi_16 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 16, 16, DEVICE)\n",
        "    phi_32 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 32, 32, DEVICE)\n",
        "print(\"φ at 16×16:\", phi_16.shape, \"| 32×32:\", phi_32.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}