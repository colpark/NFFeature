{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JEPA (Joint-Embedding Predictive Architecture) — Neural Field on CIFAR-10\n",
        "\n",
        "Learn representations by **predicting target embeddings from context embeddings** under masking, **no contrastive negatives**. Continuity-aware field gives domain-continuous tokens φ(x); JEPA predicts φ at masked coordinates from visible ones.\n",
        "\n",
        "**Distinguishable components:**\n",
        "\n",
        "1. **Dual output** — f(x) → [RGB, φ(x)] (same as semantic-token notebook)\n",
        "2. **Context / target split** — mask coords into visible (context) and predicted (target)\n",
        "3. **EMA target encoder** — φ_target = stop_grad(φ_θ̄(X_t)); θ̄ updated by momentum\n",
        "4. **Cross-attention predictor** — ẑ_t = P(z_c, X_c, X_t); predicts target embeddings from context\n",
        "5. **JEPA loss** — cosine or L2 on norm(ẑ_t) vs norm(z_t); no negatives\n",
        "6. **Optional VICReg** — variance + covariance on φ_online to avoid collapse\n",
        "7. **Optional RGB auxiliary** — small λ reconstruction so semantics aren't dominated by pixels\n",
        "\n",
        "Checkpoints: **checkpoint_jepa_best.pt** / **_last.pt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "N_FULL = IMAGE_SIZE * IMAGE_SIZE\n",
        "\n",
        "# JEPA: context vs target coord split\n",
        "N_CONTEXT = 512\n",
        "N_TARGET = 256\n",
        "USE_BLOCK_MASK = False\n",
        "\n",
        "# EMA target\n",
        "EMA_MOMENTUM = 0.996\n",
        "\n",
        "# Losses\n",
        "USE_JEPA_LOSS = True\n",
        "USE_RGB_AUX = True\n",
        "LAMBDA_RGB = 0.1\n",
        "USE_VICREG = True\n",
        "LAMBDA_VICREG = 0.1\n",
        "VICREG_SIM_WEIGHT = 25.0\n",
        "VICREG_VAR_WEIGHT = 25.0\n",
        "VICREG_COV_WEIGHT = 1.0\n",
        "\n",
        "PHI_DIM = 128\n",
        "PREDICTOR_DIM = 256\n",
        "PREDICTOR_HEADS = 4\n",
        "\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(\"N_FULL:\", N_FULL, \"| N_CONTEXT:\", N_CONTEXT, \"| N_TARGET:\", N_TARGET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Field output: RGB + φ (shared with semantic-token design)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    sampled = F.grid_sample(images, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\n",
        "    return sampled.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def get_residual(model, context):\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=context, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def decoder_forward(model, queries, residual):\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return x\n",
        "\n",
        "def get_rgb(model, queries, residual):\n",
        "    return model.to_logits(decoder_forward(model, queries, residual))\n",
        "\n",
        "def get_phi_raw(model, queries, residual):\n",
        "    return decoder_forward(model, queries, residual)\n",
        "\n",
        "def get_semantic_tokens(phi_raw, semantic_head):\n",
        "    return F.normalize(semantic_head(phi_raw), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Context/target split — random or block mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_context_target_indices(coords_full, n_context, n_target, device, block_mask=False):\n",
        "    \"\"\"Return idx_c, idx_t disjoint. block_mask: target = contiguous block (e.g. center).\"\"\"\n",
        "    n_total = coords_full.size(0)\n",
        "    if block_mask:\n",
        "        h, w = int(math.sqrt(n_total)), int(math.sqrt(n_total))\n",
        "        nh, nw = max(1, h // 4), max(1, w // 4)\n",
        "        top = random.randint(0, h - nh)\n",
        "        left = random.randint(0, w - nw)\n",
        "        target_flat = []\n",
        "        for i in range(top, top + nh):\n",
        "            for j in range(left, left + nw):\n",
        "                target_flat.append(i * w + j)\n",
        "        idx_t = torch.tensor(target_flat, device=device, dtype=torch.long)\n",
        "        idx_c = torch.tensor([k for k in range(n_total) if k not in set(target_flat)], device=device)\n",
        "        if idx_c.size(0) > n_context:\n",
        "            idx_c = idx_c[torch.randperm(idx_c.size(0), device=device)[:n_context]]\n",
        "    else:\n",
        "        perm = torch.randperm(n_total, device=device)\n",
        "        idx_c = perm[:n_context]\n",
        "        idx_t = perm[n_context : n_context + n_target]\n",
        "    return idx_c, idx_t\n",
        "\n",
        "def prepare_context_input(images, coords_full, fourier_encoder, idx_c, device):\n",
        "    \"\"\"Build (B, len(idx_c), INPUT_DIM) from pixels at context coords only.\"\"\"\n",
        "    B = images.size(0)\n",
        "    coords_c = coords_full[idx_c]\n",
        "    pixels = sample_gt_at_coords(images, coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels, pos], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Cross-attention predictor — P(z_c, X_c, X_t) → ẑ_t\n",
        "\n",
        "Query from target coords; key/value from context tokens. Field-native."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class JEPAPredictor(nn.Module):\n",
        "    \"\"\"Predict target embeddings from context: ẑ_t = Attn(q=embed(X_t), k=z_c, v=z_c).\"\"\"\n",
        "\n",
        "    def __init__(self, coord_embed_dim, phi_dim, pred_dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.to_q = nn.Linear(coord_embed_dim, pred_dim)\n",
        "        self.to_kv = nn.Linear(phi_dim, pred_dim * 2)\n",
        "        self.to_out = nn.Linear(pred_dim, phi_dim)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = pred_dim // num_heads\n",
        "\n",
        "    def forward(self, coords_t_embed, z_c):\n",
        "        B, N_t, _ = coords_t_embed.shape\n",
        "        _, N_c, _ = z_c.shape\n",
        "        q = self.to_q(coords_t_embed).view(B, N_t, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        kv = self.to_kv(z_c).view(B, N_c, 2, self.num_heads, self.d_head)\n",
        "        k, v = kv[:, :, 0].transpose(1, 2), kv[:, :, 1].transpose(1, 2)\n",
        "        scale = self.d_head ** -0.5\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(B, N_t, -1)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) JEPA loss (no negatives) + optional VICReg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jepa_cosine_loss(z_pred, z_target):\n",
        "    \"\"\"1 - cos(norm(z_pred), z_target); z_target already normalized.\"\"\"\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    return (1 - (z_pred * z_target).sum(dim=-1)).mean()\n",
        "\n",
        "def vicreg_loss(z, sim_weight=25.0, var_weight=25.0, cov_weight=1.0):\n",
        "    \"\"\"Variance + covariance regularization to avoid collapse (no negatives).\"\"\"\n",
        "    B, N, D = z.shape\n",
        "    z = z.reshape(B * N, D)\n",
        "    std = z.std(dim=0) + 1e-4\n",
        "    var_loss = torch.mean(F.relu(1 - std))\n",
        "    z_centered = z - z.mean(dim=0)\n",
        "    cov = (z_centered.T @ z_centered) / (z.size(0) - 1)\n",
        "    cov_loss = (cov.pow(2).sum() - cov.diag().pow(2).sum()) / D\n",
        "    return var_weight * var_loss + cov_weight * cov_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Online encoder + EMA target + predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def copy_ema(source, target, momentum=0.996):\n",
        "    for p_s, p_t in zip(source.parameters(), target.parameters()):\n",
        "        p_t.data.mul_(momentum).add_(p_s.data, alpha=1 - momentum)\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "semantic_head = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "\n",
        "model_ema = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "fourier_ema = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "semantic_head_ema = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "model_ema.load_state_dict(model.state_dict())\n",
        "fourier_ema.load_state_dict(fourier_encoder.state_dict())\n",
        "semantic_head_ema.load_state_dict(semantic_head.state_dict())\n",
        "\n",
        "predictor = JEPAPredictor(POS_EMBED_DIM, PHI_DIM, PREDICTOR_DIM, num_heads=PREDICTOR_HEADS).to(DEVICE)\n",
        "print(\"Online + EMA target + predictor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop — JEPA + RGB aux + VICReg + EMA update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = list(model.parameters()) + list(fourier_encoder.parameters()) + list(semantic_head.parameters()) + list(predictor.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "best_val_loss = float(\"inf\")\n",
        "CKPT_BEST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_best.pt\")\n",
        "CKPT_LAST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_last.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    fourier_encoder.train()\n",
        "    semantic_head.train()\n",
        "    predictor.train()\n",
        "    model_ema.eval()\n",
        "    fourier_ema.eval()\n",
        "    semantic_head_ema.eval()\n",
        "    total_loss = 0.0\n",
        "    total_jepa = 0.0\n",
        "    total_rgb = 0.0\n",
        "    total_vic = 0.0\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B = imgs.size(0)\n",
        "\n",
        "        idx_c, idx_t = sample_context_target_indices(coords_32, N_CONTEXT, N_TARGET, DEVICE, block_mask=USE_BLOCK_MASK)\n",
        "        coords_c = coords_32[idx_c]\n",
        "        coords_t = coords_32[idx_t]\n",
        "\n",
        "        context_input = prepare_context_input(imgs, coords_32, fourier_encoder, idx_c, DEVICE)\n",
        "        residual = get_residual(model, context_input)\n",
        "\n",
        "        queries_c = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "        queries_t = fourier_encoder(coords_t.unsqueeze(0).expand(B, -1, -1))\n",
        "\n",
        "        z_c = get_semantic_tokens(get_phi_raw(model, queries_c, residual), semantic_head)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            residual_ema = get_residual(model_ema, context_input)\n",
        "            z_t_target = get_semantic_tokens(get_phi_raw(model_ema, queries_t, residual_ema), semantic_head_ema)\n",
        "\n",
        "        z_pred = predictor(queries_t, z_c)\n",
        "        loss_jepa = jepa_cosine_loss(z_pred, z_t_target) if USE_JEPA_LOSS else torch.tensor(0.0, device=DEVICE)\n",
        "        loss = loss_jepa\n",
        "\n",
        "        if USE_VICREG:\n",
        "            loss_vic = vicreg_loss(z_c, VICREG_SIM_WEIGHT, VICREG_VAR_WEIGHT, VICREG_COV_WEIGHT)\n",
        "            loss = loss + LAMBDA_VICREG * loss_vic\n",
        "            total_vic += loss_vic.item()\n",
        "\n",
        "        if USE_RGB_AUX:\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            rgb = get_rgb(model, queries_full, residual)\n",
        "            target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            loss_rgb = F.mse_loss(rgb, target_pixels) * LAMBDA_RGB\n",
        "            loss = loss + loss_rgb\n",
        "            total_rgb += loss_rgb.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_jepa += loss_jepa.item()\n",
        "\n",
        "        copy_ema(model, model_ema, EMA_MOMENTUM)\n",
        "        copy_ema(fourier_encoder, fourier_ema, EMA_MOMENTUM)\n",
        "        copy_ema(semantic_head, semantic_head_ema, EMA_MOMENTUM)\n",
        "\n",
        "    avg = total_loss / len(train_loader)\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in test_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            rgb = get_rgb(model, queries_full, residual)\n",
        "            target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            val_loss += F.mse_loss(rgb, target_pixels).item()\n",
        "    val_loss /= len(test_loader)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "            \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "            \"predictor_state_dict\": predictor.state_dict(),\n",
        "            \"model_ema_state_dict\": model_ema.state_dict(),\n",
        "            \"fourier_ema_state_dict\": fourier_ema.state_dict(),\n",
        "            \"semantic_head_ema_state_dict\": semantic_head_ema.state_dict(),\n",
        "            \"best_val_loss\": best_val_loss,\n",
        "        }, CKPT_BEST)\n",
        "    torch.save({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "        \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "        \"predictor_state_dict\": predictor.state_dict(),\n",
        "        \"model_ema_state_dict\": model_ema.state_dict(),\n",
        "        \"fourier_ema_state_dict\": fourier_ema.state_dict(),\n",
        "        \"semantic_head_ema_state_dict\": semantic_head_ema.state_dict(),\n",
        "        \"best_val_loss\": best_val_loss,\n",
        "    }, CKPT_LAST)\n",
        "    vic_str = f\" vic: {total_vic/len(train_loader):.4f}\" if USE_VICREG else \"\"\n",
        "    rgb_str = f\" rgb: {total_rgb/len(train_loader):.4f}\" if USE_RGB_AUX else \"\"\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} jepa: {total_jepa/len(train_loader):.4f}{vic_str}{rgb_str} val_loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resolution-agnostic query (same as semantic-token notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, res_h, res_w, device):\n",
        "    coords = create_coordinate_grid(res_h, res_w, device)\n",
        "    B = residual.size(0)\n",
        "    queries = fourier_encoder(coords.unsqueeze(0).expand(B, -1, -1))\n",
        "    phi_raw = get_phi_raw(model, queries, residual)\n",
        "    return get_semantic_tokens(phi_raw, semantic_head)\n",
        "\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "semantic_head.eval()\n",
        "imgs, _ = next(iter(test_loader))\n",
        "imgs = imgs[:4].to(DEVICE)\n",
        "full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "with torch.no_grad():\n",
        "    residual = get_residual(model, full_input)\n",
        "    phi_16 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 16, 16, DEVICE)\n",
        "    phi_32 = query_semantic_tokens_at_resolution(model, semantic_head, fourier_encoder, residual, 32, 32, DEVICE)\n",
        "print(\"φ at 16×16:\", phi_16.shape, \"| 32×32:\", phi_32.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
