{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDSM Classification: Token-Decoded Spatial Map for CIFAR-10\n",
    "\n",
    "Uses the **pretrained** OmniField-style model (checkpoint from `AblationCIFAR10.ipynb`). We extract **TDSM** (one decoded \"component\" image per latent token), then train a **small CNN** on TDSM for classification. Includes visualizations of the learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from nf_feature_models import (\n",
    "    CascadedPerceiverIO,\n",
    "    GaussianFourierFeatures,\n",
    "    create_coordinate_grid,\n",
    "    prepare_model_input,\n",
    ")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "CKPT_PATH = os.path.join(CHECKPOINT_DIR, \"checkpoint_best.pt\")\n",
    "if not os.path.isfile(CKPT_PATH):\n",
    "    CKPT_PATH = os.path.join(CHECKPOINT_DIR, \"checkpoint_last.pt\")\n",
    "assert os.path.isfile(CKPT_PATH), f\"No checkpoint found in {CHECKPOINT_DIR}. Train AblationCIFAR10.ipynb first.\"\n",
    "\n",
    "# Config (must match training notebook)\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "FOURIER_MAPPING_SIZE = 96\n",
    "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
    "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
    "QUERIES_DIM = POS_EMBED_DIM\n",
    "LOGITS_DIM = CHANNELS\n",
    "NUM_LATENTS = 256\n",
    "\n",
    "fourier_encoder = GaussianFourierFeatures(in_features=2, mapping_size=FOURIER_MAPPING_SIZE, scale=15.0).to(DEVICE)\n",
    "model = CascadedPerceiverIO(\n",
    "    input_dim=INPUT_DIM,\n",
    "    queries_dim=QUERIES_DIM,\n",
    "    logits_dim=LOGITS_DIM,\n",
    "    latent_dims=(256, 384, 512),\n",
    "    num_latents=(256, 256, 256),\n",
    "    decoder_ff=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "fourier_encoder.load_state_dict(ckpt[\"fourier_encoder_state_dict\"], strict=False)\n",
    "model.eval()\n",
    "fourier_encoder.eval()\n",
    "\n",
    "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
    "print(f\"Loaded {CKPT_PATH}\")\n",
    "print(f\"Model and fourier_encoder on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual(model, data):\n",
    "    \"\"\"Run encoder + processor only; return latent field (B, num_latents, latent_dim).\"\"\"\n",
    "    residual = None\n",
    "    for block in model.encoder_blocks:\n",
    "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
    "    for sa_block in model.self_attn_blocks:\n",
    "        residual = sa_block[0](residual) + residual\n",
    "        residual = sa_block[1](residual) + residual\n",
    "    return residual\n",
    "\n",
    "\n",
    "def decoder_forward(model, queries, context):\n",
    "    \"\"\"Run decoder: queries (B,N,qd), context (B,1 or B,L,ld) -> (B,N,3).\"\"\"\n",
    "    x = model.decoder_cross_attn(queries, context=context)\n",
    "    x = x + queries\n",
    "    if model.decoder_ff is not None:\n",
    "        x = x + model.decoder_ff(x)\n",
    "    return model.to_logits(x)\n",
    "\n",
    "\n",
    "def get_tdsm(model, fourier_encoder, data, coords_32, device, num_tokens=256):\n",
    "    \"\"\"\n",
    "    Token-decoded spatial map: for each latent token, decode to 32x32x3; then take mean over RGB.\n",
    "    Returns TDSM of shape (B, num_tokens, 32, 32) for CNN (channels first).\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        residual = get_residual(model, data)\n",
    "    B = data.size(0)\n",
    "    queries_32 = fourier_encoder(repeat(coords_32, \"n d -> b n d\", b=B)).to(device)\n",
    "    component_images = []\n",
    "    for k in range(num_tokens):\n",
    "        ctx_k = residual[:, k : k + 1, :]\n",
    "        logits_k = decoder_forward(model, queries_32, ctx_k)\n",
    "        img_k = logits_k.reshape(B, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "        component_images.append(img_k)\n",
    "    component_images = torch.stack(component_images, dim=1)\n",
    "    tdsm = component_images.mean(dim=-1)\n",
    "    return tdsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDSMClassifier(nn.Module):\n",
    "    \"\"\"Small CNN on TDSM (B, num_tokens, 32, 32).\"\"\"\n",
    "\n",
    "    def __init__(self, num_tokens=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_tokens, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, tdsm):\n",
    "        return self.net(tdsm)\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "classifier = TDSMClassifier(num_tokens=NUM_LATENTS, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "    fourier_encoder.eval()\n",
    "    classifier.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        input_data, _, _ = prepare_model_input(images, coords_32, fourier_encoder)\n",
    "        tdsm = get_tdsm(model, fourier_encoder, input_data, coords_32, DEVICE)\n",
    "        logits = classifier(tdsm)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}  Train Loss: {total_loss/total:.4f}  Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        input_data, _, _ = prepare_model_input(images, coords_32, fourier_encoder)\n",
    "        tdsm = get_tdsm(model, fourier_encoder, input_data, coords_32, DEVICE)\n",
    "        logits = classifier(tdsm)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "test_acc = correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature visualizations\n",
    "\n",
    "1. **TDSM slices**: a few latent tokens' decoded maps (32Ã—32) for one sample.\n",
    "2. **Component images**: RGB \"component\" images for a few tokens (what each token contributes to the reconstruction).\n",
    "3. **TDSM channel stats**: mean and std across tokens to see which tokens are most active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "fourier_encoder.eval()\n",
    "classifier.eval()\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images[:8].to(DEVICE)\n",
    "labels = labels[:8]\n",
    "input_data, _, _ = prepare_model_input(images, coords_32, fourier_encoder)\n",
    "with torch.no_grad():\n",
    "    tdsm = get_tdsm(model, fourier_encoder, input_data, coords_32, DEVICE)\n",
    "\n",
    "sample_idx = 0\n",
    "tdsm_one = tdsm[sample_idx].cpu().numpy()\n",
    "num_show = 16\n",
    "step = max(1, NUM_LATENTS // num_show)\n",
    "indices = list(range(0, NUM_LATENTS, step))[:num_show]\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    k = indices[i]\n",
    "    im = ax.imshow(tdsm_one[k], cmap=\"viridis\")\n",
    "    ax.set_title(f\"Token {k}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"TDSM: Token-decoded spatial maps (one sample, 16 tokens)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    residual = get_residual(model, input_data)\n",
    "    queries_32 = fourier_encoder(repeat(coords_32, \"n d -> b n d\", b=images.size(0))).to(DEVICE)\n",
    "    token_indices = [0, 32, 64, 128, 192, 255]\n",
    "    comps = []\n",
    "    for k in token_indices:\n",
    "        ctx_k = residual[:, k : k + 1, :]\n",
    "        logits_k = decoder_forward(model, queries_32, ctx_k)\n",
    "        comps.append(logits_k.reshape(images.size(0), IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    comps = torch.stack(comps, dim=0)\n",
    "\n",
    "def to_display(t):\n",
    "    return (t.cpu() / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axs[0, 0].imshow(to_display(images[sample_idx]).permute(1, 2, 0).numpy())\n",
    "axs[0, 0].set_title(\"Input\")\n",
    "axs[0, 0].axis(\"off\")\n",
    "for i in range(3):\n",
    "    axs[0, i+1].imshow(to_display(comps[i, sample_idx]).numpy())\n",
    "    axs[0, i+1].set_title(f\"Token {token_indices[i]}\")\n",
    "    axs[0, i+1].axis(\"off\")\n",
    "for i in range(3):\n",
    "    axs[1, i].imshow(to_display(comps[i+3, sample_idx]).numpy())\n",
    "    axs[1, i].set_title(f\"Token {token_indices[i+3]}\")\n",
    "    axs[1, i].axis(\"off\")\n",
    "axs[1, 3].axis(\"off\")\n",
    "plt.suptitle(\"Input and token component images (same sample)\", fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_means = tdsm_one.mean(axis=(1, 2))\n",
    "token_stds = tdsm_one.std(axis=(1, 2))\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 4))\n",
    "ax1.bar(range(NUM_LATENTS), token_means, color=\"steelblue\", alpha=0.8)\n",
    "ax1.set_xlabel(\"Token index\")\n",
    "ax1.set_ylabel(\"Mean activation\")\n",
    "ax1.set_title(\"TDSM: Mean value per token (spatial mean)\")\n",
    "ax2.bar(range(NUM_LATENTS), token_stds, color=\"coral\", alpha=0.8)\n",
    "ax2.set_xlabel(\"Token index\")\n",
    "ax2.set_ylabel(\"Std\")\n",
    "ax2.set_title(\"TDSM: Spatial std per token\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    N_VAL = 500\n",
    "    all_tdsm = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            input_data, _, _ = prepare_model_input(images, coords_32, fourier_encoder)\n",
    "            tdsm = get_tdsm(model, fourier_encoder, input_data, coords_32, DEVICE)\n",
    "            feat = tdsm.mean(dim=(2, 3)).cpu().numpy()\n",
    "            all_tdsm.append(feat)\n",
    "            all_labels.append(labels.numpy())\n",
    "            if sum(len(x) for x in all_tdsm) >= N_VAL:\n",
    "                break\n",
    "    X = np.concatenate(all_tdsm, axis=0)[:N_VAL]\n",
    "    y = np.concatenate(all_labels, axis=0)[:N_VAL]\n",
    "    pca = PCA(n_components=50)\n",
    "    Xp = pca.fit_transform(X)\n",
    "    X_tsne = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(Xp)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=\"tab10\", s=10, alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Class\")\n",
    "    plt.title(\"t-SNE of TDSM pooled features (global mean over space)\")\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Install sklearn for t-SNE: pip install scikit-learn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
