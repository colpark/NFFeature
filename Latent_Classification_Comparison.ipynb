{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification comparison: five representations with a small transformer\n",
        "\n",
        "Compare CIFAR-10 classification using **one small transformer** on five inputs:\n",
        "1. **Original** – patched image pixels (same transformer, patch embedding).\n",
        "2. **Sparse (full ctx)** – last latent of sparse-context model with **full** context.\n",
        "3. **Full baseline** – last latent of full-context baseline.\n",
        "4. **Full + NCE** – last latent of full-context NCE model.\n",
        "5. **Sparse + NCE finetuned (full ctx)** – last latent of sparse-context NCE finetune with **full** context.\n",
        "\n",
        "All neural-field variants use the **last latent** \\((B, N, D)\\) (encoder + processor output). No sparsification; same data and same small transformer for comparison only. Epochs kept small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "NUM_CLASSES = 10\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "LATENT_DIM = 512\n",
        "NUM_LATENTS = 256\n",
        "\n",
        "def prepare_full_context(images, coords_full, fourier_encoder):\n",
        "    input_full, _, _ = prepare_model_input(images, coords_full, fourier_encoder)\n",
        "    return input_full\n",
        "\n",
        "def get_residual(model, data):\n",
        "    \"\"\"Encoder + processor -> (B, num_latents, latent_dim).\"\"\"\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sparse-context model (required)\n",
        "CKPT_SPARSE = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_best.pt\")\n",
        "if not os.path.isfile(CKPT_SPARSE):\n",
        "    CKPT_SPARSE = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_last.pt\")\n",
        "assert os.path.isfile(CKPT_SPARSE), \"Run SparseContext_CIFAR10.ipynb first.\"\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "sparse_model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "ckpt = torch.load(CKPT_SPARSE, map_location=DEVICE)\n",
        "sparse_model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
        "fourier_encoder.load_state_dict(ckpt[\"fourier_encoder_state_dict\"], strict=False)\n",
        "sparse_model.eval()\n",
        "fourier_encoder.eval()\n",
        "print(\"Loaded sparse:\", CKPT_SPARSE)\n",
        "\n",
        "# Optional: full-context baseline\n",
        "CKPT_BASELINE = os.path.join(CHECKPOINT_DIR, \"checkpoint_best.pt\")\n",
        "if not os.path.isfile(CKPT_BASELINE):\n",
        "    CKPT_BASELINE = os.path.join(CHECKPOINT_DIR, \"checkpoint_last.pt\")\n",
        "baseline_model = baseline_fourier = None\n",
        "if os.path.isfile(CKPT_BASELINE):\n",
        "    baseline_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    baseline_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_b = torch.load(CKPT_BASELINE, map_location=DEVICE)\n",
        "    baseline_model.load_state_dict(ckpt_b[\"model_state_dict\"], strict=False)\n",
        "    baseline_fourier.load_state_dict(ckpt_b[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    baseline_model.eval()\n",
        "    baseline_fourier.eval()\n",
        "    print(\"Loaded baseline:\", CKPT_BASELINE)\n",
        "\n",
        "# Optional: full-context NCE\n",
        "CKPT_NCE = os.path.join(CHECKPOINT_DIR, \"checkpoint_nce_best.pt\")\n",
        "if not os.path.isfile(CKPT_NCE):\n",
        "    CKPT_NCE = os.path.join(CHECKPOINT_DIR, \"softnce_best.pt\")\n",
        "nce_model = nce_fourier = None\n",
        "if os.path.isfile(CKPT_NCE):\n",
        "    nce_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    nce_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_n = torch.load(CKPT_NCE, map_location=DEVICE)\n",
        "    nce_model.load_state_dict(ckpt_n[\"model_state_dict\"], strict=False)\n",
        "    nce_fourier.load_state_dict(ckpt_n[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    nce_model.eval()\n",
        "    nce_fourier.eval()\n",
        "    print(\"Loaded NCE:\", CKPT_NCE)\n",
        "\n",
        "# Optional: sparse-context NCE finetuned\n",
        "CKPT_SFN = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_finetune_nce_best.pt\")\n",
        "if not os.path.isfile(CKPT_SFN):\n",
        "    CKPT_SFN = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_finetune_nce_last.pt\")\n",
        "sfn_model = sfn_fourier = None\n",
        "if os.path.isfile(CKPT_SFN):\n",
        "    sfn_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    sfn_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_sfn = torch.load(CKPT_SFN, map_location=DEVICE)\n",
        "    sfn_model.load_state_dict(ckpt_sfn[\"model_state_dict\"], strict=False)\n",
        "    sfn_fourier.load_state_dict(ckpt_sfn[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    sfn_model.eval()\n",
        "    sfn_fourier.eval()\n",
        "    print(\"Loaded sparse finetune NCE:\", CKPT_SFN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CIFAR-10, same transform as eval (full data, no sparsification)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Patchify images for \"original\" representation: (B,C,H,W) -> (B, n_patches, embed_dim)\n",
        "PATCH_SIZE = 4\n",
        "PATCH_DIM = PATCH_SIZE * PATCH_SIZE * CHANNELS\n",
        "N_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "def patchify(images):\n",
        "    \"\"\"(B, C, H, W) -> (B, n_patches, patch_dim).\"\"\"\n",
        "    B, C, H, W = images.shape\n",
        "    p = PATCH_SIZE\n",
        "    x = rearrange(images, \"b c (h ph) (w pw) -> b (h w) (ph pw c)\", ph=p, pw=p)\n",
        "    return x\n",
        "\n",
        "def get_representation(images, rep_name):\n",
        "    \"\"\"Return (B, seq_len, LATENT_DIM) for the small transformer. All use full context.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        if rep_name == \"original\":\n",
        "            patches = patchify(images).to(DEVICE)\n",
        "            return patches.float()\n",
        "        input_full = prepare_full_context(images, coords_32, fourier_encoder)\n",
        "        if rep_name == \"sparse\":\n",
        "            return get_residual(sparse_model, input_full)\n",
        "        if rep_name == \"baseline\" and baseline_model is not None:\n",
        "            return get_residual(baseline_model, input_full)\n",
        "        if rep_name == \"nce\" and nce_model is not None:\n",
        "            return get_residual(nce_model, input_full)\n",
        "        if rep_name == \"sparse_finetune_nce\" and sfn_model is not None:\n",
        "            return get_residual(sfn_model, input_full)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmallTransformerClassifier(nn.Module):\n",
        "    \"\"\"Small transformer on sequence (B, L, d_in); projects to d_model, then mean pool + linear to num_classes.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_model=256, num_heads=4, num_layers=2, mlp_ratio=2, num_classes=10, max_len=256):\n",
        "        super().__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_model = d_model\n",
        "        self.proj = nn.Linear(d_in, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, d_model) * 0.02)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_model * mlp_ratio,\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\",\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, _ = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x + self.pos_embed[:, :L, :]\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "def train_and_eval(rep_name, d_in, seq_len, epochs=5, lr=1e-3):\n",
        "    model = SmallTransformerClassifier(d_in=d_in, d_model=256, num_heads=4, num_layers=2, num_classes=NUM_CLASSES, max_len=max(seq_len, 256)).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            x = get_representation(images, rep_name)\n",
        "            if x is None:\n",
        "                continue\n",
        "            logits = model(x)\n",
        "            loss = ce(logits, labels)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                x = get_representation(images, rep_name)\n",
        "                if x is None:\n",
        "                    continue\n",
        "                logits = model(x)\n",
        "                correct += (logits.argmax(1) == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        acc = correct / total if total else 0\n",
        "        print(f\"  {rep_name} epoch {epoch+1}/{epochs} test acc: {acc:.4f}\")\n",
        "    return acc\n",
        "\n",
        "\n",
        "EPOCHS = 5\n",
        "print(\"Small transformer: d_model=256, 2 layers, 4 heads. Epochs:\", EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "print(\"1. Original (patched pixels)\")\n",
        "results[\"original\"] = train_and_eval(\"original\", d_in=PATCH_DIM, seq_len=N_PATCHES, epochs=EPOCHS)\n",
        "\n",
        "print(\"2. Sparse model, full context (last latent)\")\n",
        "results[\"sparse\"] = train_and_eval(\"sparse\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "\n",
        "if baseline_model is not None:\n",
        "    print(\"3. Full baseline (last latent)\")\n",
        "    results[\"baseline\"] = train_and_eval(\"baseline\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "else:\n",
        "    results[\"baseline\"] = None\n",
        "\n",
        "if nce_model is not None:\n",
        "    print(\"4. Full + NCE (last latent)\")\n",
        "    results[\"nce\"] = train_and_eval(\"nce\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "else:\n",
        "    results[\"nce\"] = None\n",
        "\n",
        "if sfn_model is not None:\n",
        "    print(\"5. Sparse + NCE finetuned, full context (last latent)\")\n",
        "    results[\"sparse_finetune_nce\"] = train_and_eval(\"sparse_finetune_nce\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "else:\n",
        "    results[\"sparse_finetune_nce\"] = None\n",
        "\n",
        "print(\"\\n--- Summary (test accuracy) ---\")\n",
        "for name, acc in results.items():\n",
        "    print(f\"  {name}: {acc:.4f}\" if acc is not None else f\"  {name}: (not loaded)\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "names = [k for k, v in results.items() if v is not None]\n",
        "accs = [results[k] for k in names]\n",
        "if names:\n",
        "    plt.bar(names, accs, color='steelblue')\n",
        "    plt.ylabel('Test accuracy')\n",
        "    plt.title('Classification comparison (small transformer, full data, few epochs)')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=15)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('latent_classification_comparison.png', dpi=100)\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
