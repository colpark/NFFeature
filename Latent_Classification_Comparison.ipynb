{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification comparison: five representations with a small transformer\n",
        "\n",
        "Compare CIFAR-10 classification using **one small transformer** on five inputs:\n",
        "1. **Original** – patched image pixels (same transformer, patch embedding).\n",
        "2. **Sparse (full ctx)** – last latent of sparse-context model with **full** context.\n",
        "3. **Full baseline** – last latent of full-context baseline.\n",
        "4. **Full + NCE** – last latent of full-context NCE model.\n",
        "5. **Sparse + NCE finetuned (full ctx)** – last latent of sparse-context NCE finetune with **full** context.\n",
        "\n",
        "All neural-field variants use the **last latent** \\((B, N, D)\\) (encoder + processor output). Same data and same small transformer; Epochs kept small.\n",
        "\n",
        "**Sparse-context-only mode** (`USE_SPARSE_CONTEXT = True` in code): every representation sees only **sparse observations** (no full image). NF models get `prepare_sparse_context` (a random subset of coords); \"original\" gets the same number of points as (x, y, r, g, b). So classification is from partial observations only — sparse-trained models may do better here.\n",
        "\n",
        "---\n",
        "\n",
        "**Why does t-SNE favor NCE/sparse while the transformer favors baseline?**\n",
        "\n",
        "- **t-SNE** reflects *neighborhood / metric structure*; NCE and sparse-context often improve that, so t-SNE looks better for them.\n",
        "\n",
        "- **Separability** (good linear/transformer accuracy) is often linked to **hierarchical concept learning / abstraction** — and **sparse context** and **masked-style** objectives are *meant* to encourage that (infer whole from parts, more semantic structure). So in principle sparse/MIM could do *better* on separability.\n",
        "\n",
        "Here the **baseline** can still win on the class probe because: (1) Full-context reconstruction pushes the latent to hold *all* pixel-level information, so it becomes very *information-dense* and easy for a simple head to separate, even if not more \"abstract.\" (2) CIFAR-10 class labels are one particular notion of \"concept\"; sparse/NCE may be learning abstraction that is more *view- or instance-level* (good for metric/t-SNE) and not aligned with those class boundaries. So we're not claiming \"baseline is more abstract\" — rather \"baseline is more class-informative in a dense, linearly usable way\" while sparse/NCE can excel on *metric* structure and possibly different abstraction. Below: **k-NN** checks the metric view; **linear/transformer** check separability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "NUM_CLASSES = 10\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "LATENT_DIM = 512\n",
        "NUM_LATENTS = 256\n",
        "\n",
        "# Sparse-context-only mode: all representations see only sparse observations (no full image).\n",
        "USE_SPARSE_CONTEXT = True\n",
        "CONTEXT_FRAC = 0.2\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "N_FULL = coords_32.size(0)\n",
        "N_SPARSE = max(64, int(N_FULL * CONTEXT_FRAC))\n",
        "\n",
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    return F.grid_sample(images, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True).squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def prepare_sparse_context(images, coords_full, fourier_encoder, num_sparse, device):\n",
        "    B = images.size(0)\n",
        "    idx = torch.randperm(coords_full.size(0), device=device)[:num_sparse]\n",
        "    coords_sparse = coords_full[idx]\n",
        "    pixels_sparse = sample_gt_at_coords(images, coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos_sparse = fourier_encoder(coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels_sparse, pos_sparse], dim=-1)\n",
        "\n",
        "def prepare_full_context(images, coords_full, fourier_encoder):\n",
        "    input_full, _, _ = prepare_model_input(images, coords_full, fourier_encoder)\n",
        "    return input_full\n",
        "\n",
        "def get_residual(model, data):\n",
        "    \"\"\"Encoder + processor -> (B, num_latents, latent_dim).\"\"\"\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "print(\"Device:\", DEVICE, \"| USE_SPARSE_CONTEXT:\", USE_SPARSE_CONTEXT, \"| N_SPARSE:\", N_SPARSE if USE_SPARSE_CONTEXT else \"N/A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sparse-context model (required)\n",
        "CKPT_SPARSE = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_best.pt\")\n",
        "if not os.path.isfile(CKPT_SPARSE):\n",
        "    CKPT_SPARSE = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_last.pt\")\n",
        "assert os.path.isfile(CKPT_SPARSE), \"Run SparseContext_CIFAR10.ipynb first.\"\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "sparse_model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "ckpt = torch.load(CKPT_SPARSE, map_location=DEVICE)\n",
        "sparse_model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
        "fourier_encoder.load_state_dict(ckpt[\"fourier_encoder_state_dict\"], strict=False)\n",
        "sparse_model.eval()\n",
        "fourier_encoder.eval()\n",
        "print(\"Loaded sparse:\", CKPT_SPARSE)\n",
        "\n",
        "# Optional: full-context baseline\n",
        "CKPT_BASELINE = os.path.join(CHECKPOINT_DIR, \"checkpoint_best.pt\")\n",
        "if not os.path.isfile(CKPT_BASELINE):\n",
        "    CKPT_BASELINE = os.path.join(CHECKPOINT_DIR, \"checkpoint_last.pt\")\n",
        "baseline_model = baseline_fourier = None\n",
        "if os.path.isfile(CKPT_BASELINE):\n",
        "    baseline_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    baseline_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_b = torch.load(CKPT_BASELINE, map_location=DEVICE)\n",
        "    baseline_model.load_state_dict(ckpt_b[\"model_state_dict\"], strict=False)\n",
        "    baseline_fourier.load_state_dict(ckpt_b[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    baseline_model.eval()\n",
        "    baseline_fourier.eval()\n",
        "    print(\"Loaded baseline:\", CKPT_BASELINE)\n",
        "\n",
        "# Optional: full-context NCE\n",
        "CKPT_NCE = os.path.join(CHECKPOINT_DIR, \"checkpoint_nce_best.pt\")\n",
        "if not os.path.isfile(CKPT_NCE):\n",
        "    CKPT_NCE = os.path.join(CHECKPOINT_DIR, \"softnce_best.pt\")\n",
        "nce_model = nce_fourier = None\n",
        "if os.path.isfile(CKPT_NCE):\n",
        "    nce_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    nce_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_n = torch.load(CKPT_NCE, map_location=DEVICE)\n",
        "    nce_model.load_state_dict(ckpt_n[\"model_state_dict\"], strict=False)\n",
        "    nce_fourier.load_state_dict(ckpt_n[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    nce_model.eval()\n",
        "    nce_fourier.eval()\n",
        "    print(\"Loaded NCE:\", CKPT_NCE)\n",
        "\n",
        "# Optional: sparse-context NCE finetuned\n",
        "CKPT_SFN = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_finetune_nce_best.pt\")\n",
        "if not os.path.isfile(CKPT_SFN):\n",
        "    CKPT_SFN = os.path.join(CHECKPOINT_DIR, \"checkpoint_sparse_finetune_nce_last.pt\")\n",
        "sfn_model = sfn_fourier = None\n",
        "if os.path.isfile(CKPT_SFN):\n",
        "    sfn_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    sfn_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    ckpt_sfn = torch.load(CKPT_SFN, map_location=DEVICE)\n",
        "    sfn_model.load_state_dict(ckpt_sfn[\"model_state_dict\"], strict=False)\n",
        "    sfn_fourier.load_state_dict(ckpt_sfn[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    sfn_model.eval()\n",
        "    sfn_fourier.eval()\n",
        "    print(\"Loaded sparse finetune NCE:\", CKPT_SFN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CIFAR-10; train with augmentation so patched-pixel baseline can compete\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_transform)\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Patchify images for \"original\" (full context); sparse original = (x,y,r,g,b) at N_SPARSE coords.\n",
        "PATCH_SIZE = 4\n",
        "PATCH_DIM = PATCH_SIZE * PATCH_SIZE * CHANNELS\n",
        "N_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "SPARSE_ORIGINAL_DIM = 5  # (x, y, r, g, b) at each sparse point\n",
        "\n",
        "def patchify(images):\n",
        "    \"\"\"(B, C, H, W) -> (B, n_patches, patch_dim).\"\"\"\n",
        "    B, C, H, W = images.shape\n",
        "    p = PATCH_SIZE\n",
        "    x = rearrange(images, \"b c (h ph) (w pw) -> b (h w) (ph pw c)\", ph=p, pw=p)\n",
        "    return x\n",
        "\n",
        "def get_representation(images, rep_name):\n",
        "    \"\"\"Return (B, seq_len, d). If USE_SPARSE_CONTEXT: all reps see only sparse observations.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        if rep_name == \"original\":\n",
        "            if USE_SPARSE_CONTEXT:\n",
        "                B = images.size(0)\n",
        "                idx = torch.randperm(coords_32.size(0), device=DEVICE)[:N_SPARSE]\n",
        "                coords_sparse = coords_32[idx]\n",
        "                pixels = sample_gt_at_coords(images, coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
        "                xy = coords_sparse.unsqueeze(0).expand(B, -1, -1)\n",
        "                return torch.cat([xy, pixels], dim=-1).float()\n",
        "            patches = patchify(images).to(DEVICE)\n",
        "            return patches.float()\n",
        "        if USE_SPARSE_CONTEXT:\n",
        "            input_ctx = prepare_sparse_context(images, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
        "        else:\n",
        "            input_ctx = prepare_full_context(images, coords_32, fourier_encoder)\n",
        "        if rep_name == \"sparse\":\n",
        "            return get_residual(sparse_model, input_ctx)\n",
        "        if rep_name == \"baseline\" and baseline_model is not None:\n",
        "            if USE_SPARSE_CONTEXT:\n",
        "                input_ctx = prepare_sparse_context(images, coords_32, baseline_fourier, N_SPARSE, DEVICE)\n",
        "            else:\n",
        "                input_ctx = prepare_full_context(images, coords_32, baseline_fourier)\n",
        "            return get_residual(baseline_model, input_ctx)\n",
        "        if rep_name == \"nce\" and nce_model is not None:\n",
        "            if USE_SPARSE_CONTEXT:\n",
        "                input_ctx = prepare_sparse_context(images, coords_32, nce_fourier, N_SPARSE, DEVICE)\n",
        "            else:\n",
        "                input_ctx = prepare_full_context(images, coords_32, nce_fourier)\n",
        "            return get_residual(nce_model, input_ctx)\n",
        "        if rep_name == \"sparse_finetune_nce\" and sfn_model is not None:\n",
        "            if USE_SPARSE_CONTEXT:\n",
        "                input_ctx = prepare_sparse_context(images, coords_32, sfn_fourier, N_SPARSE, DEVICE)\n",
        "            else:\n",
        "                input_ctx = prepare_full_context(images, coords_32, sfn_fourier)\n",
        "            return get_residual(sfn_model, input_ctx)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmallTransformerClassifier(nn.Module):\n",
        "    \"\"\"Small transformer on sequence (B, L, d_in); projects to d_model, then mean pool + linear to num_classes.\"\"\"\n",
        "\n",
        "    def __init__(self, d_in, d_model=256, num_heads=4, num_layers=2, mlp_ratio=2, num_classes=10, max_len=256):\n",
        "        super().__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_model = d_model\n",
        "        self.proj = nn.Linear(d_in, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, d_model) * 0.02)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_model * mlp_ratio,\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\",\n",
        "            norm_first=True,\n",
        "            dropout=0.1,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, _ = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x + self.pos_embed[:, :L, :]\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "def train_and_eval(rep_name, d_in, seq_len, epochs=5, lr=1e-3):\n",
        "    model = SmallTransformerClassifier(d_in=d_in, d_model=256, num_heads=4, num_layers=2, num_classes=NUM_CLASSES, max_len=max(seq_len, 256)).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            x = get_representation(images, rep_name)\n",
        "            if x is None:\n",
        "                continue\n",
        "            logits = model(x)\n",
        "            loss = ce(logits, labels)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                x = get_representation(images, rep_name)\n",
        "                if x is None:\n",
        "                    continue\n",
        "                logits = model(x)\n",
        "                correct += (logits.argmax(1) == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        acc = correct / total if total else 0\n",
        "        print(f\"  {rep_name} epoch {epoch+1}/{epochs} test acc: {acc:.4f}\")\n",
        "    return acc\n",
        "\n",
        "\n",
        "EPOCHS = 8\n",
        "print(\"Small transformer: d_model=256, 2 layers, 4 heads, dropout=0.1. Epochs:\", EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "print(\"1. Original (patched pixels)\")\n",
        "results[\"original\"] = train_and_eval(\"original\", d_in=PATCH_DIM, seq_len=N_PATCHES, epochs=EPOCHS)\n",
        "\n",
        "print(\"2. Sparse model, full context (last latent)\")\n",
        "results[\"sparse\"] = train_and_eval(\"sparse\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "\n",
        "if baseline_model is not None:\n",
        "    print(\"3. Full baseline (last latent)\")\n",
        "    results[\"baseline\"] = train_and_eval(\"baseline\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "else:\n",
        "    results[\"baseline\"] = None\n",
        "\n",
        "if nce_model is not None:\n",
        "    print(\"4. Full + NCE (last latent)\")\n",
        "    results[\"nce\"] = train_and_eval(\"nce\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "else:\n",
        "    results[\"nce\"] = None\n",
        "\n",
        "if sfn_model is not None:\n",
        "    print(\"5. Sparse + NCE finetuned, full context (last latent)\")\n",
        "    results[\"sparse_finetune_nce\"] = train_and_eval(\"sparse_finetune_nce\", d_in=LATENT_DIM, seq_len=NUM_LATENTS, epochs=EPOCHS)\n",
        "else:\n",
        "    results[\"sparse_finetune_nce\"] = None\n",
        "\n",
        "print(\"\\n--- Summary (test accuracy) ---\")\n",
        "for name, acc in results.items():\n",
        "    print(f\"  {name}: {acc:.4f}\" if acc is not None else f\"  {name}: (not loaded)\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "names = [k for k, v in results.items() if v is not None]\n",
        "accs = [results[k] for k in names]\n",
        "if names:\n",
        "    plt.bar(names, accs, color='steelblue')\n",
        "    plt.ylabel('Test accuracy')\n",
        "    plt.title('Classification comparison (small transformer, full data, few epochs)')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=15)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('latent_classification_comparison.png', dpi=100)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### k-NN and linear probe (metric vs separability)\n",
        "\n",
        "**k-NN** uses the raw distance in the representation space (no trained head). It matches what t-SNE is showing: good *neighborhood* structure → good k-NN. **Linear probe** trains a single linear layer; it often correlates with transformer accuracy. We mean-pool each representation to one vector per image, then evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def get_pooled_representation(images, rep_name):\n",
        "    \"\"\"(B, L, d) -> (B, d) by mean pool. Returns None if rep not available.\"\"\"\n",
        "    x = get_representation(images, rep_name)\n",
        "    if x is None:\n",
        "        return None\n",
        "    return x.mean(dim=1)\n",
        "\n",
        "def collect_features(loader, rep_name):\n",
        "    feats, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            z = get_pooled_representation(imgs, rep_name)\n",
        "            if z is None:\n",
        "                return None, None\n",
        "            feats.append(z.cpu().numpy())\n",
        "            labels.append(lbls.numpy())\n",
        "    return np.concatenate(feats, axis=0), np.concatenate(labels, axis=0)\n",
        "\n",
        "KNN_K = 20\n",
        "rep_names = [\"original\", \"sparse\", \"baseline\", \"nce\", \"sparse_finetune_nce\"]\n",
        "knn_results = {}\n",
        "linear_results = {}\n",
        "for name in rep_names:\n",
        "    X_tr, y_tr = collect_features(train_loader, name)\n",
        "    if X_tr is None:\n",
        "        knn_results[name] = None\n",
        "        linear_results[name] = None\n",
        "        continue\n",
        "    X_te, y_te = collect_features(test_loader, name)\n",
        "    knn = KNeighborsClassifier(n_neighbors=KNN_K, weights=\"distance\")\n",
        "    knn.fit(X_tr, y_tr)\n",
        "    knn_results[name] = knn.score(X_te, y_te)\n",
        "    lr = LogisticRegression(max_iter=500, C=0.1, multi_class=\"multinomial\")\n",
        "    lr.fit(X_tr, y_tr)\n",
        "    linear_results[name] = lr.score(X_te, y_te)\n",
        "    print(f\"{name}: k-NN (k={KNN_K}) = {knn_results[name]:.4f}, linear = {linear_results[name]:.4f}\")\n",
        "\n",
        "print(\"\\n--- k-NN (metric / neighborhood, aligns with t-SNE) ---\")\n",
        "for name in rep_names:\n",
        "    v = knn_results.get(name)\n",
        "    print(f\"  {name}: {v:.4f}\" if v is not None else f\"  {name}: (skip)\")\n",
        "print(\"\\n--- Linear probe (separability) ---\")\n",
        "for name in rep_names:\n",
        "    v = linear_results.get(name)\n",
        "    print(f\"  {name}: {v:.4f}\" if v is not None else f\"  {name}: (skip)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
