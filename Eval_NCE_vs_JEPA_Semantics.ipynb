{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic learning capacity: NCE vs JEPA\n",
        "\n",
        "Compare **per-token** semantic structure of:\n",
        "- **NCE** (SemanticTokenGenerator_CIFAR10: Soft InfoNCE + reconstruction)\n",
        "- **JEPA** (JEPA_NeuralField_CIFAR10: masked prediction of φ + EMA target + VICReg + RGB aux)\n",
        "\n",
        "**Per-token t-SNE**: each point = one φ token at one spatial location from one image; color = CIFAR-10 class of that image. Better semantic learning → clearer class clustering.\n",
        "\n",
        "Optional: **k-NN accuracy** on mean-pooled φ (one vector per image) as a scalar metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "PHI_DIM = 128\n",
        "\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_residual(model, context):\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=context, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def decoder_forward(model, queries, residual):\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return x\n",
        "\n",
        "def get_phi_raw(model, queries, residual):\n",
        "    return decoder_forward(model, queries, residual)\n",
        "\n",
        "def get_semantic_tokens(phi_raw, semantic_head):\n",
        "    return F.normalize(semantic_head(phi_raw), dim=-1)\n",
        "\n",
        "def phi_at_resolution(model, semantic_head, fourier_encoder, full_input, res_h, res_w):\n",
        "    \"\"\"(B, res_h*res_w, PHI_DIM) L2-normalized.\"\"\"\n",
        "    coords = create_coordinate_grid(res_h, res_w, full_input.device)\n",
        "    B = full_input.size(0)\n",
        "    residual = get_residual(model, full_input)\n",
        "    queries = fourier_encoder(repeat(coords, \"n d -> b n d\", b=B))\n",
        "    phi_raw = get_phi_raw(model, queries, residual)\n",
        "    return get_semantic_tokens(phi_raw, semantic_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load NCE model (semantic token checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CKPT_NCE = os.path.join(CHECKPOINT_DIR, \"checkpoint_semantic_token_best.pt\")\n",
        "if not os.path.isfile(CKPT_NCE):\n",
        "    CKPT_NCE = os.path.join(CHECKPOINT_DIR, \"checkpoint_semantic_token_last.pt\")\n",
        "nce_model = nce_fourier = nce_semantic_head = None\n",
        "if os.path.isfile(CKPT_NCE):\n",
        "    nce_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    nce_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    nce_semantic_head = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "    ckpt = torch.load(CKPT_NCE, map_location=DEVICE)\n",
        "    nce_model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
        "    nce_fourier.load_state_dict(ckpt[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    nce_semantic_head.load_state_dict(ckpt[\"semantic_head_state_dict\"], strict=False)\n",
        "    nce_model.eval()\n",
        "    nce_fourier.eval()\n",
        "    nce_semantic_head.eval()\n",
        "    print(\"Loaded NCE:\", CKPT_NCE)\n",
        "else:\n",
        "    print(\"NCE checkpoint not found; run SemanticTokenGenerator_CIFAR10.ipynb first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load JEPA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CKPT_JEPA = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_best.pt\")\n",
        "if not os.path.isfile(CKPT_JEPA):\n",
        "    CKPT_JEPA = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_last.pt\")\n",
        "jepa_model = jepa_fourier = jepa_semantic_head = None\n",
        "if os.path.isfile(CKPT_JEPA):\n",
        "    jepa_fourier = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "    jepa_model = CascadedPerceiverIO(\n",
        "        input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "        latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        "    ).to(DEVICE)\n",
        "    jepa_semantic_head = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "    ckpt = torch.load(CKPT_JEPA, map_location=DEVICE)\n",
        "    jepa_model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
        "    jepa_fourier.load_state_dict(ckpt[\"fourier_encoder_state_dict\"], strict=False)\n",
        "    jepa_semantic_head.load_state_dict(ckpt[\"semantic_head_state_dict\"], strict=False)\n",
        "    jepa_model.eval()\n",
        "    jepa_fourier.eval()\n",
        "    jepa_semantic_head.eval()\n",
        "    print(\"Loaded JEPA:\", CKPT_JEPA)\n",
        "else:\n",
        "    print(\"JEPA checkpoint not found; run JEPA_NeuralField_CIFAR10.ipynb first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "N_EVAL = min(400, len(test_ds))\n",
        "TOKEN_RES = 16\n",
        "N_TOKENS_PER_IMAGE = TOKEN_RES * TOKEN_RES\n",
        "print(\"N_EVAL images:\", N_EVAL, \"| Tokens per image:\", N_TOKENS_PER_IMAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collect per-token φ (and labels) for NCE and JEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_phi_nce, all_phi_jepa, all_labels = [], [], []\n",
        "n_done = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        if n_done >= N_EVAL:\n",
        "            break\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B = imgs.size(0)\n",
        "        labels_rep = labels.numpy().repeat(N_TOKENS_PER_IMAGE)\n",
        "        all_labels.append(labels_rep)\n",
        "        if nce_model is not None:\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, nce_fourier)\n",
        "            phi_nce = phi_at_resolution(nce_model, nce_semantic_head, nce_fourier, full_input, TOKEN_RES, TOKEN_RES)\n",
        "            all_phi_nce.append(phi_nce.reshape(B * N_TOKENS_PER_IMAGE, PHI_DIM).cpu().numpy())\n",
        "        if jepa_model is not None:\n",
        "            full_input_j, _, _ = prepare_model_input(imgs, coords_32, jepa_fourier)\n",
        "            phi_jepa = phi_at_resolution(jepa_model, jepa_semantic_head, jepa_fourier, full_input_j, TOKEN_RES, TOKEN_RES)\n",
        "            all_phi_jepa.append(phi_jepa.reshape(B * N_TOKENS_PER_IMAGE, PHI_DIM).cpu().numpy())\n",
        "        n_done += B\n",
        "\n",
        "y_all = np.concatenate(all_labels, axis=0)\n",
        "X_nce = np.concatenate(all_phi_nce, axis=0) if all_phi_nce else None\n",
        "X_jepa = np.concatenate(all_phi_jepa, axis=0) if all_phi_jepa else None\n",
        "print(\"Per-token features: NCE\", X_nce.shape if X_nce is not None else None, \"| JEPA\", X_jepa.shape if X_jepa is not None else None)\n",
        "print(\"Labels (one per token):\", y_all.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-token t-SNE: NCE vs JEPA (colored by image class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "    n_comp = min(50, (X_nce.shape[1] if X_nce is not None else X_jepa.shape[1]), (X_nce.shape[0] if X_nce is not None else X_jepa.shape[0]) - 1)\n",
        "    n_plots = (1 if X_nce is not None else 0) + (1 if X_jepa is not None else 0)\n",
        "    if n_plots == 0:\n",
        "        raise RuntimeError(\"No model loaded\")\n",
        "    fig, axs = plt.subplots(1, n_plots, figsize=(7 * n_plots, 6))\n",
        "    if n_plots == 1:\n",
        "        axs = [axs]\n",
        "    idx = 0\n",
        "    perplexity = min(30, X_nce.shape[0] // 4) if X_nce is not None else min(30, X_jepa.shape[0] // 4)\n",
        "    if X_nce is not None:\n",
        "        X_nce_pca = PCA(n_components=n_comp).fit_transform(X_nce)\n",
        "        X_nce_tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(X_nce_pca)\n",
        "        sc = axs[idx].scatter(X_nce_tsne[:, 0], X_nce_tsne[:, 1], c=y_all, cmap=\"tab10\", s=2, alpha=0.5)\n",
        "        axs[idx].set_title(\"NCE (per-token φ, color = image class)\")\n",
        "        axs[idx].set_xlabel(\"t-SNE 1\")\n",
        "        axs[idx].set_ylabel(\"t-SNE 2\")\n",
        "        idx += 1\n",
        "    if X_jepa is not None:\n",
        "        X_jepa_pca = PCA(n_components=n_comp).fit_transform(X_jepa)\n",
        "        X_jepa_tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(X_jepa_pca)\n",
        "        sc = axs[idx].scatter(X_jepa_tsne[:, 0], X_jepa_tsne[:, 1], c=y_all, cmap=\"tab10\", s=2, alpha=0.5)\n",
        "        axs[idx].set_title(\"JEPA (per-token φ, color = image class)\")\n",
        "        axs[idx].set_xlabel(\"t-SNE 1\")\n",
        "        axs[idx].set_ylabel(\"t-SNE 2\")\n",
        "    plt.colorbar(sc, ax=axs, label=\"CIFAR-10 class\", shrink=0.6)\n",
        "    plt.suptitle(\"Semantics: NCE vs JEPA — per-token t-SNE (each point = one φ token)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"eval_nce_vs_jepa_tsne.png\", dpi=100)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"t-SNE failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: k-NN accuracy on mean-pooled φ (one vector per image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "n_imgs = len(y_all) // N_TOKENS_PER_IMAGE\n",
        "y_pooled = y_all[::N_TOKENS_PER_IMAGE]\n",
        "\n",
        "if X_nce is not None:\n",
        "    X_nce_pooled = X_nce.reshape(n_imgs, N_TOKENS_PER_IMAGE, -1).mean(axis=1)\n",
        "    knn_nce = KNeighborsClassifier(n_neighbors=20, weights=\"distance\")\n",
        "    knn_nce.fit(X_nce_pooled, y_pooled)\n",
        "    acc_nce = knn_nce.score(X_nce_pooled, y_pooled)\n",
        "    print(\"NCE k-NN accuracy (mean-pooled φ, train on same set):\", f\"{acc_nce:.4f}\")\n",
        "if X_jepa is not None:\n",
        "    X_jepa_pooled = X_jepa.reshape(n_imgs, N_TOKENS_PER_IMAGE, -1).mean(axis=1)\n",
        "    knn_jepa = KNeighborsClassifier(n_neighbors=20, weights=\"distance\")\n",
        "    knn_jepa.fit(X_jepa_pooled, y_pooled)\n",
        "    acc_jepa = knn_jepa.score(X_jepa_pooled, y_pooled)\n",
        "    print(\"JEPA k-NN accuracy (mean-pooled φ, train on same set):\", f\"{acc_jepa:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
