{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JEPA from Pretrained OmniField — CIFAR-10\n",
        "\n",
        "Load a **reconstruction** checkpoint (OmniField or AblationCIFAR10), then run **JEPA training** from that initialization.\n",
        "\n",
        "- Discovers a suitable pretrained checkpoint automatically (prefers `checkpoints_omnifield_cifar10/omnifield_cifar10_best.pt`, else `checkpoints/checkpoint_best.pt`).\n",
        "- Loads pretrained weights into the encoder and fourier encoder; creates semantic_head + predictor from scratch.\n",
        "- Runs the same JEPA loop as `JEPA_NeuralField_CIFAR10.ipynb` (VICReg, EMA, block masking, RGB aux, etc.).\n",
        "- Saves checkpoints as `checkpoint_jepa_from_pretrained_best.pt` / `_last.pt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from nf_feature_models import (\n",
        "    CascadedPerceiverIO,\n",
        "    GaussianFourierFeatures,\n",
        "    create_coordinate_grid,\n",
        "    prepare_model_input,\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "print(\"Device:\", DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "FOURIER_MAPPING_SIZE = 96\n",
        "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
        "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
        "QUERIES_DIM = POS_EMBED_DIM\n",
        "LOGITS_DIM = CHANNELS\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "N_FULL = IMAGE_SIZE * IMAGE_SIZE\n",
        "\n",
        "# JEPA: context vs target coord split\n",
        "MASK_STYLE = \"ijepa_block\"\n",
        "N_CONTEXT = 512\n",
        "N_TARGET = 256\n",
        "USE_BLOCK_MASK = MASK_STYLE == \"ijepa_block\"\n",
        "\n",
        "# I-JEPA-style block masking\n",
        "PATCH_SIZE = 4\n",
        "ENC_MASK_SCALE = (0.85, 1.0)\n",
        "PRED_MASK_SCALE = (0.15, 0.25)\n",
        "ASPECT_RATIO = (0.75, 1.5)\n",
        "N_ENC_MASKS = 1\n",
        "N_PRED_MASKS = 2\n",
        "ALLOW_OVERLAP = False\n",
        "MIN_KEEP = 4\n",
        "\n",
        "# Verbose training\n",
        "VERBOSE = True\n",
        "LOG_EVERY = 20\n",
        "\n",
        "# I-JEPA faithful: target = EMA(full image) at target coords\n",
        "TARGET_FROM_FULL_CONTEXT = True\n",
        "\n",
        "# EMA target\n",
        "EMA_MOMENTUM = 0.996\n",
        "EMA_MOMENTUM_RAMP = True\n",
        "EMA_MOMENTUM_START = 0.996\n",
        "EMA_MOMENTUM_END = 0.999\n",
        "\n",
        "# Losses\n",
        "USE_JEPA_LOSS = True\n",
        "USE_LAYERNORM_JEPA = True\n",
        "JEPA_LOSS_TYPE = \"smooth_l1\"\n",
        "USE_RGB_AUX = True\n",
        "LAMBDA_RGB = 0.15\n",
        "RGB_QUERY_RES = 16\n",
        "USE_VICREG = True\n",
        "LAMBDA_VICREG = 0.02\n",
        "VICREG_SIM_WEIGHT = 25.0\n",
        "VICREG_VAR_WEIGHT = 25.0\n",
        "VICREG_COV_WEIGHT = 1.0\n",
        "\n",
        "PHI_DIM = 128\n",
        "PREDICTOR_DIM = 256\n",
        "PREDICTOR_HEADS = 4\n",
        "\n",
        "# Override: set to a specific path to skip auto-detection\n",
        "PRETRAINED_CKPT_PATH = None  # e.g. \"checkpoints_omnifield_cifar10/omnifield_cifar10_best.pt\"\n",
        "\n",
        "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
        "print(\"N_FULL:\", N_FULL, \"| MASK_STYLE:\", MASK_STYLE, \"| TARGET_FROM_FULL_CONTEXT:\", TARGET_FROM_FULL_CONTEXT)\n",
        "print(\"JEPA_LOSS_TYPE:\", JEPA_LOSS_TYPE, \"| USE_LAYERNORM_JEPA:\", USE_LAYERNORM_JEPA, \"| USE_VICREG:\", USE_VICREG, \"| EMA_ramp:\", EMA_MOMENTUM_RAMP, end=\"\")\n",
        "if EMA_MOMENTUM_RAMP:\n",
        "    print(\" [%s, %s]\" % (EMA_MOMENTUM_START, EMA_MOMENTUM_END))\n",
        "else:\n",
        "    print()\n",
        "if MASK_STYLE == \"ijepa_block\":\n",
        "    print(\"  I-JEPA blocks: patch_size=%d enc_scale=%s pred_scale=%s aspect=%s n_enc=%d n_pred=%d\" % (PATCH_SIZE, ENC_MASK_SCALE, PRED_MASK_SCALE, ASPECT_RATIO, N_ENC_MASKS, N_PRED_MASKS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpoint Discovery\n",
        "\n",
        "Auto-detect the best pretrained reconstruction checkpoint. Preference order:\n",
        "1. `checkpoints_omnifield_cifar10/omnifield_cifar10_best.pt` (OmniField)\n",
        "2. `checkpoints/checkpoint_best.pt` (AblationCIFAR10)\n",
        "\n",
        "If both exist, prefer the one with lower `best_val_loss` / `val_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def discover_pretrained_checkpoint(override_path=None):\n",
        "    \"\"\"Find the best pretrained reconstruction checkpoint.\n",
        "    Returns (path, source_name) or raises if none found.\"\"\"\n",
        "    if override_path is not None:\n",
        "        if os.path.isfile(override_path):\n",
        "            return override_path, \"user-specified\"\n",
        "        raise FileNotFoundError(f\"User-specified checkpoint not found: {override_path}\")\n",
        "\n",
        "    candidates = [\n",
        "        (\"checkpoints_omnifield_cifar10/omnifield_cifar10_best.pt\", \"OmniFieldCifar10\"),\n",
        "        (\"checkpoints_omnifield_cifar10/omnifield_cifar10_last.pt\", \"OmniFieldCifar10 (last)\"),\n",
        "        (\"checkpoints/checkpoint_best.pt\", \"AblationCIFAR10\"),\n",
        "        (\"checkpoints/checkpoint_last.pt\", \"AblationCIFAR10 (last)\"),\n",
        "    ]\n",
        "    found = [(p, name) for p, name in candidates if os.path.isfile(p)]\n",
        "    if not found:\n",
        "        raise FileNotFoundError(\n",
        "            \"No pretrained reconstruction checkpoint found!\\n\"\n",
        "            \"Please run OmniFieldCifar10.ipynb or AblationCIFAR10.ipynb first to produce a checkpoint.\\n\"\n",
        "            f\"Searched: {[p for p, _ in candidates]}\"\n",
        "        )\n",
        "\n",
        "    # If we have both OmniField-best and Ablation-best, compare val_loss\n",
        "    omni_best = next(((p, n) for p, n in found if p == \"checkpoints_omnifield_cifar10/omnifield_cifar10_best.pt\"), None)\n",
        "    abl_best = next(((p, n) for p, n in found if p == \"checkpoints/checkpoint_best.pt\"), None)\n",
        "    if omni_best and abl_best:\n",
        "        try:\n",
        "            ckpt_o = torch.load(omni_best[0], map_location=\"cpu\")\n",
        "            ckpt_a = torch.load(abl_best[0], map_location=\"cpu\")\n",
        "            loss_o = ckpt_o.get(\"val_loss\", ckpt_o.get(\"best_val_loss\", float(\"inf\")))\n",
        "            loss_a = ckpt_a.get(\"best_val_loss\", ckpt_a.get(\"val_loss\", float(\"inf\")))\n",
        "            print(f\"  OmniField val_loss={loss_o:.6f}, Ablation best_val_loss={loss_a:.6f}\")\n",
        "            if loss_o <= loss_a:\n",
        "                return omni_best\n",
        "            else:\n",
        "                return abl_best\n",
        "        except Exception:\n",
        "            pass  # fallback to preference order\n",
        "\n",
        "    return found[0]  # first available in preference order\n",
        "\n",
        "\n",
        "PRETRAINED_PATH, PRETRAINED_SOURCE = discover_pretrained_checkpoint(PRETRAINED_CKPT_PATH)\n",
        "print(f\"Selected pretrained checkpoint: {PRETRAINED_PATH} (source: {PRETRAINED_SOURCE})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "Field output (RGB + φ), context/target split, I-JEPA block masking, predictor, losses, EMA — same as `JEPA_NeuralField_CIFAR10.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Field output helpers ──\n",
        "\n",
        "def sample_gt_at_coords(images, coords):\n",
        "    B, C, H, W = images.shape\n",
        "    N = coords.shape[1]\n",
        "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
        "    sampled = F.grid_sample(images, grid, mode=\"bilinear\", padding_mode=\"border\", align_corners=True)\n",
        "    return sampled.squeeze(2).permute(0, 2, 1)\n",
        "\n",
        "def get_residual(model, context):\n",
        "    residual = None\n",
        "    for block in model.encoder_blocks:\n",
        "        residual = block(x=residual, context=context, mask=None, residual=residual)\n",
        "    for sa_block in model.self_attn_blocks:\n",
        "        residual = sa_block[0](residual) + residual\n",
        "        residual = sa_block[1](residual) + residual\n",
        "    return residual\n",
        "\n",
        "def decoder_forward(model, queries, residual):\n",
        "    x = model.decoder_cross_attn(queries, context=residual)\n",
        "    x = x + queries\n",
        "    if model.decoder_ff is not None:\n",
        "        x = x + model.decoder_ff(x)\n",
        "    return x\n",
        "\n",
        "def get_rgb(model, queries, residual):\n",
        "    return model.to_logits(decoder_forward(model, queries, residual))\n",
        "\n",
        "def get_phi_raw(model, queries, residual):\n",
        "    return decoder_forward(model, queries, residual)\n",
        "\n",
        "def get_semantic_tokens(phi_raw, semantic_head):\n",
        "    return F.normalize(semantic_head(phi_raw), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── I-JEPA block masking ──\n",
        "\n",
        "def _patch_to_pixel_indices(patch_indices_flat, h_patch, w_patch, patch_size, image_size, device):\n",
        "    pixels_per_patch = patch_size * patch_size\n",
        "    pixel_indices = []\n",
        "    for p_flat in patch_indices_flat:\n",
        "        pi, pj = p_flat // w_patch, p_flat % w_patch\n",
        "        for di in range(patch_size):\n",
        "            for dj in range(patch_size):\n",
        "                i, j = pi * patch_size + di, pj * patch_size + dj\n",
        "                pixel_indices.append(i * image_size + j)\n",
        "    return torch.tensor(pixel_indices, device=device, dtype=torch.long)\n",
        "\n",
        "def sample_ijepa_block_indices(image_size, patch_size, enc_mask_scale, pred_mask_scale, aspect_ratio,\n",
        "                               n_enc_masks, n_pred_masks, allow_overlap, min_keep, device, generator=None):\n",
        "    h_patch = image_size // patch_size\n",
        "    w_patch = image_size // patch_size\n",
        "    n_patches = h_patch * w_patch\n",
        "    if generator is None:\n",
        "        generator = torch.Generator(device=device)\n",
        "\n",
        "    def sample_block_size(scale_lo, scale_hi, aspect_lo, aspect_hi):\n",
        "        s = scale_lo + torch.rand(1, device=device, generator=generator).item() * (scale_hi - scale_lo)\n",
        "        ar = aspect_lo + torch.rand(1, device=device, generator=generator).item() * (aspect_hi - aspect_lo)\n",
        "        max_keep = int(n_patches * s)\n",
        "        h = max(1, min(h_patch - 1, int(round((max_keep * ar) ** 0.5))))\n",
        "        w = max(1, min(w_patch - 1, int(round((max_keep / ar) ** 0.5))))\n",
        "        return h, w\n",
        "\n",
        "    def sample_one_block(bh, bw, acceptable_region=None):\n",
        "        for _ in range(30):\n",
        "            top = torch.randint(0, h_patch - bh + 1, (1,), device=device, generator=generator).item()\n",
        "            left = torch.randint(0, w_patch - bw + 1, (1,), device=device, generator=generator).item()\n",
        "            block = torch.zeros(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "            block[top : top + bh, left : left + bw] = True\n",
        "            if acceptable_region is not None:\n",
        "                if not (block & acceptable_region == block).all():\n",
        "                    continue\n",
        "            idx = torch.nonzero(block.flatten(), as_tuple=False).squeeze(-1)\n",
        "            if len(idx) >= min_keep:\n",
        "                complement = torch.ones(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "                complement[top : top + bh, left : left + bw] = False\n",
        "                return idx, complement\n",
        "        return None, None\n",
        "\n",
        "    # 1) Target (pred) blocks\n",
        "    ph, pw = sample_block_size(pred_mask_scale[0], pred_mask_scale[1], aspect_ratio[0], aspect_ratio[1])\n",
        "    all_pred_patches = set()\n",
        "    pred_complements = []\n",
        "    for _ in range(n_pred_masks):\n",
        "        idx, comp = sample_one_block(ph, pw, None)\n",
        "        if idx is None:\n",
        "            continue\n",
        "        pred_complements.append(comp)\n",
        "        for i in idx.cpu().tolist():\n",
        "            all_pred_patches.add(i)\n",
        "    pred_patches_flat = list(all_pred_patches)\n",
        "    if len(pred_patches_flat) < min_keep:\n",
        "        pred_patches_flat = list(range(n_patches))[: min_keep + 1]\n",
        "    idx_t = _patch_to_pixel_indices(pred_patches_flat, h_patch, w_patch, patch_size, image_size, device)\n",
        "\n",
        "    # 2) Context (enc) blocks: only in complement of target\n",
        "    acceptable = None if allow_overlap else torch.ones(h_patch, w_patch, dtype=torch.bool, device=device)\n",
        "    if not allow_overlap and pred_complements:\n",
        "        for c in pred_complements:\n",
        "            acceptable = acceptable & c\n",
        "    eh, ew = sample_block_size(enc_mask_scale[0], enc_mask_scale[1], 1.0, 1.0)\n",
        "    all_enc_patches = set()\n",
        "    for _ in range(n_enc_masks):\n",
        "        idx, _ = sample_one_block(eh, ew, acceptable)\n",
        "        if idx is not None:\n",
        "            for i in idx.cpu().tolist():\n",
        "                all_enc_patches.add(i)\n",
        "    enc_patches_flat = list(all_enc_patches)\n",
        "    if len(enc_patches_flat) < min_keep:\n",
        "        enc_patches_flat = [k for k in range(n_patches) if k not in all_pred_patches][: min_keep + 1]\n",
        "    if not enc_patches_flat:\n",
        "        enc_patches_flat = [k for k in range(n_patches) if k not in all_pred_patches]\n",
        "    idx_c = _patch_to_pixel_indices(enc_patches_flat, h_patch, w_patch, patch_size, image_size, device)\n",
        "\n",
        "    return idx_c, idx_t\n",
        "\n",
        "def sample_context_target_indices(coords_full, n_context, n_target, device, block_mask=False,\n",
        "                                  mask_style=\"random\", image_size=32, patch_size=4,\n",
        "                                  enc_mask_scale=(0.85, 1.0), pred_mask_scale=(0.15, 0.25),\n",
        "                                  aspect_ratio=(0.75, 1.5), n_enc_masks=1, n_pred_masks=2,\n",
        "                                  allow_overlap=False, min_keep=4):\n",
        "    n_total = coords_full.size(0)\n",
        "    if mask_style == \"ijepa_block\":\n",
        "        return sample_ijepa_block_indices(\n",
        "            image_size, patch_size, enc_mask_scale, pred_mask_scale, aspect_ratio,\n",
        "            n_enc_masks, n_pred_masks, allow_overlap, min_keep, device)\n",
        "    if block_mask:\n",
        "        h, w = int(math.sqrt(n_total)), int(math.sqrt(n_total))\n",
        "        nh, nw = max(1, h // 4), max(1, w // 4)\n",
        "        top = random.randint(0, h - nh)\n",
        "        left = random.randint(0, w - nw)\n",
        "        target_flat = []\n",
        "        for i in range(top, top + nh):\n",
        "            for j in range(left, left + nw):\n",
        "                target_flat.append(i * w + j)\n",
        "        idx_t = torch.tensor(target_flat, device=device, dtype=torch.long)\n",
        "        idx_c = torch.tensor([k for k in range(n_total) if k not in set(target_flat)], device=device)\n",
        "        if idx_c.size(0) > n_context:\n",
        "            idx_c = idx_c[torch.randperm(idx_c.size(0), device=device)[:n_context]]\n",
        "    else:\n",
        "        perm = torch.randperm(n_total, device=device)\n",
        "        idx_c = perm[:n_context]\n",
        "        idx_t = perm[n_context : n_context + n_target]\n",
        "    return idx_c, idx_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Encoder input preparation ──\n",
        "\n",
        "def prepare_context_input(images, coords_full, fourier_encoder, idx_c, device):\n",
        "    B = images.size(0)\n",
        "    coords_c = coords_full[idx_c]\n",
        "    pixels = sample_gt_at_coords(images, coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    pos = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "    return torch.cat([pixels, pos], dim=-1)\n",
        "\n",
        "def prepare_jepa_encoder_input(images, coords_full, fourier_encoder, idx_c, idx_t, device, mask_rgb):\n",
        "    B = images.size(0)\n",
        "    context_input = prepare_context_input(images, coords_full, fourier_encoder, idx_c, device)\n",
        "    coords_t = coords_full[idx_t]\n",
        "    pos_t = fourier_encoder(coords_t.unsqueeze(0).expand(B, -1, -1))\n",
        "    rgb_t = mask_rgb.expand(B, coords_t.size(0), -1)\n",
        "    target_placeholder = torch.cat([rgb_t, pos_t], dim=-1)\n",
        "    return torch.cat([context_input, target_placeholder], dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Cross-attention predictor ──\n",
        "\n",
        "class JEPAPredictor(nn.Module):\n",
        "    \"\"\"Predict target embeddings from context: z_hat_t = Attn(q=embed(X_t), k=z_c, v=z_c).\"\"\"\n",
        "\n",
        "    def __init__(self, coord_embed_dim, phi_dim, pred_dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.to_q = nn.Linear(coord_embed_dim, pred_dim)\n",
        "        self.to_kv = nn.Linear(phi_dim, pred_dim * 2)\n",
        "        self.to_out = nn.Linear(pred_dim, phi_dim)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = pred_dim // num_heads\n",
        "\n",
        "    def forward(self, coords_t_embed, z_c):\n",
        "        B, N_t, _ = coords_t_embed.shape\n",
        "        _, N_c, _ = z_c.shape\n",
        "        q = self.to_q(coords_t_embed).view(B, N_t, self.num_heads, self.d_head).transpose(1, 2)\n",
        "        kv = self.to_kv(z_c).view(B, N_c, 2, self.num_heads, self.d_head)\n",
        "        k, v = kv[:, :, 0].transpose(1, 2), kv[:, :, 1].transpose(1, 2)\n",
        "        scale = self.d_head ** -0.5\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).reshape(B, N_t, -1)\n",
        "        return self.to_out(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── JEPA losses + VICReg ──\n",
        "\n",
        "def jepa_cosine_loss(z_pred, z_target):\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    return (1 - (z_pred * z_target).sum(dim=-1)).mean()\n",
        "\n",
        "def jepa_l2_loss(z_pred, z_target):\n",
        "    z_pred = F.normalize(z_pred, dim=-1)\n",
        "    return (2 - 2 * (z_pred * z_target).sum(dim=-1)).mean()\n",
        "\n",
        "def jepa_smooth_l1_loss(z_pred, z_target_raw):\n",
        "    return F.smooth_l1_loss(z_pred, z_target_raw).mean()\n",
        "\n",
        "def vicreg_loss(z, sim_weight=25.0, var_weight=25.0, cov_weight=1.0):\n",
        "    B, N, D = z.shape\n",
        "    z = z.reshape(B * N, D)\n",
        "    std = z.std(dim=0) + 1e-4\n",
        "    var_loss = torch.mean(F.relu(1 - std))\n",
        "    z_centered = z - z.mean(dim=0)\n",
        "    cov = (z_centered.T @ z_centered) / (z.size(0) - 1)\n",
        "    cov_loss = (cov.pow(2).sum() - cov.diag().pow(2).sum()) / D\n",
        "    return var_weight * var_loss + cov_weight * cov_loss\n",
        "\n",
        "def copy_ema(source, target, momentum=0.996):\n",
        "    for p_s, p_t in zip(source.parameters(), target.parameters()):\n",
        "        p_t.data.mul_(momentum).add_(p_s.data, alpha=1 - momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Models + Load Pretrained Checkpoint\n",
        "\n",
        "Build the same architecture as `JEPA_NeuralField_CIFAR10.ipynb`, then load pretrained weights into `model` and `fourier_encoder` from the discovered checkpoint. `semantic_head`, `predictor`, and `mask_rgb` are created fresh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Build models (same constructors as JEPA notebook) ──\n",
        "\n",
        "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "model = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "semantic_head = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)  # new, not from pretrained\n",
        "\n",
        "# EMA copies\n",
        "model_ema = CascadedPerceiverIO(\n",
        "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
        "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
        ").to(DEVICE)\n",
        "fourier_ema = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
        "semantic_head_ema = nn.Linear(QUERIES_DIM, PHI_DIM).to(DEVICE)\n",
        "\n",
        "# Learnable [MASK] for target positions\n",
        "mask_rgb = nn.Parameter(torch.zeros(1, 1, CHANNELS).to(DEVICE))\n",
        "\n",
        "# Predictor (new, not from pretrained)\n",
        "predictor = JEPAPredictor(POS_EMBED_DIM, PHI_DIM, PREDICTOR_DIM, num_heads=PREDICTOR_HEADS).to(DEVICE)\n",
        "\n",
        "print(\"Models built (same architecture as JEPA notebook).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Load pretrained checkpoint ──\n",
        "\n",
        "ckpt = torch.load(PRETRAINED_PATH, map_location=DEVICE)\n",
        "print(f\"Checkpoint keys: {list(ckpt.keys())}\")\n",
        "\n",
        "# Handle two key conventions:\n",
        "# OmniFieldCifar10: model_state / fourier_state\n",
        "# AblationCIFAR10:  model_state_dict / fourier_encoder_state_dict\n",
        "if \"model_state_dict\" in ckpt:\n",
        "    model_sd = ckpt[\"model_state_dict\"]\n",
        "    fourier_sd = ckpt[\"fourier_encoder_state_dict\"]\n",
        "    key_style = \"model_state_dict / fourier_encoder_state_dict\"\n",
        "elif \"model_state\" in ckpt:\n",
        "    model_sd = ckpt[\"model_state\"]\n",
        "    fourier_sd = ckpt[\"fourier_state\"]\n",
        "    key_style = \"model_state / fourier_state\"\n",
        "else:\n",
        "    raise KeyError(f\"Checkpoint has neither 'model_state_dict' nor 'model_state'. Keys: {list(ckpt.keys())}\")\n",
        "\n",
        "# Load into model and fourier_encoder (strict=False: pretrained has no semantic_head, may differ slightly)\n",
        "missing_m, unexpected_m = model.load_state_dict(model_sd, strict=False)\n",
        "missing_f, unexpected_f = fourier_encoder.load_state_dict(fourier_sd, strict=False)\n",
        "\n",
        "print(f\"\\nLoaded pretrained from: {PRETRAINED_PATH} (source: {PRETRAINED_SOURCE})\")\n",
        "print(f\"  Key style: {key_style}\")\n",
        "if \"best_val_loss\" in ckpt:\n",
        "    print(f\"  Pretrained best_val_loss: {ckpt['best_val_loss']:.6f}\")\n",
        "elif \"val_loss\" in ckpt:\n",
        "    print(f\"  Pretrained val_loss: {ckpt['val_loss']:.6f}\")\n",
        "if \"epoch\" in ckpt:\n",
        "    print(f\"  Pretrained epoch: {ckpt['epoch']}\")\n",
        "if missing_m:\n",
        "    print(f\"  Model missing keys (expected for new arch): {missing_m}\")\n",
        "if unexpected_m:\n",
        "    print(f\"  Model unexpected keys (ignored): {unexpected_m}\")\n",
        "if missing_f:\n",
        "    print(f\"  Fourier missing keys: {missing_f}\")\n",
        "if unexpected_f:\n",
        "    print(f\"  Fourier unexpected keys: {unexpected_f}\")\n",
        "\n",
        "# Init EMA from loaded online model + new semantic_head\n",
        "model_ema.load_state_dict(model.state_dict())\n",
        "fourier_ema.load_state_dict(fourier_encoder.state_dict())\n",
        "semantic_head_ema.load_state_dict(semantic_head.state_dict())\n",
        "\n",
        "print(\"\\nEMA initialized from loaded model + new semantic_head. Predictor + mask_rgb init from scratch.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostic: Verify Pretrained Weights Are Active\n",
        "\n",
        "Compute initial val_loss (RGB reconstruction) **before any training**. If pretrained loaded correctly, this should be much lower than a random model (random ≈ 0.08–0.12; pretrained ≈ 0.001–0.01). This is the definitive check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Diagnostic: initial reconstruction quality (before any JEPA training) ──\n",
        "# If pretrained weights loaded correctly, val_loss should be MUCH lower than random init.\n",
        "# Random init typically gives val_loss ≈ 0.08–0.12; a good pretrained model ≈ 0.001–0.01.\n",
        "\n",
        "model.eval()\n",
        "fourier_encoder.eval()\n",
        "\n",
        "# Quick check on a few test batches\n",
        "_diag_loss = 0.0\n",
        "_diag_n = 0\n",
        "with torch.no_grad():\n",
        "    for _i, (imgs, _) in enumerate(test_loader):\n",
        "        if _i >= 10:  # 10 batches is enough to see the difference\n",
        "            break\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B = imgs.size(0)\n",
        "        full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "        residual = get_residual(model, full_input)\n",
        "        queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "        rgb = get_rgb(model, queries_full, residual)\n",
        "        target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "        _diag_loss += F.mse_loss(rgb, target_pixels).item()\n",
        "        _diag_n += 1\n",
        "\n",
        "_diag_loss /= _diag_n\n",
        "print(f\"=== DIAGNOSTIC: Initial RGB reconstruction MSE = {_diag_loss:.6f} ===\")\n",
        "if _diag_loss > 0.05:\n",
        "    print(\"  WARNING: This is in the 'random init' range (>0.05).\")\n",
        "    print(\"  Pretrained weights may NOT have loaded correctly!\")\n",
        "    print(\"  Check the missing/unexpected keys printed above.\")\n",
        "else:\n",
        "    print(f\"  GOOD: pretrained encoder is active (MSE {_diag_loss:.6f} << random ~0.08–0.12).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Representation Quality Monitoring\n",
        "\n",
        "Collapse metrics (per-dimension std of mean-pooled φ) and k-NN accuracy on a fixed subset every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_MONITOR_TRAIN = 1000\n",
        "N_MONITOR_TEST = 500\n",
        "MONITOR_GRID = 16\n",
        "K_NN = 5\n",
        "\n",
        "monitor_train_indices = list(range(min(N_MONITOR_TRAIN, len(train_ds))))\n",
        "monitor_test_indices = list(range(min(N_MONITOR_TEST, len(test_ds))))\n",
        "monitor_train_subset = Subset(train_ds, monitor_train_indices)\n",
        "monitor_test_subset = Subset(test_ds, monitor_test_indices)\n",
        "monitor_train_loader = DataLoader(monitor_train_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "monitor_test_loader = DataLoader(monitor_test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "def compute_representation_metrics(model, semantic_head, fourier_encoder, coords_32, device,\n",
        "                                  monitor_train_loader, monitor_test_loader, grid_size=16, k=5):\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    semantic_head.eval()\n",
        "    coords_monitor = create_coordinate_grid(grid_size, grid_size, device)\n",
        "    feats_train, labels_train = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in monitor_train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries = fourier_encoder(coords_monitor.unsqueeze(0).expand(B, -1, -1))\n",
        "            phi = get_semantic_tokens(get_phi_raw(model, queries, residual), semantic_head)\n",
        "            phi_pooled = phi.mean(dim=1)\n",
        "            feats_train.append(phi_pooled.cpu())\n",
        "            labels_train.append(labels)\n",
        "    feats_train = torch.cat(feats_train, dim=0)\n",
        "    labels_train = torch.cat(labels_train, dim=0)\n",
        "    feats_test, labels_test = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in monitor_test_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries = fourier_encoder(coords_monitor.unsqueeze(0).expand(B, -1, -1))\n",
        "            phi = get_semantic_tokens(get_phi_raw(model, queries, residual), semantic_head)\n",
        "            phi_pooled = phi.mean(dim=1)\n",
        "            feats_test.append(phi_pooled.cpu())\n",
        "            labels_test.append(labels)\n",
        "    feats_test = torch.cat(feats_test, dim=0)\n",
        "    labels_test = torch.cat(labels_test, dim=0)\n",
        "\n",
        "    all_feats = torch.cat([feats_train, feats_test], dim=0)\n",
        "    std_per_dim = all_feats.std(dim=0)\n",
        "    phi_std_mean = std_per_dim.mean().item()\n",
        "    phi_std_min = std_per_dim.min().item()\n",
        "\n",
        "    dist = torch.cdist(feats_test, feats_train)\n",
        "    _, idx = dist.topk(k, dim=1, largest=False)\n",
        "    neighbor_labels = labels_train[idx]\n",
        "    votes = torch.mode(neighbor_labels, dim=1).values\n",
        "    knn_acc = (votes == labels_test).float().mean().item()\n",
        "    return {\"phi_std_mean\": phi_std_mean, \"phi_std_min\": phi_std_min, \"knn_acc\": knn_acc}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop — JEPA + RGB Aux + VICReg + EMA Update\n",
        "\n",
        "Same loop as `JEPA_NeuralField_CIFAR10.ipynb`. Saves to `checkpoint_jepa_from_pretrained_best.pt` / `_last.pt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "params = (\n",
        "    list(model.parameters())\n",
        "    + list(fourier_encoder.parameters())\n",
        "    + list(semantic_head.parameters())\n",
        "    + list(predictor.parameters())\n",
        "    + [mask_rgb]\n",
        ")\n",
        "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "best_val_loss = float(\"inf\")\n",
        "CKPT_BEST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_from_pretrained_best.pt\")\n",
        "CKPT_LAST = os.path.join(CHECKPOINT_DIR, \"checkpoint_jepa_from_pretrained_last.pt\")\n",
        "repr_history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    fourier_encoder.train()\n",
        "    semantic_head.train()\n",
        "    predictor.train()\n",
        "    model_ema.eval()\n",
        "    fourier_ema.eval()\n",
        "    semantic_head_ema.eval()\n",
        "    total_loss = 0.0\n",
        "    total_jepa = 0.0\n",
        "    total_rgb = 0.0\n",
        "    total_vic = 0.0\n",
        "    if VERBOSE:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} (batches: {len(train_loader)})\")\n",
        "    for batch_ix, (imgs, _) in enumerate(train_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        B = imgs.size(0)\n",
        "\n",
        "        idx_c, idx_t = sample_context_target_indices(\n",
        "            coords_32, N_CONTEXT, N_TARGET, DEVICE, block_mask=USE_BLOCK_MASK, mask_style=MASK_STYLE,\n",
        "            image_size=IMAGE_SIZE, patch_size=PATCH_SIZE, enc_mask_scale=ENC_MASK_SCALE,\n",
        "            pred_mask_scale=PRED_MASK_SCALE, aspect_ratio=ASPECT_RATIO, n_enc_masks=N_ENC_MASKS,\n",
        "            n_pred_masks=N_PRED_MASKS, allow_overlap=ALLOW_OVERLAP, min_keep=MIN_KEEP)\n",
        "        coords_c = coords_32[idx_c]\n",
        "        coords_t = coords_32[idx_t]\n",
        "\n",
        "        if VERBOSE and batch_ix == 0:\n",
        "            print(f\"  [epoch {epoch+1}] batch 0: N_context={len(idx_c)}, N_target={len(idx_t)} (mask_style={MASK_STYLE})\")\n",
        "\n",
        "        encoder_input = prepare_jepa_encoder_input(imgs, coords_32, fourier_encoder, idx_c, idx_t, DEVICE, mask_rgb)\n",
        "        residual = get_residual(model, encoder_input)\n",
        "\n",
        "        queries_c = fourier_encoder(coords_c.unsqueeze(0).expand(B, -1, -1))\n",
        "        queries_t = fourier_encoder(coords_t.unsqueeze(0).expand(B, -1, -1))\n",
        "\n",
        "        z_c = get_semantic_tokens(get_phi_raw(model, queries_c, residual), semantic_head)\n",
        "\n",
        "        # Target branch: STOP-GRAD\n",
        "        with torch.no_grad():\n",
        "            if TARGET_FROM_FULL_CONTEXT:\n",
        "                full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_ema)\n",
        "                residual_ema = get_residual(model_ema, full_input)\n",
        "            else:\n",
        "                residual_ema = get_residual(model_ema, encoder_input)\n",
        "            phi_t_raw = get_phi_raw(model_ema, queries_t, residual_ema)\n",
        "            z_t_target = get_semantic_tokens(phi_t_raw, semantic_head_ema)\n",
        "            if JEPA_LOSS_TYPE == \"smooth_l1\" or USE_LAYERNORM_JEPA:\n",
        "                z_t_target_raw = semantic_head_ema(phi_t_raw)\n",
        "\n",
        "        z_pred = predictor(queries_t, z_c)\n",
        "        if not USE_JEPA_LOSS:\n",
        "            loss_jepa = torch.tensor(0.0, device=DEVICE)\n",
        "        elif USE_LAYERNORM_JEPA:\n",
        "            z_pred_ln = F.layer_norm(z_pred, (z_pred.size(-1),))\n",
        "            z_t_ln = F.layer_norm(z_t_target_raw, (z_t_target_raw.size(-1),))\n",
        "            loss_jepa = F.smooth_l1_loss(z_pred_ln, z_t_ln).mean()\n",
        "        elif JEPA_LOSS_TYPE == \"cosine\":\n",
        "            loss_jepa = jepa_cosine_loss(z_pred, z_t_target)\n",
        "        elif JEPA_LOSS_TYPE == \"l2\":\n",
        "            loss_jepa = jepa_l2_loss(z_pred, z_t_target)\n",
        "        else:\n",
        "            loss_jepa = jepa_smooth_l1_loss(z_pred, z_t_target_raw)\n",
        "        loss = loss_jepa\n",
        "        loss_rgb = torch.tensor(0.0, device=DEVICE)\n",
        "        loss_vic = torch.tensor(0.0, device=DEVICE)\n",
        "\n",
        "        if USE_VICREG:\n",
        "            loss_vic = vicreg_loss(z_c, VICREG_SIM_WEIGHT, VICREG_VAR_WEIGHT, VICREG_COV_WEIGHT)\n",
        "            loss = loss + LAMBDA_VICREG * loss_vic\n",
        "            total_vic += loss_vic.item()\n",
        "\n",
        "        if USE_RGB_AUX:\n",
        "            if RGB_QUERY_RES >= IMAGE_SIZE:\n",
        "                queries_rgb = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "                rgb = get_rgb(model, queries_rgb, residual)\n",
        "                target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            else:\n",
        "                coords_rgb = create_coordinate_grid(RGB_QUERY_RES, RGB_QUERY_RES, DEVICE)\n",
        "                queries_rgb = fourier_encoder(coords_rgb.unsqueeze(0).expand(B, -1, -1))\n",
        "                rgb = get_rgb(model, queries_rgb, residual)\n",
        "                target_pixels = sample_gt_at_coords(imgs, coords_rgb.unsqueeze(0).expand(B, -1, -1))\n",
        "            loss_rgb = F.mse_loss(rgb, target_pixels) * LAMBDA_RGB\n",
        "            loss = loss + loss_rgb\n",
        "            total_rgb += loss_rgb.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_jepa += loss_jepa.item()\n",
        "\n",
        "        if VERBOSE and LOG_EVERY > 0 and (batch_ix + 1) % LOG_EVERY == 0:\n",
        "            rgb_val = total_rgb / (batch_ix + 1) if USE_RGB_AUX else 0.0\n",
        "            vic_val = total_vic / (batch_ix + 1) if USE_VICREG else 0.0\n",
        "            print(f\"  [epoch {epoch+1}] batch {batch_ix+1}/{len(train_loader)}: loss={loss.item():.4f} jepa={loss_jepa.item():.4f}\" +\n",
        "                  (f\" rgb={loss_rgb.item():.4f}\" if USE_RGB_AUX else \"\") +\n",
        "                  (f\" vic={loss_vic.item():.4f}\" if USE_VICREG else \"\") +\n",
        "                  f\" (avg so far: loss={total_loss/(batch_ix+1):.4f})\")\n",
        "\n",
        "        if EMA_MOMENTUM_RAMP:\n",
        "            total_steps = EPOCHS * len(train_loader)\n",
        "            step = epoch * len(train_loader) + batch_ix\n",
        "            current_momentum = EMA_MOMENTUM_START + (EMA_MOMENTUM_END - EMA_MOMENTUM_START) * min(1.0, step / total_steps)\n",
        "        else:\n",
        "            current_momentum = EMA_MOMENTUM\n",
        "        copy_ema(model, model_ema, current_momentum)\n",
        "        copy_ema(fourier_encoder, fourier_ema, current_momentum)\n",
        "        copy_ema(semantic_head, semantic_head_ema, current_momentum)\n",
        "\n",
        "    avg = total_loss / len(train_loader)\n",
        "    model.eval()\n",
        "    fourier_encoder.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, _ in test_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            B = imgs.size(0)\n",
        "            full_input, _, _ = prepare_model_input(imgs, coords_32, fourier_encoder)\n",
        "            residual = get_residual(model, full_input)\n",
        "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
        "            rgb = get_rgb(model, queries_full, residual)\n",
        "            target_pixels = rearrange(imgs, \"b c h w -> b (h w) c\")\n",
        "            val_loss += F.mse_loss(rgb, target_pixels).item()\n",
        "    val_loss /= len(test_loader)\n",
        "    repr_metrics = compute_representation_metrics(\n",
        "        model, semantic_head, fourier_encoder, coords_32, DEVICE,\n",
        "        monitor_train_loader, monitor_test_loader, grid_size=MONITOR_GRID, k=K_NN)\n",
        "    repr_history.append(repr_metrics)\n",
        "    if VERBOSE:\n",
        "        print(f\"  repr: phi_std_mean={repr_metrics['phi_std_mean']:.4f} phi_std_min={repr_metrics['phi_std_min']:.4f} knn_acc={repr_metrics['knn_acc']:.4f}\")\n",
        "\n",
        "    ckpt_data = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"fourier_encoder_state_dict\": fourier_encoder.state_dict(),\n",
        "        \"semantic_head_state_dict\": semantic_head.state_dict(),\n",
        "        \"predictor_state_dict\": predictor.state_dict(),\n",
        "        \"model_ema_state_dict\": model_ema.state_dict(),\n",
        "        \"fourier_ema_state_dict\": fourier_ema.state_dict(),\n",
        "        \"semantic_head_ema_state_dict\": semantic_head_ema.state_dict(),\n",
        "        \"best_val_loss\": best_val_loss,\n",
        "    }\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        ckpt_data[\"best_val_loss\"] = best_val_loss\n",
        "        torch.save(ckpt_data, CKPT_BEST)\n",
        "    torch.save(ckpt_data, CKPT_LAST)\n",
        "\n",
        "    vic_str = f\" vic: {total_vic/len(train_loader):.4f}\" if USE_VICREG else \"\"\n",
        "    rgb_str = f\" rgb: {total_rgb/len(train_loader):.4f}\" if USE_RGB_AUX else \"\"\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} jepa: {total_jepa/len(train_loader):.4f}{vic_str}{rgb_str} val_loss: {val_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot representation quality over epochs\n",
        "if len(repr_history) > 0:\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
        "    epochs_arr = np.arange(1, len(repr_history) + 1)\n",
        "    ax[0].plot(epochs_arr, [m[\"phi_std_mean\"] for m in repr_history], \"o-\")\n",
        "    ax[0].set_title(\"phi std (mean over dims)\")\n",
        "    ax[0].set_xlabel(\"Epoch\")\n",
        "    ax[1].plot(epochs_arr, [m[\"phi_std_min\"] for m in repr_history], \"o-\")\n",
        "    ax[1].set_title(\"phi std (min dim)\")\n",
        "    ax[1].set_xlabel(\"Epoch\")\n",
        "    ax[2].plot(epochs_arr, [m[\"knn_acc\"] for m in repr_history], \"o-\")\n",
        "    ax[2].set_title(\"k-NN accuracy\")\n",
        "    ax[2].set_xlabel(\"Epoch\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}