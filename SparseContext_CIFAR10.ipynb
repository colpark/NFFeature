{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Context Reconstruction + NCE (CIFAR-10)\n",
    "\n",
    "Based on **AblationCIFAR10.ipynb**. The model sees only a **random subset of pixels** (sparse context) per image and must reconstruct the **full** 32Ã—32. This encourages structure/prior over texture. Optional **Soft InfoNCE** is integrated during training.\n",
    "\n",
    "- Checkpoints: **checkpoint_sparse_best.pt**, **checkpoint_sparse_last.pt** (separate from Ablation).\n",
    "- Config: `CONTEXT_FRAC` (e.g. 0.2), `USE_NCE` (bool), NCE params if enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from nf_feature_models import (\n",
    "    CascadedPerceiverIO,\n",
    "    GaussianFourierFeatures,\n",
    "    create_coordinate_grid,\n",
    "    prepare_model_input,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "FOURIER_MAPPING_SIZE = 96\n",
    "POS_EMBED_DIM = FOURIER_MAPPING_SIZE * 2\n",
    "INPUT_DIM = CHANNELS + POS_EMBED_DIM\n",
    "QUERIES_DIM = POS_EMBED_DIM\n",
    "LOGITS_DIM = CHANNELS\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "CONTEXT_FRAC = 0.2\n",
    "USE_NCE = True\n",
    "NCE_RAMP_STEPS = 500\n",
    "LAMBDA_NCE = 0.1\n",
    "N_A, N_B = 256, 1024\n",
    "TAU, SIGMA = 0.1, 0.08\n",
    "PROJ_DIM = 128\n",
    "\n",
    "coords_32 = create_coordinate_grid(IMAGE_SIZE, IMAGE_SIZE, DEVICE)\n",
    "N_FULL = coords_32.size(0)\n",
    "N_SPARSE = max(64, int(N_FULL * CONTEXT_FRAC))\n",
    "print(f'Full grid: {N_FULL}, Sparse context: {N_SPARSE}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gt_at_coords(images, coords):\n",
    "    B, C, H, W = images.shape\n",
    "    N = coords.shape[1]\n",
    "    grid = coords[..., [1, 0]].view(B, 1, N, 2)\n",
    "    sampled = F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    return sampled.squeeze(2).permute(0, 2, 1)\n",
    "\n",
    "def prepare_sparse_context(images, coords_full, fourier_encoder, num_sparse, device):\n",
    "    B, C, H, W = images.shape\n",
    "    idx = torch.randperm(coords_full.size(0), device=device)[:num_sparse]\n",
    "    coords_sparse = coords_full[idx]\n",
    "    pixels_sparse = sample_gt_at_coords(images, coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
    "    pos_sparse = fourier_encoder(coords_sparse.unsqueeze(0).expand(B, -1, -1))\n",
    "    input_sparse = torch.cat([pixels_sparse, pos_sparse], dim=-1)\n",
    "    return input_sparse\n",
    "\n",
    "def get_residual(model, data):\n",
    "    residual = None\n",
    "    for block in model.encoder_blocks:\n",
    "        residual = block(x=residual, context=data, mask=None, residual=residual)\n",
    "    for sa_block in model.self_attn_blocks:\n",
    "        residual = sa_block[0](residual) + residual\n",
    "        residual = sa_block[1](residual) + residual\n",
    "    return residual\n",
    "\n",
    "def get_rgb_and_phi_raw(model, queries, residual):\n",
    "    x = model.decoder_cross_attn(queries, context=residual)\n",
    "    x = x + queries\n",
    "    if model.decoder_ff is not None:\n",
    "        x = x + model.decoder_ff(x)\n",
    "    return model.to_logits(x), x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_encoder = GaussianFourierFeatures(2, FOURIER_MAPPING_SIZE, 15.0).to(DEVICE)\n",
    "model = CascadedPerceiverIO(\n",
    "    input_dim=INPUT_DIM, queries_dim=QUERIES_DIM, logits_dim=LOGITS_DIM,\n",
    "    latent_dims=(256, 384, 512), num_latents=(256, 256, 256), decoder_ff=True,\n",
    ").to(DEVICE)\n",
    "projection_head = nn.Linear(QUERIES_DIM, PROJ_DIM).to(DEVICE)\n",
    "def proj_norm(z):\n",
    "    return F.normalize(projection_head(z), dim=-1)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_affine_params(batch_size, device, scale_range=(0.85, 1.0), max_translate=0.1, max_angle_deg=12):\n",
    "    angle = (torch.rand(batch_size, device=device) * 2 - 1) * (max_angle_deg * math.pi / 180)\n",
    "    scale = scale_range[0] + torch.rand(batch_size, device=device) * (scale_range[1] - scale_range[0])\n",
    "    tx = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
    "    ty = (torch.rand(batch_size, device=device) * 2 - 1) * max_translate\n",
    "    c, s = torch.cos(angle), torch.sin(angle)\n",
    "    R = torch.stack([c*scale, -s*scale, s*scale, c*scale], dim=-1).view(batch_size, 2, 2)\n",
    "    t = torch.stack([tx, ty], dim=1)\n",
    "    return R, t\n",
    "\n",
    "def apply_affine_to_coords(coords, R, t):\n",
    "    return torch.einsum('bed,bnd->bne', R, coords) + t.unsqueeze(1)\n",
    "\n",
    "def apply_affine_to_image(images, R, t):\n",
    "    R_inv = torch.inverse(R)\n",
    "    theta = torch.cat([R_inv, -(R_inv @ t.unsqueeze(2))], dim=2)\n",
    "    grid = F.affine_grid(theta, images.size(), align_corners=True)\n",
    "    return F.grid_sample(images, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "\n",
    "def soft_infonce_loss(phi_a, phi_b, coords_a, coords_b, R, t, tau=0.1, sigma=0.08):\n",
    "    B, N_a, _ = phi_a.shape\n",
    "    logits = torch.bmm(phi_a, phi_b.transpose(1, 2)) / tau\n",
    "    xi_mapped = apply_affine_to_coords(coords_a, R, t)\n",
    "    sqd = ((coords_b.unsqueeze(1) - xi_mapped.unsqueeze(2)) ** 2).sum(-1)\n",
    "    w = torch.exp(-sqd / (2 * sigma ** 2))\n",
    "    w = w / (w.sum(dim=2, keepdim=True) + 1e-8)\n",
    "    return -(w * F.log_softmax(logits, dim=-1)).sum(-1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print('Train batches:', len(train_loader), 'Test batches:', len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters()) + list(fourier_encoder.parameters()) + list(projection_head.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "step = [0]\n",
    "best_val_loss = float('inf')\n",
    "CKPT_PREFIX = 'checkpoint_sparse'\n",
    "CKPT_BEST = os.path.join(CHECKPOINT_DIR, CKPT_PREFIX + '_best.pt')\n",
    "CKPT_LAST = os.path.join(CHECKPOINT_DIR, CKPT_PREFIX + '_last.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    fourier_encoder.train()\n",
    "    projection_head.train()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_nce = 0.0\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        B = imgs.size(0)\n",
    "        input_sparse = prepare_sparse_context(imgs, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
    "        target_pixels = rearrange(imgs, 'b c h w -> b (h w) c')\n",
    "        queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
    "        reconstructed = model(input_sparse, queries=queries_full)\n",
    "        loss_recon = F.mse_loss(reconstructed, target_pixels)\n",
    "        loss = loss_recon\n",
    "        if USE_NCE:\n",
    "            R, t = sample_affine_params(B, DEVICE)\n",
    "            imgs_b = apply_affine_to_image(imgs, R, t)\n",
    "            input_sparse_b = prepare_sparse_context(imgs_b, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
    "            residual_a = get_residual(model, input_sparse)\n",
    "            residual_b = get_residual(model, input_sparse_b)\n",
    "            anchors_a = (torch.rand(B, N_A, 2, device=DEVICE) * 2 - 1)\n",
    "            candidates_b = (torch.rand(B, N_B, 2, device=DEVICE) * 2 - 1)\n",
    "            q_a = fourier_encoder(anchors_a)\n",
    "            q_b = fourier_encoder(candidates_b)\n",
    "            _, phi_raw_a = get_rgb_and_phi_raw(model, q_a, residual_a)\n",
    "            _, phi_raw_b = get_rgb_and_phi_raw(model, q_b, residual_b)\n",
    "            phi_a = proj_norm(phi_raw_a)\n",
    "            phi_b = proj_norm(phi_raw_b)\n",
    "            lam = LAMBDA_NCE if step[0] >= NCE_RAMP_STEPS else LAMBDA_NCE * (step[0] / NCE_RAMP_STEPS)\n",
    "            loss_nce = soft_infonce_loss(phi_a, phi_b, anchors_a, candidates_b, R, t, TAU, SIGMA)\n",
    "            loss = loss + lam * loss_nce\n",
    "            total_nce += loss_nce.item()\n",
    "            step[0] += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_recon += loss_recon.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_recon = total_recon / len(train_loader)\n",
    "    model.eval()\n",
    "    fourier_encoder.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in test_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            B = imgs.size(0)\n",
    "            input_sparse = prepare_sparse_context(imgs, coords_32, fourier_encoder, N_SPARSE, DEVICE)\n",
    "            target_pixels = rearrange(imgs, 'b c h w -> b (h w) c')\n",
    "            queries_full = fourier_encoder(coords_32.unsqueeze(0).expand(B, -1, -1))\n",
    "            reconstructed = model(input_sparse, queries=queries_full)\n",
    "            val_loss += F.mse_loss(reconstructed, target_pixels).item()\n",
    "    val_loss /= len(test_loader)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict(), 'fourier_encoder_state_dict': fourier_encoder.state_dict(), 'projection_head_state_dict': projection_head.state_dict(), 'best_val_loss': best_val_loss}, CKPT_BEST)\n",
    "    torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict(), 'fourier_encoder_state_dict': fourier_encoder.state_dict(), 'projection_head_state_dict': projection_head.state_dict(), 'best_val_loss': best_val_loss}, CKPT_LAST)\n",
    "    nce_str = f' NCE: {total_nce/len(train_loader):.4f}' if USE_NCE else ''\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} recon: {avg_recon:.4f}{nce_str} val_loss: {val_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
